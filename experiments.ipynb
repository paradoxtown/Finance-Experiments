{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DataProcessor(object):\n",
    "    def __init__(self, data_path, features, target, feature_type, train_ratio=0.7, test_ratio=0.2, random_state=42):\n",
    "        self.data_path = data_path\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.feature_type = feature_type\n",
    "        \n",
    "        self.train_ratio = train_ratio\n",
    "        self.test_ratio = test_ratio\n",
    "        self.val_ratio = 1 - train_ratio - test_ratio\n",
    "        \n",
    "        self.random_state = random_state\n",
    "        self.__load_data__()\n",
    "    \n",
    "    def __load_data__(self):\n",
    "        self.df_raw = pd.read_csv(self.data_path, parse_dates=True, index_col='date')\n",
    "        self.df_raw = self.df_raw.sort_values(by='date', ascending=True)\n",
    "        \n",
    "    def get_data(self):\n",
    "        self.scaler = preprocessing.StandardScaler()\n",
    "        \n",
    "        if self.target == 'trend':\n",
    "            self.get_target_trend()\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.df_raw[self.features], \n",
    "                                                            self.df_raw[self.target], \n",
    "                                                            test_size=self.test_ratio, \n",
    "                                                            random_state=self.random_state,\n",
    "                                                            shuffle=False)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                          y_train, \n",
    "                                                          test_size=self.val_ratio, \n",
    "                                                          random_state=self.random_state,\n",
    "                                                          shuffle=False)\n",
    "        self.scaler.fit(X_train)\n",
    "        X_train = self.scaler.transform(X_train)\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test       \n",
    "            \n",
    "    def get_target_trend(self):\n",
    "        self.df_raw['trend'] = self.df_raw['close'].diff()\n",
    "        self.df_raw['trend'] = self.df_raw['trend'].apply(lambda x: 1 if x > 0 else 0)\n",
    "        self.df_raw['trend'] = self.df_raw['trend'].shift(-1, fill_value=1)\n",
    "    \n",
    "    def add_technical_indicators(self):\n",
    "        # moving average\n",
    "        self.df_raw['ma5'] = self.df_raw['close'].rolling(5).mean()\n",
    "        # # exponential moving average\n",
    "        # self.df_raw['ema5'] = self.df_raw['close'].ewm(span=5, adjust=False).mean()\n",
    "        # # moving average convergence divergence\n",
    "        # self.df_raw['macd'] = self.df_raw['close'].ewm(span=12, adjust=False).mean() - self.df_raw['close'].ewm(span=26, adjust=False).mean()\n",
    "        # # relative strength index\n",
    "        # self.df_raw['rsi'] = self.df_raw['close'].ewm(span=14, adjust=False).mean() / self.df_raw['close'].ewm(span=14, adjust=False).mean()\n",
    "        # # stochastic oscillator\n",
    "        # self.df_raw['stoch'] = (self.df_raw['close'] - self.df_raw['low'].rolling(14).min()) / (self.df_raw['high'].rolling(14).max() - self.df_raw['low'].rolling(14).min())\n",
    "        # # williams %R\n",
    "        # self.df_raw['williams'] = (self.df_raw['high'].rolling(14).max() - self.df_raw['close']) / (self.df_raw['high'].rolling(14).max() - self.df_raw['low'].rolling(14).min())\n",
    "    \n",
    "    def missing_value(self):\n",
    "        if self.df_raw.isnull().values.any():\n",
    "            self.df_raw = self.df_raw.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataProcessor(data_path='./datasets/A_price_data.csv',\n",
    "                               features=['open', 'high', 'low', 'close', 'volume', 'vwap', 'ma5'],\n",
    "                               target='trend',\n",
    "                               feature_type='daily_point')\n",
    "data_processor.add_technical_indicators()\n",
    "data_processor.missing_value()\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = data_processor.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4038, 7), (449, 7), (1122, 7), (4038,), (449,), (1122,))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5597147950089126"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(y_test).count(1.0) / len(list(y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', tol=0.0001, C=1.0, fit_intercept=True, verbose=10, max_iter=100, random_state=0, solver='lbfgs', n_jobs=1, multi_class='auto', warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =            8     M =           10\n",
      "\n",
      " L =  0.0000D+00  0.0000D+00  0.0000D+00  0.0000D+00  0.0000D+00  0.0000D+00\n",
      "      0.0000D+00  0.0000D+00\n",
      "\n",
      "X0 =  0.0000D+00  0.0000D+00  0.0000D+00  0.0000D+00  0.0000D+00  0.0000D+00\n",
      "      0.0000D+00  0.0000D+00\n",
      "\n",
      " U =  0.0000D+00  0.0000D+00  0.0000D+00  0.0000D+00  0.0000D+00  0.0000D+00\n",
      "      0.0000D+00  0.0000D+00\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.79893D+03    |proj g|=  8.19104D+01\n",
      "\n",
      "\n",
      "ITERATION     1\n",
      "\n",
      "---------------- CAUCHY entered-------------------\n",
      " There are            0   breakpoints \n",
      "\n",
      " GCP found in this segment\n",
      "Piece      1 --f1, f2 at start point  -3.9796D+04  3.9796D+04\n",
      "Distance to the stationary point =   1.0000D+00\n",
      "Cauchy X =  \n",
      "     -8.0446D+01 -8.1910D+01 -7.7618D+01 -7.9972D+01  3.9973D+01 -7.9956D+01\n",
      "     -7.8739D+01  2.0000D+00\n",
      "\n",
      "---------------- exit CAUCHY----------------------\n",
      "\n",
      "           8  variables are free at GCP            1\n",
      " LINE SEARCH           1  times; norm of step =    3.7816320294176461E-002\n",
      "\n",
      "At iterate    1    f=  2.79561D+03    |proj g|=  2.34202D+01\n",
      "\n",
      " X = -1.5250D-02 -1.5527D-02 -1.4714D-02 -1.5160D-02  7.5776D-03 -1.5157D-02\n",
      "     -1.4926D-02  3.7913D-04\n",
      "\n",
      " G = -1.1125D+01 -9.5629D+00 -1.4032D+01 -1.1589D+01 -2.3420D+01 -1.1693D+01\n",
      "     -1.2666D+01 -1.5146D+00\n",
      "\n",
      "\n",
      "ITERATION     2\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    5.5184403021994967E-003\n",
      "\n",
      "At iterate    2    f=  2.79546D+03    |proj g|=  2.08905D+01\n",
      "\n",
      " X = -1.3624D-02 -1.4135D-02 -1.2653D-02 -1.3465D-02  1.1089D-02 -1.3446D-02\n",
      "     -1.3070D-02  6.0595D-04\n",
      "\n",
      " G = -1.2210D+00  3.6679D-01 -4.1586D+00 -1.6887D+00 -2.0891D+01 -1.7816D+00\n",
      "     -2.7688D+00 -1.3156D+00\n",
      "\n",
      "\n",
      "ITERATION     3\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    9.5547692813284305E-003\n",
      "\n",
      "At iterate    3    f=  2.79532D+03    |proj g|=  1.25975D+01\n",
      "\n",
      " X = -1.2685D-02 -1.3857D-02 -1.0490D-02 -1.2331D-02  2.0042D-02 -1.2273D-02\n",
      "     -1.1484D-02  1.1709D-03\n",
      "\n",
      " G =  5.1353D+00  6.8008D+00  2.1135D+00  4.6618D+00 -1.2597D+01  4.5785D+00\n",
      "      3.5976D+00 -7.6258D-01\n",
      "\n",
      "\n",
      "ITERATION     4\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.0850163566472315E-002\n",
      "\n",
      "At iterate    4    f=  2.79523D+03    |proj g|=  5.59168D+00\n",
      "\n",
      " X = -1.3214D-02 -1.5604D-02 -8.8015D-03 -1.2512D-02  3.0563D-02 -1.2388D-02\n",
      "     -1.0880D-02  1.8249D-03\n",
      "\n",
      " G =  3.8336D+00  5.5917D+00  7.1400D-01  3.3544D+00 -2.0351D+00  3.2739D+00\n",
      "      2.3230D+00 -1.0487D-01\n",
      "\n",
      "\n",
      "ITERATION     5\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    4.8959546837912720E-003\n",
      "\n",
      "At iterate    5    f=  2.79521D+03    |proj g|=  2.74373D+00\n",
      "\n",
      " X = -1.4359D-02 -1.8658D-02 -6.5566D-03 -1.3136D-02  3.3274D-02 -1.2922D-02\n",
      "     -1.0383D-02  1.9843D-03\n",
      "\n",
      " G =  9.7315D-01  2.7437D+00 -2.1611D+00  4.8984D-01  8.8205D-01  4.0884D-01\n",
      "     -5.2037D-01  6.1232D-02\n",
      "\n",
      "\n",
      "ITERATION     6\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    6.8415490893186087E-003\n",
      "\n",
      "At iterate    6    f=  2.79519D+03    |proj g|=  4.87898D+00\n",
      "\n",
      " X = -1.5953D-02 -2.3410D-02 -2.5506D-03 -1.3869D-02  3.5154D-02 -1.3507D-02\n",
      "     -9.2837D-03  2.0879D-03\n",
      "\n",
      " G = -1.7472D+00  1.8134D-02 -4.8790D+00 -2.2367D+00  2.9257D+00 -2.3172D+00\n",
      "     -3.2207D+00  1.7206D-01\n",
      "\n",
      "\n",
      "ITERATION     7\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.3747049500459836E-002\n",
      "\n",
      "At iterate    7    f=  2.79516D+03    |proj g|=  7.86667D+00\n",
      "\n",
      " X = -1.8864D-02 -3.2920D-02  6.2325D-03 -1.4981D-02  3.7094D-02 -1.4310D-02\n",
      "     -6.5852D-03  2.1843D-03\n",
      "\n",
      " G = -4.7605D+00 -3.0265D+00 -7.8667D+00 -5.2627D+00  4.9644D+00 -5.3402D+00\n",
      "     -6.2005D+00  2.7781D-01\n",
      "\n",
      "\n",
      "ITERATION     8\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    4.1405818576453396E-002\n",
      "\n",
      "At iterate    8    f=  2.79508D+03    |proj g|=  1.28375D+01\n",
      "\n",
      " X = -2.7072D-02 -6.1184D-02  3.3582D-02 -1.7707D-02  4.0496D-02 -1.6103D-02\n",
      "      2.2432D-03  2.3292D-03\n",
      "\n",
      " G = -9.8349D+00 -8.2207D+00 -1.2838D+01 -1.0374D+01  8.2918D+00 -1.0440D+01\n",
      "     -1.1185D+01  4.4347D-01\n",
      "\n",
      "\n",
      "ITERATION     9\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    9.6247820403894499E-002\n",
      "\n",
      "At iterate    9    f=  2.79490D+03    |proj g|=  1.86441D+01\n",
      "\n",
      " X = -4.5319D-02 -1.2618D-01  9.8271D-02 -2.3138D-02  4.5123D-02 -1.9365D-02\n",
      "      2.3701D-02  2.4757D-03\n",
      "\n",
      " G = -1.5915D+01 -1.4608D+01 -1.8644D+01 -1.6536D+01  1.2219D+01 -1.6570D+01\n",
      "     -1.7073D+01  6.3109D-01\n",
      "\n",
      "\n",
      "ITERATION    10\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =   0.19571515841202450     \n",
      "\n",
      "At iterate   10    f=  2.79456D+03    |proj g|=  2.24961D+01\n",
      "\n",
      " X = -8.1285D-02 -2.5723D-01  2.3123D-01 -3.2890D-02  5.0085D-02 -2.4728D-02\n",
      "      6.8549D-02  2.5254D-03\n",
      "\n",
      " G = -2.0364D+01 -1.9697D+01 -2.2496D+01 -2.1126D+01  1.5060D+01 -2.1098D+01\n",
      "     -2.1161D+01  7.6766D-01\n",
      "\n",
      "\n",
      "ITERATION    11\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =   0.26147669913371852     \n",
      "\n",
      "At iterate   11    f=  2.79415D+03    |proj g|=  1.80329D+01\n",
      "\n",
      " X = -1.2834D-01 -4.3073D-01  4.1058D-01 -4.4060D-02  5.1684D-02 -3.0172D-02\n",
      "      1.2961D-01  2.3287D-03\n",
      "\n",
      " G = -1.6743D+01 -1.6929D+01 -1.8033D+01 -1.7644D+01  1.3021D+01 -1.7541D+01\n",
      "     -1.7095D+01  6.9522D-01\n",
      "\n",
      "\n",
      "ITERATION    12\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =   0.15730606606769040     \n",
      "\n",
      "At iterate   12    f=  2.79394D+03    |proj g|=  7.83706D+00\n",
      "\n",
      " X = -1.5723D-01 -5.3330D-01  5.2016D-01 -4.8313D-02  4.6945D-02 -3.1457D-02\n",
      "      1.6620D-01  1.7234D-03\n",
      "\n",
      " G = -6.8829D+00 -7.6181D+00 -7.5947D+00 -7.8371D+00  5.4480D+00 -7.6910D+00\n",
      "     -6.9959D+00  1.4401D-01\n",
      "\n",
      "\n",
      "ITERATION    13\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    5.9024363731207423E-003\n",
      "\n",
      "At iterate   13    f=  2.79390D+03    |proj g|=  1.70313D+00\n",
      "\n",
      " X = -1.5795D-01 -5.3417D-01  5.2373D-01 -4.6707D-02  4.3021D-02 -3.0070D-02\n",
      "      1.6714D-01  1.5026D-03\n",
      "\n",
      " G = -7.5014D-01 -1.5449D+00 -1.3950D+00 -1.7031D+00  9.2460D-01 -1.5508D+00\n",
      "     -8.7075D-01 -1.1007D-01\n",
      "\n",
      "\n",
      "ITERATION    14\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    2.3148616664107086E-002\n",
      "\n",
      "At iterate   14    f=  2.79389D+03    |proj g|=  7.05206D-01\n",
      "\n",
      " X = -1.5384D-01 -5.1846D-01  5.0828D-01 -4.5192D-02  4.1822D-02 -2.9152D-02\n",
      "      1.6179D-01  1.5041D-03\n",
      "\n",
      " G =  7.0521D-01 -3.2593D-02  2.8012D-03 -2.3846D-01 -1.2186D-01 -8.9884D-02\n",
      "      5.4282D-01 -1.3302D-01\n",
      "\n",
      "\n",
      "ITERATION    15\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    6.9195660239023777E-003\n",
      "\n",
      "At iterate   15    f=  2.79389D+03    |proj g|=  7.80918D-01\n",
      "\n",
      " X = -1.5283D-01 -5.1377D-01  5.0365D-01 -4.4692D-02  4.1702D-02 -2.8870D-02\n",
      "      1.6003D-01  1.5611D-03\n",
      "\n",
      " G =  7.8092D-01  6.4583D-02  5.9238D-02 -1.5783D-01 -1.6381D-01 -1.1305D-02\n",
      "      6.0618D-01 -8.1045D-02\n",
      "\n",
      "\n",
      "ITERATION    16\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    9.9572982181941848E-003\n",
      "\n",
      "At iterate   16    f=  2.79389D+03    |proj g|=  6.04270D-01\n",
      "\n",
      " X = -1.5534D-01 -5.0739D-01  4.9935D-01 -4.1673D-02  4.1176D-02 -2.7059D-02\n",
      "      1.5545D-01  1.7244D-03\n",
      "\n",
      " G =  6.0427D-01 -6.7178D-02 -1.1433D-01 -2.9131D-01 -5.7874D-01 -1.5728D-01\n",
      "      4.0423D-01  7.7919D-02\n",
      "\n",
      "\n",
      "ITERATION    17\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           1  times; norm of step =    5.6664476291424022E-002\n",
      "\n",
      "At iterate   17    f=  2.79387D+03    |proj g|=  2.39086D+00\n",
      "\n",
      " X = -1.7739D-01 -4.7490D-01  4.8378D-01 -1.9918D-02  4.3757D-02 -1.3651D-02\n",
      "      1.2777D-01  2.3365D-03\n",
      "\n",
      " G =  2.3909D+00  2.0352D+00  1.7090D+00  1.8010D+00  2.1579D+00  1.8536D+00\n",
      "      2.0548D+00  6.4307D-01\n",
      "\n",
      "\n",
      "ITERATION    18\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    5.4500859185247816E-002\n",
      "\n",
      "At iterate   18    f=  2.79385D+03    |proj g|=  1.18500D+00\n",
      "\n",
      " X = -2.1518D-01 -4.7632D-01  5.0477D-01  2.4291D-03  4.0476D-02  1.0075D-04\n",
      "      1.0776D-01  2.1600D-03\n",
      "\n",
      " G =  4.8926D-01  2.8011D-01  9.4744D-02  2.4998D-01 -1.1850D+00  2.1042D-01\n",
      "      7.8129D-02  4.8247D-01\n",
      "\n",
      "\n",
      "ITERATION    19\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    3.7323322779101331E-002\n",
      "\n",
      "At iterate   19    f=  2.79384D+03    |proj g|=  6.74506D-01\n",
      "\n",
      " X = -2.3355D-01 -4.9448D-01  5.3049D-01  9.1199D-03  4.1376D-02  4.3989D-03\n",
      "      1.0710D-01  1.9948D-03\n",
      "\n",
      " G = -4.1962D-02 -2.6635D-01 -2.7040D-01 -1.7265D-01 -6.7451D-01 -2.3664D-01\n",
      "     -4.2999D-01  3.3648D-01\n",
      "\n",
      "\n",
      "ITERATION    20\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    2.4328214589006164E-002\n",
      "\n",
      "At iterate   20    f=  2.79384D+03    |proj g|=  5.84912D-01\n",
      "\n",
      " X = -2.4421D-01 -5.0741D-01  5.4762D-01  1.1825D-02  4.2231D-02  6.3932D-03\n",
      "      1.0929D-01  1.0696D-03\n",
      "\n",
      " G =  3.3873D-01  8.8848D-02  2.0807D-01  2.5540D-01 -1.5345D-01  1.8306D-01\n",
      "     -2.0135D-02 -5.8491D-01\n",
      "\n",
      "\n",
      "ITERATION    21\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    4.6568938996807687E-003\n",
      "\n",
      "At iterate   21    f=  2.79384D+03    |proj g|=  2.17708D-01\n",
      "\n",
      " X = -2.4567D-01 -5.1018D-01  5.5068D-01  1.1576D-02  4.2573D-02  6.3440D-03\n",
      "      1.1059D-01  1.8628D-03\n",
      "\n",
      " G =  1.3642D-01 -1.2222D-01  1.7681D-02  5.2936D-02  1.5073D-01 -1.8659D-02\n",
      "     -2.1121D-01  2.1771D-01\n",
      "\n",
      "\n",
      "ITERATION    22\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.4031049392505691E-003\n",
      "\n",
      "At iterate   22    f=  2.79384D+03    |proj g|=  1.92910D-01\n",
      "\n",
      " X = -2.4542D-01 -5.0926D-01  5.4968D-01  1.1397D-02  4.2519D-02  6.2801D-03\n",
      "      1.1067D-01  1.7483D-03\n",
      "\n",
      " G =  1.5449D-01 -1.0128D-01  3.0668D-02  6.9473D-02  1.1412D-01 -1.7405D-03\n",
      "     -1.9291D-01  1.0145D-01\n",
      "\n",
      "\n",
      "ITERATION    23\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    8.6552934819041204E-003\n",
      "\n",
      "At iterate   23    f=  2.79384D+03    |proj g|=  5.25640D-01\n",
      "\n",
      " X = -2.4667D-01 -5.0319D-01  5.4426D-01  1.0006D-02  4.2233D-02  6.1503D-03\n",
      "      1.1285D-01  1.1304D-03\n",
      "\n",
      " G =  2.1887D-01 -1.2635D-02  7.0243D-02  1.3189D-01 -5.9281D-02  6.2378D-02\n",
      "     -1.1139D-01 -5.2564D-01\n",
      "\n",
      "\n",
      "ITERATION    24\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.2881249365420237E-002\n",
      "\n",
      "At iterate   24    f=  2.79383D+03    |proj g|=  1.04904D+00\n",
      "\n",
      " X = -2.5326D-01 -4.9547D-01  5.3940D-01  7.8217D-03  4.1820D-02  6.5070D-03\n",
      "      1.1867D-01  6.1628D-04\n",
      "\n",
      " G =  4.7001D-01  2.7622D-01  3.0704D-01  3.9400D-01 -3.6005D-01  3.2576D-01\n",
      "      1.8933D-01 -1.0490D+00\n",
      "\n",
      "\n",
      "ITERATION    25\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    3.1485104468726086E-002\n",
      "\n",
      "At iterate   25    f=  2.79383D+03    |proj g|=  2.14231D+00\n",
      "\n",
      " X = -2.7528D-01 -4.8008D-01  5.3274D-01  3.6363D-03  4.2312D-02  8.2517D-03\n",
      "      1.3293D-01 -4.7103D-04\n",
      "\n",
      " G = -1.1020D+00 -1.1820D+00 -1.2784D+00 -1.1195D+00  5.1936D-01 -1.1934D+00\n",
      "     -1.2472D+00 -2.1423D+00\n",
      "\n",
      "\n",
      "ITERATION    26\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.9583179544823713E-002\n",
      "\n",
      "At iterate   26    f=  2.79383D+03    |proj g|=  1.46308D+00\n",
      "\n",
      " X = -2.9065D-01 -4.7639D-01  5.3569D-01  1.4770D-03  4.1395D-02  9.7550D-03\n",
      "      1.4373D-01  2.0869D-04\n",
      "\n",
      " G =  3.2229D-01  2.6748D-01  1.8883D-01  3.4041D-01 -5.0517D-01  2.6666D-01\n",
      "      2.7012D-01 -1.4631D+00\n",
      "\n",
      "\n",
      "ITERATION    27\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.4628571186038743E-002\n",
      "\n",
      "At iterate   27    f=  2.79383D+03    |proj g|=  5.87019D-01\n",
      "\n",
      " X = -3.0132D-01 -4.7969D-01  5.4322D-01  1.3127D-03  4.1598D-02  1.0996D-02\n",
      "      1.4924D-01  1.0733D-03\n",
      "\n",
      " G =  4.2217D-01  3.7192D-01  3.3914D-01  4.7222D-01 -4.1098D-01  3.9516D-01\n",
      "      4.2249D-01 -5.8702D-01\n",
      "\n",
      "\n",
      "ITERATION    28\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.1671587540828020E-002\n",
      "\n",
      "At iterate   28    f=  2.79383D+03    |proj g|=  1.65048D-01\n",
      "\n",
      " X = -3.1046D-01 -4.8149D-01  5.4875D-01  1.1846D-03  4.2005D-02  1.2120D-02\n",
      "      1.5336D-01  1.6549D-03\n",
      "\n",
      " G =  8.4947D-02  4.7863D-02  3.8477D-02  1.6505D-01 -4.0663D-02  8.3986D-02\n",
      "      1.2743D-01  4.5901D-03\n",
      "\n",
      "\n",
      "ITERATION    29\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    6.3620915063097199E-004\n",
      "\n",
      "At iterate   29    f=  2.79383D+03    |proj g|=  6.59920D-02\n",
      "\n",
      " X = -3.1068D-01 -4.8183D-01  5.4923D-01  1.1795D-03  4.2032D-02  1.2159D-02\n",
      "      1.5330D-01  1.6706D-03\n",
      "\n",
      " G = -1.5072D-02 -5.2774D-02 -5.9262D-02  6.5992D-02 -1.2217D-02 -1.5284D-02\n",
      "      2.7342D-02  2.1344D-02\n",
      "\n",
      "\n",
      "ITERATION    30\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    5.1319873963163987E-004\n",
      "\n",
      "At iterate   30    f=  2.79383D+03    |proj g|=  1.47342D-01\n",
      "\n",
      " X = -3.1099D-01 -4.8195D-01  5.4961D-01  1.0681D-03  4.2046D-02  1.2203D-02\n",
      "      1.5333D-01  1.6683D-03\n",
      "\n",
      " G = -1.0487D-01 -1.4245D-01 -1.4734D-01 -2.2984D-02  6.1883D-03 -1.0430D-01\n",
      "     -6.1797D-02  1.9817D-02\n",
      "\n",
      "\n",
      "ITERATION    31\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    7.7490833210301549E-004\n",
      "\n",
      "At iterate   31    f=  2.79383D+03    |proj g|=  2.40634D-01\n",
      "\n",
      " X = -3.1141D-01 -4.8204D-01  5.5019D-01  8.0520D-04  4.2054D-02  1.2270D-02\n",
      "      1.5337D-01  1.6569D-03\n",
      "\n",
      " G = -2.0066D-01 -2.3837D-01 -2.4063D-01 -1.1811D-01  1.7351D-02 -1.9920D-01\n",
      "     -1.5676D-01  9.2984D-03\n",
      "\n",
      "\n",
      "ITERATION    32\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    2.0158537489129336E-003\n",
      "\n",
      "At iterate   32    f=  2.79383D+03    |proj g|=  3.96279D-01\n",
      "\n",
      " X = -3.1242D-01 -4.8216D-01  5.5171D-01 -2.1108D-05  4.2060D-02  1.2446D-02\n",
      "      1.5346D-01  1.6170D-03\n",
      "\n",
      " G = -3.5774D-01 -3.9628D-01 -3.9151D-01 -2.7449D-01  2.3118D-02 -3.5448D-01\n",
      "     -3.1209D-01 -2.9079D-02\n",
      "\n",
      "\n",
      "ITERATION    33\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    4.0862535930265113E-003\n",
      "\n",
      "At iterate   33    f=  2.79383D+03    |proj g|=  5.73086D-01\n",
      "\n",
      " X = -3.1429D-01 -4.8212D-01  5.5477D-01 -1.9315D-03  4.2065D-02  1.2810D-02\n",
      "      1.5359D-01  1.5220D-03\n",
      "\n",
      " G = -5.3238D-01 -5.7309D-01 -5.5439D-01 -4.4936D-01  1.6931D-02 -5.2631D-01\n",
      "     -4.8400D-01 -1.2201D-01\n",
      "\n",
      "\n",
      "ITERATION    34\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    7.6066996097870427E-003\n",
      "\n",
      "At iterate   34    f=  2.79383D+03    |proj g|=  7.03312D-01\n",
      "\n",
      " X = -3.1740D-01 -4.8143D-01  5.6032D-01 -5.9819D-03  4.2087D-02  1.3491D-02\n",
      "      1.5368D-01  1.3326D-03\n",
      "\n",
      " G = -6.5860D-01 -7.0331D-01 -6.6138D-01 -5.7923D-01  5.8925D-03 -6.4903D-01\n",
      "     -6.0700D-01 -3.0930D-01\n",
      "\n",
      "\n",
      "ITERATION    35\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.1410999429147565E-002\n",
      "\n",
      "At iterate   35    f=  2.79383D+03    |proj g|=  8.00826D-01\n",
      "\n",
      " X = -3.1945D-01 -4.7828D-01  5.6753D-01 -1.3823D-02  4.2066D-02  1.4289D-02\n",
      "      1.5230D-01  1.1280D-03\n",
      "\n",
      " G = -7.4840D-01 -8.0083D-01 -7.3609D-01 -6.8917D-01 -4.5521D-02 -7.4342D-01\n",
      "     -7.0655D-01 -5.1239D-01\n",
      "\n",
      "\n",
      "ITERATION    36\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.9969156012722609E-002\n",
      "\n",
      "At iterate   36    f=  2.79382D+03    |proj g|=  7.03297D-01\n",
      "\n",
      " X = -3.1997D-01 -4.7181D-01  5.7912D-01 -2.7968D-02  4.2553D-02  1.5539D-02\n",
      "      1.4778D-01  1.1725D-03\n",
      "\n",
      " G = -6.3860D-01 -7.0330D-01 -6.1518D-01 -6.2481D-01  3.6990D-01 -6.4930D-01\n",
      "     -6.3175D-01 -4.6639D-01\n",
      "\n",
      "\n",
      "ITERATION    37\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           1  times; norm of step =    9.4374894065155489E-003\n",
      "\n",
      "At iterate   37    f=  2.79382D+03    |proj g|=  7.72154D-01\n",
      "\n",
      " X = -3.1655D-01 -4.6635D-01  5.7930D-01 -3.4291D-02  4.2005D-02  1.5363D-02\n",
      "      1.4510D-01  1.0784D-03\n",
      "\n",
      " G = -7.0059D-01 -7.7215D-01 -6.9457D-01 -7.2036D-01 -1.1796D-01 -7.3054D-01\n",
      "     -7.1977D-01 -5.6246D-01\n",
      "\n",
      "\n",
      "ITERATION    38\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    9.7104563997675118E-003\n",
      "\n",
      "At iterate   38    f=  2.79382D+03    |proj g|=  3.27472D-01\n",
      "\n",
      " X = -3.2045D-01 -4.6365D-01  5.8553D-01 -3.9899D-02  4.1885D-02  1.6433D-02\n",
      "      1.4517D-01  1.7257D-03\n",
      "\n",
      " G = -1.3564D-01 -2.0924D-01 -1.0735D-01 -1.6103D-01 -3.2747D-01 -1.6042D-01\n",
      "     -1.5168D-01  8.8316D-02\n",
      "\n",
      "\n",
      "ITERATION    39\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    5.2860067610985420E-003\n",
      "\n",
      "At iterate   39    f=  2.79382D+03    |proj g|=  1.99213D-01\n",
      "\n",
      " X = -3.1787D-01 -4.6233D-01  5.8131D-01 -3.8696D-02  4.1970D-02  1.5928D-02\n",
      "      1.4489D-01  1.6569D-03\n",
      "\n",
      " G = -5.3093D-02 -1.2065D-01 -4.6243D-02 -8.3525D-02 -1.9921D-01 -8.4325D-02\n",
      "     -7.3584D-02  1.5458D-02\n",
      "\n",
      "\n",
      "ITERATION    40\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    6.6025159484375019E-003\n",
      "\n",
      "At iterate   40    f=  2.79382D+03    |proj g|=  7.76798D-02\n",
      "\n",
      " X = -3.1496D-01 -4.5937D-01  5.7623D-01 -3.8459D-02  4.2102D-02  1.5383D-02\n",
      "      1.4456D-01  1.7112D-03\n",
      "\n",
      " G =  7.7680D-02  2.0306D-02  5.5360D-02  3.7435D-02 -4.9662D-03  3.7374D-02\n",
      "      5.1988D-02  6.5149D-02\n",
      "\n",
      "\n",
      "ITERATION    41\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    8.9365343768281458E-004\n",
      "\n",
      "At iterate   41    f=  2.79382D+03    |proj g|=  1.07336D-01\n",
      "\n",
      " X = -3.1529D-01 -4.5869D-01  5.7602D-01 -3.8833D-02  4.2109D-02  1.5425D-02\n",
      "      1.4478D-01  1.7052D-03\n",
      "\n",
      " G =  1.0734D-01  5.2545D-02  8.3516D-02  6.6807D-02  8.0194D-03  6.7367D-02\n",
      "      8.3648D-02  5.8594D-02\n",
      "\n",
      "\n",
      "ITERATION    42\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    7.6733284305869438E-003\n",
      "\n",
      "At iterate   42    f=  2.79382D+03    |proj g|=  1.58958D-01\n",
      "\n",
      " X = -3.1964D-01 -4.5385D-01  5.7606D-01 -4.2145D-02  4.2120D-02  1.5993D-02\n",
      "      1.4708D-01  1.6750D-03\n",
      "\n",
      " G =  1.5896D-01  1.2455D-01  1.3315D-01  1.2114D-01  6.4319D-02  1.2632D-01\n",
      "      1.5706D-01  2.6321D-02\n",
      "\n",
      "\n",
      "ITERATION    43\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           1  times; norm of step =    1.3705475175837625E-003\n",
      "\n",
      "At iterate   43    f=  2.79382D+03    |proj g|=  1.07438D-01\n",
      "\n",
      " X = -3.2028D-01 -4.5295D-01  5.7619D-01 -4.2907D-02  4.2141D-02  1.6094D-02\n",
      "      1.4724D-01  1.7551D-03\n",
      "\n",
      " G =  4.6008D-02  1.5245D-02  1.9532D-02  8.0674D-03  1.0371D-01  1.4251D-02\n",
      "      4.6207D-02  1.0744D-01\n",
      "\n",
      "\n",
      "ITERATION    44\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    3.7282254034126860E-003\n",
      "\n",
      "At iterate   44    f=  2.79382D+03    |proj g|=  4.94281D-02\n",
      "\n",
      " X = -3.2322D-01 -4.5197D-01  5.7735D-01 -4.3868D-02  4.2076D-02  1.6484D-02\n",
      "      1.4861D-01  1.6964D-03\n",
      "\n",
      " G =  3.5952D-02  1.1795D-02  1.7717D-02  4.5920D-03  4.0880D-02  1.1317D-02\n",
      "      4.9428D-02  4.8906D-02\n",
      "\n",
      "\n",
      "ITERATION    45\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.4376659884931882E-003\n",
      "\n",
      "At iterate   45    f=  2.79382D+03    |proj g|=  2.71584D-02\n",
      "\n",
      " X = -3.2423D-01 -4.5184D-01  5.7828D-01 -4.4213D-02  4.2067D-02  1.6715D-02\n",
      "      1.4863D-01  1.6522D-03\n",
      "\n",
      " G =  3.8045D-03 -1.8709D-02 -9.1025D-03 -2.4237D-02  2.7158D-02 -1.7466D-02\n",
      "      1.8720D-02  5.0302D-03\n",
      "\n",
      "\n",
      "ITERATION    46\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.7361820254178535E-003\n",
      "\n",
      "At iterate   46    f=  2.79382D+03    |proj g|=  6.17082D-02\n",
      "\n",
      " X = -3.2556D-01 -4.5178D-01  5.7929D-01 -4.4133D-02  4.2036D-02  1.7088D-02\n",
      "      1.4839D-01  1.6090D-03\n",
      "\n",
      " G = -4.1311D-02 -6.0247D-02 -4.6203D-02 -6.1708D-02 -9.1515D-03 -5.6253D-02\n",
      "     -2.6059D-02 -3.7692D-02\n",
      "\n",
      "\n",
      "ITERATION    47\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.4717535574455566E-003\n",
      "\n",
      "At iterate   47    f=  2.79382D+03    |proj g|=  7.86628D-02\n",
      "\n",
      " X = -3.2656D-01 -4.5139D-01  5.7989D-01 -4.3891D-02  4.2019D-02  1.7496D-02\n",
      "      1.4772D-01  1.5866D-03\n",
      "\n",
      " G = -6.5073D-02 -7.8663D-02 -6.3826D-02 -7.7204D-02 -2.7968D-02 -7.3456D-02\n",
      "     -5.2735D-02 -6.0258D-02\n",
      "\n",
      "\n",
      "ITERATION    48\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           0  times; norm of step =    1.9602331368869935E-003\n",
      "\n",
      "At iterate   48    f=  2.79382D+03    |proj g|=  6.28439D-02\n",
      "\n",
      " X = -3.2713D-01 -4.5023D-01  5.7963D-01 -4.3406D-02  4.1979D-02  1.7974D-02\n",
      "      1.4645D-01  1.5942D-03\n",
      "\n",
      " G = -5.8465D-02 -6.2844D-02 -5.4365D-02 -6.0781D-02 -5.8307D-02 -5.9475D-02\n",
      "     -5.3510D-02 -5.3548D-02\n",
      "\n",
      "\n",
      "ITERATION    49\n",
      "\n",
      "----------------SUBSM entered-----------------\n",
      "\n",
      "\n",
      "----------------exit SUBSM --------------------\n",
      "\n",
      " LINE SEARCH           1  times; norm of step =    6.1136022767847907E-004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(n_jobs=1, random_state=0, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(n_jobs=1, random_state=0, verbose=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(n_jobs=1, random_state=0, verbose=10)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   49    f=  2.79382D+03    |proj g|=  4.38745D-02\n",
      "\n",
      " X = -3.2750D-01 -4.5016D-01  5.8006D-01 -4.3594D-02  4.1999D-02  1.8075D-02\n",
      "      1.4643D-01  1.6202D-03\n",
      "\n",
      " G = -3.5085D-02 -3.8892D-02 -2.8948D-02 -3.6401D-02 -4.3875D-02 -3.4884D-02\n",
      "     -2.9824D-02 -2.7419D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "    8     49     55      1     0     0   4.387D-02   2.794D+03\n",
      "\n",
      " X = -3.2750D-01 -4.5016D-01  5.8006D-01 -4.3594D-02  4.1999D-02  1.8075D-02\n",
      "      1.4643D-01  1.6202D-03\n",
      "  F =   2793.8234426823992     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paradoxtown/miniconda3/envs/times/lib/python3.8/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5597147950089126"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:1000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_cl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4835262689225289"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:100]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.70825623\n",
      "Iteration 2, loss = 0.70820678\n",
      "Iteration 3, loss = 0.70815769\n",
      "Iteration 4, loss = 0.70810908\n",
      "Iteration 5, loss = 0.70805787\n",
      "Iteration 6, loss = 0.70800589\n",
      "Iteration 7, loss = 0.70795551\n",
      "Iteration 8, loss = 0.70790495\n",
      "Iteration 9, loss = 0.70785538\n",
      "Iteration 10, loss = 0.70780466\n",
      "Iteration 11, loss = 0.70775703\n",
      "Iteration 12, loss = 0.70771103\n",
      "Iteration 13, loss = 0.70766388\n",
      "Iteration 14, loss = 0.70761368\n",
      "Iteration 15, loss = 0.70756539\n",
      "Iteration 16, loss = 0.70751563\n",
      "Iteration 17, loss = 0.70746957\n",
      "Iteration 18, loss = 0.70742193\n",
      "Iteration 19, loss = 0.70737621\n",
      "Iteration 20, loss = 0.70732992\n",
      "Iteration 21, loss = 0.70728269\n",
      "Iteration 22, loss = 0.70723240\n",
      "Iteration 23, loss = 0.70718617\n",
      "Iteration 24, loss = 0.70713762\n",
      "Iteration 25, loss = 0.70709141\n",
      "Iteration 26, loss = 0.70704263\n",
      "Iteration 27, loss = 0.70699279\n",
      "Iteration 28, loss = 0.70694395\n",
      "Iteration 29, loss = 0.70689784\n",
      "Iteration 30, loss = 0.70685200\n",
      "Iteration 31, loss = 0.70680528\n",
      "Iteration 32, loss = 0.70675826\n",
      "Iteration 33, loss = 0.70670998\n",
      "Iteration 34, loss = 0.70666457\n",
      "Iteration 35, loss = 0.70661782\n",
      "Iteration 36, loss = 0.70657548\n",
      "Iteration 37, loss = 0.70652825\n",
      "Iteration 38, loss = 0.70648109\n",
      "Iteration 39, loss = 0.70643591\n",
      "Iteration 40, loss = 0.70638774\n",
      "Iteration 41, loss = 0.70634542\n",
      "Iteration 42, loss = 0.70629894\n",
      "Iteration 43, loss = 0.70625539\n",
      "Iteration 44, loss = 0.70621188\n",
      "Iteration 45, loss = 0.70616644\n",
      "Iteration 46, loss = 0.70612219\n",
      "Iteration 47, loss = 0.70607630\n",
      "Iteration 48, loss = 0.70603199\n",
      "Iteration 49, loss = 0.70598994\n",
      "Iteration 50, loss = 0.70594386\n",
      "Iteration 51, loss = 0.70590214\n",
      "Iteration 52, loss = 0.70585513\n",
      "Iteration 53, loss = 0.70581328\n",
      "Iteration 54, loss = 0.70576989\n",
      "Iteration 55, loss = 0.70572980\n",
      "Iteration 56, loss = 0.70568985\n",
      "Iteration 57, loss = 0.70564498\n",
      "Iteration 58, loss = 0.70560274\n",
      "Iteration 59, loss = 0.70556258\n",
      "Iteration 60, loss = 0.70551876\n",
      "Iteration 61, loss = 0.70547733\n",
      "Iteration 62, loss = 0.70543387\n",
      "Iteration 63, loss = 0.70539454\n",
      "Iteration 64, loss = 0.70535413\n",
      "Iteration 65, loss = 0.70531204\n",
      "Iteration 66, loss = 0.70527064\n",
      "Iteration 67, loss = 0.70523137\n",
      "Iteration 68, loss = 0.70519048\n",
      "Iteration 69, loss = 0.70514796\n",
      "Iteration 70, loss = 0.70510692\n",
      "Iteration 71, loss = 0.70506485\n",
      "Iteration 72, loss = 0.70502269\n",
      "Iteration 73, loss = 0.70498228\n",
      "Iteration 74, loss = 0.70494165\n",
      "Iteration 75, loss = 0.70490044\n",
      "Iteration 76, loss = 0.70485932\n",
      "Iteration 77, loss = 0.70482089\n",
      "Iteration 78, loss = 0.70478385\n",
      "Iteration 79, loss = 0.70474546\n",
      "Iteration 80, loss = 0.70470793\n",
      "Iteration 81, loss = 0.70466955\n",
      "Iteration 82, loss = 0.70463155\n",
      "Iteration 83, loss = 0.70459502\n",
      "Iteration 84, loss = 0.70455754\n",
      "Iteration 85, loss = 0.70452120\n",
      "Iteration 86, loss = 0.70448224\n",
      "Iteration 87, loss = 0.70444362\n",
      "Iteration 88, loss = 0.70440462\n",
      "Iteration 89, loss = 0.70436364\n",
      "Iteration 90, loss = 0.70432423\n",
      "Iteration 91, loss = 0.70428535\n",
      "Iteration 92, loss = 0.70424779\n",
      "Iteration 93, loss = 0.70420718\n",
      "Iteration 94, loss = 0.70416838\n",
      "Iteration 95, loss = 0.70412745\n",
      "Iteration 96, loss = 0.70408938\n",
      "Iteration 97, loss = 0.70405293\n",
      "Iteration 98, loss = 0.70401448\n",
      "Iteration 99, loss = 0.70397864\n",
      "Iteration 100, loss = 0.70394239\n",
      "Iteration 101, loss = 0.70390559\n",
      "Iteration 102, loss = 0.70386811\n",
      "Iteration 103, loss = 0.70383045\n",
      "Iteration 104, loss = 0.70379572\n",
      "Iteration 105, loss = 0.70376051\n",
      "Iteration 106, loss = 0.70372221\n",
      "Iteration 107, loss = 0.70368707\n",
      "Iteration 108, loss = 0.70365359\n",
      "Iteration 109, loss = 0.70361669\n",
      "Iteration 110, loss = 0.70358436\n",
      "Iteration 111, loss = 0.70354971\n",
      "Iteration 112, loss = 0.70351413\n",
      "Iteration 113, loss = 0.70347847\n",
      "Iteration 114, loss = 0.70344356\n",
      "Iteration 115, loss = 0.70340878\n",
      "Iteration 116, loss = 0.70337292\n",
      "Iteration 117, loss = 0.70333723\n",
      "Iteration 118, loss = 0.70330253\n",
      "Iteration 119, loss = 0.70326715\n",
      "Iteration 120, loss = 0.70323306\n",
      "Iteration 121, loss = 0.70319815\n",
      "Iteration 122, loss = 0.70316374\n",
      "Iteration 123, loss = 0.70313070\n",
      "Iteration 124, loss = 0.70309561\n",
      "Iteration 125, loss = 0.70306268\n",
      "Iteration 126, loss = 0.70302446\n",
      "Iteration 127, loss = 0.70299219\n",
      "Iteration 128, loss = 0.70296043\n",
      "Iteration 129, loss = 0.70292940\n",
      "Iteration 130, loss = 0.70289679\n",
      "Iteration 131, loss = 0.70286542\n",
      "Iteration 132, loss = 0.70283085\n",
      "Iteration 133, loss = 0.70279838\n",
      "Iteration 134, loss = 0.70276735\n",
      "Iteration 135, loss = 0.70273612\n",
      "Iteration 136, loss = 0.70270504\n",
      "Iteration 137, loss = 0.70267143\n",
      "Iteration 138, loss = 0.70263751\n",
      "Iteration 139, loss = 0.70260546\n",
      "Iteration 140, loss = 0.70257388\n",
      "Iteration 141, loss = 0.70254229\n",
      "Iteration 142, loss = 0.70250914\n",
      "Iteration 143, loss = 0.70247738\n",
      "Iteration 144, loss = 0.70244552\n",
      "Iteration 145, loss = 0.70241407\n",
      "Iteration 146, loss = 0.70237900\n",
      "Iteration 147, loss = 0.70234972\n",
      "Iteration 148, loss = 0.70231628\n",
      "Iteration 149, loss = 0.70228515\n",
      "Iteration 150, loss = 0.70225327\n",
      "Iteration 151, loss = 0.70222038\n",
      "Iteration 152, loss = 0.70219036\n",
      "Iteration 153, loss = 0.70215887\n",
      "Iteration 154, loss = 0.70213067\n",
      "Iteration 155, loss = 0.70209739\n",
      "Iteration 156, loss = 0.70206653\n",
      "Iteration 157, loss = 0.70203470\n",
      "Iteration 158, loss = 0.70200438\n",
      "Iteration 159, loss = 0.70197297\n",
      "Iteration 160, loss = 0.70194234\n",
      "Iteration 161, loss = 0.70191032\n",
      "Iteration 162, loss = 0.70188066\n",
      "Iteration 163, loss = 0.70185188\n",
      "Iteration 164, loss = 0.70182416\n",
      "Iteration 165, loss = 0.70179533\n",
      "Iteration 166, loss = 0.70176864\n",
      "Iteration 167, loss = 0.70173923\n",
      "Iteration 168, loss = 0.70171123\n",
      "Iteration 169, loss = 0.70167981\n",
      "Iteration 170, loss = 0.70165343\n",
      "Iteration 171, loss = 0.70162229\n",
      "Iteration 172, loss = 0.70159429\n",
      "Iteration 173, loss = 0.70156323\n",
      "Iteration 174, loss = 0.70153595\n",
      "Iteration 175, loss = 0.70150931\n",
      "Iteration 176, loss = 0.70148235\n",
      "Iteration 177, loss = 0.70145757\n",
      "Iteration 178, loss = 0.70142822\n",
      "Iteration 179, loss = 0.70140149\n",
      "Iteration 180, loss = 0.70137214\n",
      "Iteration 181, loss = 0.70134529\n",
      "Iteration 182, loss = 0.70131707\n",
      "Iteration 183, loss = 0.70129036\n",
      "Iteration 184, loss = 0.70126250\n",
      "Iteration 185, loss = 0.70123447\n",
      "Iteration 186, loss = 0.70120711\n",
      "Iteration 187, loss = 0.70117882\n",
      "Iteration 188, loss = 0.70114899\n",
      "Iteration 189, loss = 0.70112206\n",
      "Iteration 190, loss = 0.70109534\n",
      "Iteration 191, loss = 0.70106958\n",
      "Iteration 192, loss = 0.70104312\n",
      "Iteration 193, loss = 0.70101600\n",
      "Iteration 194, loss = 0.70099128\n",
      "Iteration 195, loss = 0.70096519\n",
      "Iteration 196, loss = 0.70093863\n",
      "Iteration 197, loss = 0.70091305\n",
      "Iteration 198, loss = 0.70088704\n",
      "Iteration 199, loss = 0.70085987\n",
      "Iteration 200, loss = 0.70083140\n",
      "Iteration 201, loss = 0.70080502\n",
      "Iteration 202, loss = 0.70078037\n",
      "Iteration 203, loss = 0.70075538\n",
      "Iteration 204, loss = 0.70072886\n",
      "Iteration 205, loss = 0.70070297\n",
      "Iteration 206, loss = 0.70067788\n",
      "Iteration 207, loss = 0.70065274\n",
      "Iteration 208, loss = 0.70062821\n",
      "Iteration 209, loss = 0.70060229\n",
      "Iteration 210, loss = 0.70057698\n",
      "Iteration 211, loss = 0.70055256\n",
      "Iteration 212, loss = 0.70052615\n",
      "Iteration 213, loss = 0.70050083\n",
      "Iteration 214, loss = 0.70047796\n",
      "Iteration 215, loss = 0.70045220\n",
      "Iteration 216, loss = 0.70042765\n",
      "Iteration 217, loss = 0.70040307\n",
      "Iteration 218, loss = 0.70037945\n",
      "Iteration 219, loss = 0.70035473\n",
      "Iteration 220, loss = 0.70033182\n",
      "Iteration 221, loss = 0.70030935\n",
      "Iteration 222, loss = 0.70028518\n",
      "Iteration 223, loss = 0.70026069\n",
      "Iteration 224, loss = 0.70023627\n",
      "Iteration 225, loss = 0.70021215\n",
      "Iteration 226, loss = 0.70019009\n",
      "Iteration 227, loss = 0.70016657\n",
      "Iteration 228, loss = 0.70014372\n",
      "Iteration 229, loss = 0.70012187\n",
      "Iteration 230, loss = 0.70009798\n",
      "Iteration 231, loss = 0.70007603\n",
      "Iteration 232, loss = 0.70005074\n",
      "Iteration 233, loss = 0.70002817\n",
      "Iteration 234, loss = 0.70000537\n",
      "Iteration 235, loss = 0.69998170\n",
      "Iteration 236, loss = 0.69995904\n",
      "Iteration 237, loss = 0.69993606\n",
      "Iteration 238, loss = 0.69991272\n",
      "Iteration 239, loss = 0.69989248\n",
      "Iteration 240, loss = 0.69986861\n",
      "Iteration 241, loss = 0.69984548\n",
      "Iteration 242, loss = 0.69982238\n",
      "Iteration 243, loss = 0.69980080\n",
      "Iteration 244, loss = 0.69977790\n",
      "Iteration 245, loss = 0.69975696\n",
      "Iteration 246, loss = 0.69973536\n",
      "Iteration 247, loss = 0.69971411\n",
      "Iteration 248, loss = 0.69969173\n",
      "Iteration 249, loss = 0.69967039\n",
      "Iteration 250, loss = 0.69964856\n",
      "Iteration 251, loss = 0.69962781\n",
      "Iteration 252, loss = 0.69960789\n",
      "Iteration 253, loss = 0.69958857\n",
      "Iteration 254, loss = 0.69956854\n",
      "Iteration 255, loss = 0.69954727\n",
      "Iteration 256, loss = 0.69952693\n",
      "Iteration 257, loss = 0.69950759\n",
      "Iteration 258, loss = 0.69948743\n",
      "Iteration 259, loss = 0.69946763\n",
      "Iteration 260, loss = 0.69944583\n",
      "Iteration 261, loss = 0.69942807\n",
      "Iteration 262, loss = 0.69940545\n",
      "Iteration 263, loss = 0.69938502\n",
      "Iteration 264, loss = 0.69936713\n",
      "Iteration 265, loss = 0.69934774\n",
      "Iteration 266, loss = 0.69932462\n",
      "Iteration 267, loss = 0.69930545\n",
      "Iteration 268, loss = 0.69928446\n",
      "Iteration 269, loss = 0.69926421\n",
      "Iteration 270, loss = 0.69924586\n",
      "Iteration 271, loss = 0.69922728\n",
      "Iteration 272, loss = 0.69920711\n",
      "Iteration 273, loss = 0.69918720\n",
      "Iteration 274, loss = 0.69916704\n",
      "Iteration 275, loss = 0.69914818\n",
      "Iteration 276, loss = 0.69912881\n",
      "Iteration 277, loss = 0.69910817\n",
      "Iteration 278, loss = 0.69908882\n",
      "Iteration 279, loss = 0.69906999\n",
      "Iteration 280, loss = 0.69905058\n",
      "Iteration 281, loss = 0.69903194\n",
      "Iteration 282, loss = 0.69901095\n",
      "Iteration 283, loss = 0.69899204\n",
      "Iteration 284, loss = 0.69897370\n",
      "Iteration 285, loss = 0.69895521\n",
      "Iteration 286, loss = 0.69893513\n",
      "Iteration 287, loss = 0.69891661\n",
      "Iteration 288, loss = 0.69889858\n",
      "Iteration 289, loss = 0.69888098\n",
      "Iteration 290, loss = 0.69886192\n",
      "Iteration 291, loss = 0.69884313\n",
      "Iteration 292, loss = 0.69882282\n",
      "Iteration 293, loss = 0.69880446\n",
      "Iteration 294, loss = 0.69878566\n",
      "Iteration 295, loss = 0.69876889\n",
      "Iteration 296, loss = 0.69875032\n",
      "Iteration 297, loss = 0.69873115\n",
      "Iteration 298, loss = 0.69871310\n",
      "Iteration 299, loss = 0.69869394\n",
      "Iteration 300, loss = 0.69867706\n",
      "Iteration 301, loss = 0.69865894\n",
      "Iteration 302, loss = 0.69864186\n",
      "Iteration 303, loss = 0.69862328\n",
      "Iteration 304, loss = 0.69860577\n",
      "Iteration 305, loss = 0.69858802\n",
      "Iteration 306, loss = 0.69857204\n",
      "Iteration 307, loss = 0.69855525\n",
      "Iteration 308, loss = 0.69853751\n",
      "Iteration 309, loss = 0.69851941\n",
      "Iteration 310, loss = 0.69850088\n",
      "Iteration 311, loss = 0.69848184\n",
      "Iteration 312, loss = 0.69846401\n",
      "Iteration 313, loss = 0.69844554\n",
      "Iteration 314, loss = 0.69842972\n",
      "Iteration 315, loss = 0.69841240\n",
      "Iteration 316, loss = 0.69839500\n",
      "Iteration 317, loss = 0.69837856\n",
      "Iteration 318, loss = 0.69836234\n",
      "Iteration 319, loss = 0.69834570\n",
      "Iteration 320, loss = 0.69833002\n",
      "Iteration 321, loss = 0.69831209\n",
      "Iteration 322, loss = 0.69829589\n",
      "Iteration 323, loss = 0.69828054\n",
      "Iteration 324, loss = 0.69826399\n",
      "Iteration 325, loss = 0.69824791\n",
      "Iteration 326, loss = 0.69823283\n",
      "Iteration 327, loss = 0.69821745\n",
      "Iteration 328, loss = 0.69820182\n",
      "Iteration 329, loss = 0.69818515\n",
      "Iteration 330, loss = 0.69816922\n",
      "Iteration 331, loss = 0.69815203\n",
      "Iteration 332, loss = 0.69813583\n",
      "Iteration 333, loss = 0.69811985\n",
      "Iteration 334, loss = 0.69810220\n",
      "Iteration 335, loss = 0.69808634\n",
      "Iteration 336, loss = 0.69807003\n",
      "Iteration 337, loss = 0.69805374\n",
      "Iteration 338, loss = 0.69803824\n",
      "Iteration 339, loss = 0.69802340\n",
      "Iteration 340, loss = 0.69800846\n",
      "Iteration 341, loss = 0.69799271\n",
      "Iteration 342, loss = 0.69797700\n",
      "Iteration 343, loss = 0.69796283\n",
      "Iteration 344, loss = 0.69794941\n",
      "Iteration 345, loss = 0.69793435\n",
      "Iteration 346, loss = 0.69792048\n",
      "Iteration 347, loss = 0.69790623\n",
      "Iteration 348, loss = 0.69789331\n",
      "Iteration 349, loss = 0.69787773\n",
      "Iteration 350, loss = 0.69786241\n",
      "Iteration 351, loss = 0.69784828\n",
      "Iteration 352, loss = 0.69783328\n",
      "Iteration 353, loss = 0.69781780\n",
      "Iteration 354, loss = 0.69780369\n",
      "Iteration 355, loss = 0.69778900\n",
      "Iteration 356, loss = 0.69777416\n",
      "Iteration 357, loss = 0.69776021\n",
      "Iteration 358, loss = 0.69774674\n",
      "Iteration 359, loss = 0.69773182\n",
      "Iteration 360, loss = 0.69771694\n",
      "Iteration 361, loss = 0.69770195\n",
      "Iteration 362, loss = 0.69768835\n",
      "Iteration 363, loss = 0.69767333\n",
      "Iteration 364, loss = 0.69765992\n",
      "Iteration 365, loss = 0.69764510\n",
      "Iteration 366, loss = 0.69763232\n",
      "Iteration 367, loss = 0.69761739\n",
      "Iteration 368, loss = 0.69760407\n",
      "Iteration 369, loss = 0.69759147\n",
      "Iteration 370, loss = 0.69757783\n",
      "Iteration 371, loss = 0.69756395\n",
      "Iteration 372, loss = 0.69754904\n",
      "Iteration 373, loss = 0.69753440\n",
      "Iteration 374, loss = 0.69752102\n",
      "Iteration 375, loss = 0.69750659\n",
      "Iteration 376, loss = 0.69749239\n",
      "Iteration 377, loss = 0.69747860\n",
      "Iteration 378, loss = 0.69746471\n",
      "Iteration 379, loss = 0.69745084\n",
      "Iteration 380, loss = 0.69743635\n",
      "Iteration 381, loss = 0.69742204\n",
      "Iteration 382, loss = 0.69740877\n",
      "Iteration 383, loss = 0.69739458\n",
      "Iteration 384, loss = 0.69737924\n",
      "Iteration 385, loss = 0.69736567\n",
      "Iteration 386, loss = 0.69735202\n",
      "Iteration 387, loss = 0.69733763\n",
      "Iteration 388, loss = 0.69732484\n",
      "Iteration 389, loss = 0.69731225\n",
      "Iteration 390, loss = 0.69729820\n",
      "Iteration 391, loss = 0.69728489\n",
      "Iteration 392, loss = 0.69727296\n",
      "Iteration 393, loss = 0.69725920\n",
      "Iteration 394, loss = 0.69724457\n",
      "Iteration 395, loss = 0.69723122\n",
      "Iteration 396, loss = 0.69721624\n",
      "Iteration 397, loss = 0.69720291\n",
      "Iteration 398, loss = 0.69719101\n",
      "Iteration 399, loss = 0.69717821\n",
      "Iteration 400, loss = 0.69716635\n",
      "Iteration 401, loss = 0.69715336\n",
      "Iteration 402, loss = 0.69714163\n",
      "Iteration 403, loss = 0.69712790\n",
      "Iteration 404, loss = 0.69711525\n",
      "Iteration 405, loss = 0.69710250\n",
      "Iteration 406, loss = 0.69708966\n",
      "Iteration 407, loss = 0.69707781\n",
      "Iteration 408, loss = 0.69706551\n",
      "Iteration 409, loss = 0.69705418\n",
      "Iteration 410, loss = 0.69704162\n",
      "Iteration 411, loss = 0.69703277\n",
      "Iteration 412, loss = 0.69701784\n",
      "Iteration 413, loss = 0.69700530\n",
      "Iteration 414, loss = 0.69699521\n",
      "Iteration 415, loss = 0.69698305\n",
      "Iteration 416, loss = 0.69697148\n",
      "Iteration 417, loss = 0.69695926\n",
      "Iteration 418, loss = 0.69694839\n",
      "Iteration 419, loss = 0.69693659\n",
      "Iteration 420, loss = 0.69692778\n",
      "Iteration 421, loss = 0.69691672\n",
      "Iteration 422, loss = 0.69690617\n",
      "Iteration 423, loss = 0.69689524\n",
      "Iteration 424, loss = 0.69688317\n",
      "Iteration 425, loss = 0.69687127\n",
      "Iteration 426, loss = 0.69686001\n",
      "Iteration 427, loss = 0.69684959\n",
      "Iteration 428, loss = 0.69683722\n",
      "Iteration 429, loss = 0.69682668\n",
      "Iteration 430, loss = 0.69681511\n",
      "Iteration 431, loss = 0.69680508\n",
      "Iteration 432, loss = 0.69679461\n",
      "Iteration 433, loss = 0.69678355\n",
      "Iteration 434, loss = 0.69677345\n",
      "Iteration 435, loss = 0.69676417\n",
      "Iteration 436, loss = 0.69675344\n",
      "Iteration 437, loss = 0.69674189\n",
      "Iteration 438, loss = 0.69672909\n",
      "Iteration 439, loss = 0.69671814\n",
      "Iteration 440, loss = 0.69670630\n",
      "Iteration 441, loss = 0.69669687\n",
      "Iteration 442, loss = 0.69668623\n",
      "Iteration 443, loss = 0.69667435\n",
      "Iteration 444, loss = 0.69666321\n",
      "Iteration 445, loss = 0.69665335\n",
      "Iteration 446, loss = 0.69664207\n",
      "Iteration 447, loss = 0.69663058\n",
      "Iteration 448, loss = 0.69662073\n",
      "Iteration 449, loss = 0.69660952\n",
      "Iteration 450, loss = 0.69659873\n",
      "Iteration 451, loss = 0.69658810\n",
      "Iteration 452, loss = 0.69657806\n",
      "Iteration 453, loss = 0.69656741\n",
      "Iteration 454, loss = 0.69655541\n",
      "Iteration 455, loss = 0.69654596\n",
      "Iteration 456, loss = 0.69653255\n",
      "Iteration 457, loss = 0.69652173\n",
      "Iteration 458, loss = 0.69651046\n",
      "Iteration 459, loss = 0.69650083\n",
      "Iteration 460, loss = 0.69649227\n",
      "Iteration 461, loss = 0.69648189\n",
      "Iteration 462, loss = 0.69647170\n",
      "Iteration 463, loss = 0.69646171\n",
      "Iteration 464, loss = 0.69645251\n",
      "Iteration 465, loss = 0.69644215\n",
      "Iteration 466, loss = 0.69643205\n",
      "Iteration 467, loss = 0.69642096\n",
      "Iteration 468, loss = 0.69641023\n",
      "Iteration 469, loss = 0.69639957\n",
      "Iteration 470, loss = 0.69639076\n",
      "Iteration 471, loss = 0.69637964\n",
      "Iteration 472, loss = 0.69637000\n",
      "Iteration 473, loss = 0.69635963\n",
      "Iteration 474, loss = 0.69634937\n",
      "Iteration 475, loss = 0.69633900\n",
      "Iteration 476, loss = 0.69632924\n",
      "Iteration 477, loss = 0.69632024\n",
      "Iteration 478, loss = 0.69631046\n",
      "Iteration 479, loss = 0.69630194\n",
      "Iteration 480, loss = 0.69629343\n",
      "Iteration 481, loss = 0.69628504\n",
      "Iteration 482, loss = 0.69627528\n",
      "Iteration 483, loss = 0.69626627\n",
      "Iteration 484, loss = 0.69625774\n",
      "Iteration 485, loss = 0.69624891\n",
      "Iteration 486, loss = 0.69624175\n",
      "Iteration 487, loss = 0.69623245\n",
      "Iteration 488, loss = 0.69622460\n",
      "Iteration 489, loss = 0.69621533\n",
      "Iteration 490, loss = 0.69620642\n",
      "Iteration 491, loss = 0.69619771\n",
      "Iteration 492, loss = 0.69618773\n",
      "Iteration 493, loss = 0.69617836\n",
      "Iteration 494, loss = 0.69617037\n",
      "Iteration 495, loss = 0.69616077\n",
      "Iteration 496, loss = 0.69615030\n",
      "Iteration 497, loss = 0.69614209\n",
      "Iteration 498, loss = 0.69613323\n",
      "Iteration 499, loss = 0.69612438\n",
      "Iteration 500, loss = 0.69611543\n",
      "Iteration 501, loss = 0.69610460\n",
      "Iteration 502, loss = 0.69609501\n",
      "Iteration 503, loss = 0.69608585\n",
      "Iteration 504, loss = 0.69607805\n",
      "Iteration 505, loss = 0.69606859\n",
      "Iteration 506, loss = 0.69606047\n",
      "Iteration 507, loss = 0.69605226\n",
      "Iteration 508, loss = 0.69604412\n",
      "Iteration 509, loss = 0.69603652\n",
      "Iteration 510, loss = 0.69602854\n",
      "Iteration 511, loss = 0.69602085\n",
      "Iteration 512, loss = 0.69601028\n",
      "Iteration 513, loss = 0.69600229\n",
      "Iteration 514, loss = 0.69599451\n",
      "Iteration 515, loss = 0.69598682\n",
      "Iteration 516, loss = 0.69597811\n",
      "Iteration 517, loss = 0.69597062\n",
      "Iteration 518, loss = 0.69596222\n",
      "Iteration 519, loss = 0.69595334\n",
      "Iteration 520, loss = 0.69594642\n",
      "Iteration 521, loss = 0.69593824\n",
      "Iteration 522, loss = 0.69593101\n",
      "Iteration 523, loss = 0.69592321\n",
      "Iteration 524, loss = 0.69591434\n",
      "Iteration 525, loss = 0.69590717\n",
      "Iteration 526, loss = 0.69589908\n",
      "Iteration 527, loss = 0.69589175\n",
      "Iteration 528, loss = 0.69588428\n",
      "Iteration 529, loss = 0.69587566\n",
      "Iteration 530, loss = 0.69586616\n",
      "Iteration 531, loss = 0.69585871\n",
      "Iteration 532, loss = 0.69584943\n",
      "Iteration 533, loss = 0.69584128\n",
      "Iteration 534, loss = 0.69583440\n",
      "Iteration 535, loss = 0.69582644\n",
      "Iteration 536, loss = 0.69581908\n",
      "Iteration 537, loss = 0.69581009\n",
      "Iteration 538, loss = 0.69580251\n",
      "Iteration 539, loss = 0.69579419\n",
      "Iteration 540, loss = 0.69578711\n",
      "Iteration 541, loss = 0.69577823\n",
      "Iteration 542, loss = 0.69577058\n",
      "Iteration 543, loss = 0.69576124\n",
      "Iteration 544, loss = 0.69575256\n",
      "Iteration 545, loss = 0.69574595\n",
      "Iteration 546, loss = 0.69573714\n",
      "Iteration 547, loss = 0.69573040\n",
      "Iteration 548, loss = 0.69572398\n",
      "Iteration 549, loss = 0.69571608\n",
      "Iteration 550, loss = 0.69570915\n",
      "Iteration 551, loss = 0.69570237\n",
      "Iteration 552, loss = 0.69569481\n",
      "Iteration 553, loss = 0.69568809\n",
      "Iteration 554, loss = 0.69567998\n",
      "Iteration 555, loss = 0.69567356\n",
      "Iteration 556, loss = 0.69566647\n",
      "Iteration 557, loss = 0.69565842\n",
      "Iteration 558, loss = 0.69565021\n",
      "Iteration 559, loss = 0.69564365\n",
      "Iteration 560, loss = 0.69563688\n",
      "Iteration 561, loss = 0.69562889\n",
      "Iteration 562, loss = 0.69562246\n",
      "Iteration 563, loss = 0.69561476\n",
      "Iteration 564, loss = 0.69560771\n",
      "Iteration 565, loss = 0.69560100\n",
      "Iteration 566, loss = 0.69559415\n",
      "Iteration 567, loss = 0.69558639\n",
      "Iteration 568, loss = 0.69557934\n",
      "Iteration 569, loss = 0.69557175\n",
      "Iteration 570, loss = 0.69556470\n",
      "Iteration 571, loss = 0.69555792\n",
      "Iteration 572, loss = 0.69555042\n",
      "Iteration 573, loss = 0.69554421\n",
      "Iteration 574, loss = 0.69553680\n",
      "Iteration 575, loss = 0.69552878\n",
      "Iteration 576, loss = 0.69552177\n",
      "Iteration 577, loss = 0.69551426\n",
      "Iteration 578, loss = 0.69550817\n",
      "Iteration 579, loss = 0.69550098\n",
      "Iteration 580, loss = 0.69549382\n",
      "Iteration 581, loss = 0.69548725\n",
      "Iteration 582, loss = 0.69548096\n",
      "Iteration 583, loss = 0.69547452\n",
      "Iteration 584, loss = 0.69546763\n",
      "Iteration 585, loss = 0.69546131\n",
      "Iteration 586, loss = 0.69545450\n",
      "Iteration 587, loss = 0.69544952\n",
      "Iteration 588, loss = 0.69544104\n",
      "Iteration 589, loss = 0.69543383\n",
      "Iteration 590, loss = 0.69542736\n",
      "Iteration 591, loss = 0.69542070\n",
      "Iteration 592, loss = 0.69541447\n",
      "Iteration 593, loss = 0.69540789\n",
      "Iteration 594, loss = 0.69540022\n",
      "Iteration 595, loss = 0.69539297\n",
      "Iteration 596, loss = 0.69538622\n",
      "Iteration 597, loss = 0.69538003\n",
      "Iteration 598, loss = 0.69537413\n",
      "Iteration 599, loss = 0.69536832\n",
      "Iteration 600, loss = 0.69536335\n",
      "Iteration 601, loss = 0.69535530\n",
      "Iteration 602, loss = 0.69534926\n",
      "Iteration 603, loss = 0.69534244\n",
      "Iteration 604, loss = 0.69533551\n",
      "Iteration 605, loss = 0.69533007\n",
      "Iteration 606, loss = 0.69532327\n",
      "Iteration 607, loss = 0.69531624\n",
      "Iteration 608, loss = 0.69531032\n",
      "Iteration 609, loss = 0.69530342\n",
      "Iteration 610, loss = 0.69529781\n",
      "Iteration 611, loss = 0.69529161\n",
      "Iteration 612, loss = 0.69528510\n",
      "Iteration 613, loss = 0.69527776\n",
      "Iteration 614, loss = 0.69527152\n",
      "Iteration 615, loss = 0.69526410\n",
      "Iteration 616, loss = 0.69525651\n",
      "Iteration 617, loss = 0.69525024\n",
      "Iteration 618, loss = 0.69524310\n",
      "Iteration 619, loss = 0.69523672\n",
      "Iteration 620, loss = 0.69523051\n",
      "Iteration 621, loss = 0.69522413\n",
      "Iteration 622, loss = 0.69521826\n",
      "Iteration 623, loss = 0.69521231\n",
      "Iteration 624, loss = 0.69520698\n",
      "Iteration 625, loss = 0.69520174\n",
      "Iteration 626, loss = 0.69519514\n",
      "Iteration 627, loss = 0.69518882\n",
      "Iteration 628, loss = 0.69518331\n",
      "Iteration 629, loss = 0.69517712\n",
      "Iteration 630, loss = 0.69517129\n",
      "Iteration 631, loss = 0.69516552\n",
      "Iteration 632, loss = 0.69516102\n",
      "Iteration 633, loss = 0.69515490\n",
      "Iteration 634, loss = 0.69514881\n",
      "Iteration 635, loss = 0.69514294\n",
      "Iteration 636, loss = 0.69513610\n",
      "Iteration 637, loss = 0.69513117\n",
      "Iteration 638, loss = 0.69512555\n",
      "Iteration 639, loss = 0.69512018\n",
      "Iteration 640, loss = 0.69511516\n",
      "Iteration 641, loss = 0.69511052\n",
      "Iteration 642, loss = 0.69510593\n",
      "Iteration 643, loss = 0.69510132\n",
      "Iteration 644, loss = 0.69509545\n",
      "Iteration 645, loss = 0.69509073\n",
      "Iteration 646, loss = 0.69508482\n",
      "Iteration 647, loss = 0.69507969\n",
      "Iteration 648, loss = 0.69507359\n",
      "Iteration 649, loss = 0.69506846\n",
      "Iteration 650, loss = 0.69506164\n",
      "Iteration 651, loss = 0.69505671\n",
      "Iteration 652, loss = 0.69505089\n",
      "Iteration 653, loss = 0.69504595\n",
      "Iteration 654, loss = 0.69504123\n",
      "Iteration 655, loss = 0.69503537\n",
      "Iteration 656, loss = 0.69503011\n",
      "Iteration 657, loss = 0.69502457\n",
      "Iteration 658, loss = 0.69501854\n",
      "Iteration 659, loss = 0.69501273\n",
      "Iteration 660, loss = 0.69500787\n",
      "Iteration 661, loss = 0.69500145\n",
      "Iteration 662, loss = 0.69499649\n",
      "Iteration 663, loss = 0.69498947\n",
      "Iteration 664, loss = 0.69498416\n",
      "Iteration 665, loss = 0.69497879\n",
      "Iteration 666, loss = 0.69497347\n",
      "Iteration 667, loss = 0.69496774\n",
      "Iteration 668, loss = 0.69496332\n",
      "Iteration 669, loss = 0.69495727\n",
      "Iteration 670, loss = 0.69495290\n",
      "Iteration 671, loss = 0.69494653\n",
      "Iteration 672, loss = 0.69494134\n",
      "Iteration 673, loss = 0.69493645\n",
      "Iteration 674, loss = 0.69493073\n",
      "Iteration 675, loss = 0.69492621\n",
      "Iteration 676, loss = 0.69492132\n",
      "Iteration 677, loss = 0.69491561\n",
      "Iteration 678, loss = 0.69491048\n",
      "Iteration 679, loss = 0.69490504\n",
      "Iteration 680, loss = 0.69489935\n",
      "Iteration 681, loss = 0.69489353\n",
      "Iteration 682, loss = 0.69488977\n",
      "Iteration 683, loss = 0.69488280\n",
      "Iteration 684, loss = 0.69487833\n",
      "Iteration 685, loss = 0.69487398\n",
      "Iteration 686, loss = 0.69486818\n",
      "Iteration 687, loss = 0.69486379\n",
      "Iteration 688, loss = 0.69485858\n",
      "Iteration 689, loss = 0.69485371\n",
      "Iteration 690, loss = 0.69484835\n",
      "Iteration 691, loss = 0.69484327\n",
      "Iteration 692, loss = 0.69483998\n",
      "Iteration 693, loss = 0.69483516\n",
      "Iteration 694, loss = 0.69483000\n",
      "Iteration 695, loss = 0.69482608\n",
      "Iteration 696, loss = 0.69482195\n",
      "Iteration 697, loss = 0.69481728\n",
      "Iteration 698, loss = 0.69481339\n",
      "Iteration 699, loss = 0.69480932\n",
      "Iteration 700, loss = 0.69480426\n",
      "Iteration 701, loss = 0.69479852\n",
      "Iteration 702, loss = 0.69479429\n",
      "Iteration 703, loss = 0.69478976\n",
      "Iteration 704, loss = 0.69478514\n",
      "Iteration 705, loss = 0.69478006\n",
      "Iteration 706, loss = 0.69477495\n",
      "Iteration 707, loss = 0.69477051\n",
      "Iteration 708, loss = 0.69476673\n",
      "Iteration 709, loss = 0.69476118\n",
      "Iteration 710, loss = 0.69475701\n",
      "Iteration 711, loss = 0.69475223\n",
      "Iteration 712, loss = 0.69474803\n",
      "Iteration 713, loss = 0.69474367\n",
      "Iteration 714, loss = 0.69473884\n",
      "Iteration 715, loss = 0.69473355\n",
      "Iteration 716, loss = 0.69472934\n",
      "Iteration 717, loss = 0.69472595\n",
      "Iteration 718, loss = 0.69472048\n",
      "Iteration 719, loss = 0.69471636\n",
      "Iteration 720, loss = 0.69471122\n",
      "Iteration 721, loss = 0.69470673\n",
      "Iteration 722, loss = 0.69470300\n",
      "Iteration 723, loss = 0.69469885\n",
      "Iteration 724, loss = 0.69469400\n",
      "Iteration 725, loss = 0.69468909\n",
      "Iteration 726, loss = 0.69468487\n",
      "Iteration 727, loss = 0.69468045\n",
      "Iteration 728, loss = 0.69467716\n",
      "Iteration 729, loss = 0.69467345\n",
      "Iteration 730, loss = 0.69466883\n",
      "Iteration 731, loss = 0.69466370\n",
      "Iteration 732, loss = 0.69465948\n",
      "Iteration 733, loss = 0.69465446\n",
      "Iteration 734, loss = 0.69465101\n",
      "Iteration 735, loss = 0.69464553\n",
      "Iteration 736, loss = 0.69464205\n",
      "Iteration 737, loss = 0.69463744\n",
      "Iteration 738, loss = 0.69463363\n",
      "Iteration 739, loss = 0.69462961\n",
      "Iteration 740, loss = 0.69462484\n",
      "Iteration 741, loss = 0.69462140\n",
      "Iteration 742, loss = 0.69461684\n",
      "Iteration 743, loss = 0.69461299\n",
      "Iteration 744, loss = 0.69460889\n",
      "Iteration 745, loss = 0.69460446\n",
      "Iteration 746, loss = 0.69459871\n",
      "Iteration 747, loss = 0.69459401\n",
      "Iteration 748, loss = 0.69458893\n",
      "Iteration 749, loss = 0.69458609\n",
      "Iteration 750, loss = 0.69458086\n",
      "Iteration 751, loss = 0.69457580\n",
      "Iteration 752, loss = 0.69457116\n",
      "Iteration 753, loss = 0.69456703\n",
      "Iteration 754, loss = 0.69456186\n",
      "Iteration 755, loss = 0.69455715\n",
      "Iteration 756, loss = 0.69455397\n",
      "Iteration 757, loss = 0.69454775\n",
      "Iteration 758, loss = 0.69454370\n",
      "Iteration 759, loss = 0.69453919\n",
      "Iteration 760, loss = 0.69453601\n",
      "Iteration 761, loss = 0.69453065\n",
      "Iteration 762, loss = 0.69452652\n",
      "Iteration 763, loss = 0.69452270\n",
      "Iteration 764, loss = 0.69451851\n",
      "Iteration 765, loss = 0.69451436\n",
      "Iteration 766, loss = 0.69451091\n",
      "Iteration 767, loss = 0.69450756\n",
      "Iteration 768, loss = 0.69450262\n",
      "Iteration 769, loss = 0.69449902\n",
      "Iteration 770, loss = 0.69449552\n",
      "Iteration 771, loss = 0.69449149\n",
      "Iteration 772, loss = 0.69448640\n",
      "Iteration 773, loss = 0.69448293\n",
      "Iteration 774, loss = 0.69447885\n",
      "Iteration 775, loss = 0.69447578\n",
      "Iteration 776, loss = 0.69447188\n",
      "Iteration 777, loss = 0.69446851\n",
      "Iteration 778, loss = 0.69446447\n",
      "Iteration 779, loss = 0.69445994\n",
      "Iteration 780, loss = 0.69445664\n",
      "Iteration 781, loss = 0.69445169\n",
      "Iteration 782, loss = 0.69444765\n",
      "Iteration 783, loss = 0.69444392\n",
      "Iteration 784, loss = 0.69443999\n",
      "Iteration 785, loss = 0.69443553\n",
      "Iteration 786, loss = 0.69443152\n",
      "Iteration 787, loss = 0.69442701\n",
      "Iteration 788, loss = 0.69442298\n",
      "Iteration 789, loss = 0.69441961\n",
      "Iteration 790, loss = 0.69441542\n",
      "Iteration 791, loss = 0.69441207\n",
      "Iteration 792, loss = 0.69440749\n",
      "Iteration 793, loss = 0.69440282\n",
      "Iteration 794, loss = 0.69439866\n",
      "Iteration 795, loss = 0.69439505\n",
      "Iteration 796, loss = 0.69439177\n",
      "Iteration 797, loss = 0.69438817\n",
      "Iteration 798, loss = 0.69438422\n",
      "Iteration 799, loss = 0.69438062\n",
      "Iteration 800, loss = 0.69437696\n",
      "Iteration 801, loss = 0.69437211\n",
      "Iteration 802, loss = 0.69436796\n",
      "Iteration 803, loss = 0.69436415\n",
      "Iteration 804, loss = 0.69436029\n",
      "Iteration 805, loss = 0.69435661\n",
      "Iteration 806, loss = 0.69435310\n",
      "Iteration 807, loss = 0.69434944\n",
      "Iteration 808, loss = 0.69434552\n",
      "Iteration 809, loss = 0.69434216\n",
      "Iteration 810, loss = 0.69433860\n",
      "Iteration 811, loss = 0.69433523\n",
      "Iteration 812, loss = 0.69433173\n",
      "Iteration 813, loss = 0.69432773\n",
      "Iteration 814, loss = 0.69432436\n",
      "Iteration 815, loss = 0.69432172\n",
      "Iteration 816, loss = 0.69431853\n",
      "Iteration 817, loss = 0.69431547\n",
      "Iteration 818, loss = 0.69431153\n",
      "Iteration 819, loss = 0.69430770\n",
      "Iteration 820, loss = 0.69430416\n",
      "Iteration 821, loss = 0.69430006\n",
      "Iteration 822, loss = 0.69429670\n",
      "Iteration 823, loss = 0.69429321\n",
      "Iteration 824, loss = 0.69428948\n",
      "Iteration 825, loss = 0.69428618\n",
      "Iteration 826, loss = 0.69428279\n",
      "Iteration 827, loss = 0.69428029\n",
      "Iteration 828, loss = 0.69427607\n",
      "Iteration 829, loss = 0.69427303\n",
      "Iteration 830, loss = 0.69426961\n",
      "Iteration 831, loss = 0.69426652\n",
      "Iteration 832, loss = 0.69426288\n",
      "Iteration 833, loss = 0.69425881\n",
      "Iteration 834, loss = 0.69425538\n",
      "Iteration 835, loss = 0.69425209\n",
      "Iteration 836, loss = 0.69424914\n",
      "Iteration 837, loss = 0.69424590\n",
      "Iteration 838, loss = 0.69424197\n",
      "Iteration 839, loss = 0.69423968\n",
      "Iteration 840, loss = 0.69423473\n",
      "Iteration 841, loss = 0.69423182\n",
      "Iteration 842, loss = 0.69422794\n",
      "Iteration 843, loss = 0.69422464\n",
      "Iteration 844, loss = 0.69422054\n",
      "Iteration 845, loss = 0.69421675\n",
      "Iteration 846, loss = 0.69421387\n",
      "Iteration 847, loss = 0.69421082\n",
      "Iteration 848, loss = 0.69420707\n",
      "Iteration 849, loss = 0.69420477\n",
      "Iteration 850, loss = 0.69420079\n",
      "Iteration 851, loss = 0.69419704\n",
      "Iteration 852, loss = 0.69419389\n",
      "Iteration 853, loss = 0.69419016\n",
      "Iteration 854, loss = 0.69418774\n",
      "Iteration 855, loss = 0.69418367\n",
      "Iteration 856, loss = 0.69418070\n",
      "Iteration 857, loss = 0.69417766\n",
      "Iteration 858, loss = 0.69417451\n",
      "Iteration 859, loss = 0.69417096\n",
      "Iteration 860, loss = 0.69416770\n",
      "Iteration 861, loss = 0.69416474\n",
      "Iteration 862, loss = 0.69416100\n",
      "Iteration 863, loss = 0.69415862\n",
      "Iteration 864, loss = 0.69415540\n",
      "Iteration 865, loss = 0.69415172\n",
      "Iteration 866, loss = 0.69414874\n",
      "Iteration 867, loss = 0.69414630\n",
      "Iteration 868, loss = 0.69414133\n",
      "Iteration 869, loss = 0.69413868\n",
      "Iteration 870, loss = 0.69413557\n",
      "Iteration 871, loss = 0.69413214\n",
      "Iteration 872, loss = 0.69412932\n",
      "Iteration 873, loss = 0.69412661\n",
      "Iteration 874, loss = 0.69412332\n",
      "Iteration 875, loss = 0.69412041\n",
      "Iteration 876, loss = 0.69411772\n",
      "Iteration 877, loss = 0.69411437\n",
      "Iteration 878, loss = 0.69411205\n",
      "Iteration 879, loss = 0.69410881\n",
      "Iteration 880, loss = 0.69410606\n",
      "Iteration 881, loss = 0.69410334\n",
      "Iteration 882, loss = 0.69410021\n",
      "Iteration 883, loss = 0.69409723\n",
      "Iteration 884, loss = 0.69409395\n",
      "Iteration 885, loss = 0.69409058\n",
      "Iteration 886, loss = 0.69408718\n",
      "Iteration 887, loss = 0.69408421\n",
      "Iteration 888, loss = 0.69408144\n",
      "Iteration 889, loss = 0.69407787\n",
      "Iteration 890, loss = 0.69407490\n",
      "Iteration 891, loss = 0.69407229\n",
      "Iteration 892, loss = 0.69406943\n",
      "Iteration 893, loss = 0.69406626\n",
      "Iteration 894, loss = 0.69406403\n",
      "Iteration 895, loss = 0.69406056\n",
      "Iteration 896, loss = 0.69405806\n",
      "Iteration 897, loss = 0.69405453\n",
      "Iteration 898, loss = 0.69405192\n",
      "Iteration 899, loss = 0.69404903\n",
      "Iteration 900, loss = 0.69404583\n",
      "Iteration 901, loss = 0.69404317\n",
      "Iteration 902, loss = 0.69403982\n",
      "Iteration 903, loss = 0.69403734\n",
      "Iteration 904, loss = 0.69403472\n",
      "Iteration 905, loss = 0.69403108\n",
      "Iteration 906, loss = 0.69402835\n",
      "Iteration 907, loss = 0.69402495\n",
      "Iteration 908, loss = 0.69402223\n",
      "Iteration 909, loss = 0.69401929\n",
      "Iteration 910, loss = 0.69401615\n",
      "Iteration 911, loss = 0.69401314\n",
      "Iteration 912, loss = 0.69401005\n",
      "Iteration 913, loss = 0.69400739\n",
      "Iteration 914, loss = 0.69400471\n",
      "Iteration 915, loss = 0.69400144\n",
      "Iteration 916, loss = 0.69399753\n",
      "Iteration 917, loss = 0.69399343\n",
      "Iteration 918, loss = 0.69399097\n",
      "Iteration 919, loss = 0.69398829\n",
      "Iteration 920, loss = 0.69398569\n",
      "Iteration 921, loss = 0.69398307\n",
      "Iteration 922, loss = 0.69397939\n",
      "Iteration 923, loss = 0.69397667\n",
      "Iteration 924, loss = 0.69397285\n",
      "Iteration 925, loss = 0.69397175\n",
      "Iteration 926, loss = 0.69396680\n",
      "Iteration 927, loss = 0.69396336\n",
      "Iteration 928, loss = 0.69396080\n",
      "Iteration 929, loss = 0.69395804\n",
      "Iteration 930, loss = 0.69395627\n",
      "Iteration 931, loss = 0.69395297\n",
      "Iteration 932, loss = 0.69395034\n",
      "Iteration 933, loss = 0.69394767\n",
      "Iteration 934, loss = 0.69394504\n",
      "Iteration 935, loss = 0.69394208\n",
      "Iteration 936, loss = 0.69393990\n",
      "Iteration 937, loss = 0.69393643\n",
      "Iteration 938, loss = 0.69393349\n",
      "Iteration 939, loss = 0.69393100\n",
      "Iteration 940, loss = 0.69392876\n",
      "Iteration 941, loss = 0.69392616\n",
      "Iteration 942, loss = 0.69392312\n",
      "Iteration 943, loss = 0.69392121\n",
      "Iteration 944, loss = 0.69391801\n",
      "Iteration 945, loss = 0.69391580\n",
      "Iteration 946, loss = 0.69391178\n",
      "Iteration 947, loss = 0.69390768\n",
      "Iteration 948, loss = 0.69390481\n",
      "Iteration 949, loss = 0.69390205\n",
      "Iteration 950, loss = 0.69389896\n",
      "Iteration 951, loss = 0.69389626\n",
      "Iteration 952, loss = 0.69389403\n",
      "Iteration 953, loss = 0.69389072\n",
      "Iteration 954, loss = 0.69388840\n",
      "Iteration 955, loss = 0.69388564\n",
      "Iteration 956, loss = 0.69388242\n",
      "Iteration 957, loss = 0.69388051\n",
      "Iteration 958, loss = 0.69387753\n",
      "Iteration 959, loss = 0.69387471\n",
      "Iteration 960, loss = 0.69387212\n",
      "Iteration 961, loss = 0.69386947\n",
      "Iteration 962, loss = 0.69386694\n",
      "Iteration 963, loss = 0.69386438\n",
      "Iteration 964, loss = 0.69386248\n",
      "Iteration 965, loss = 0.69385939\n",
      "Iteration 966, loss = 0.69385698\n",
      "Iteration 967, loss = 0.69385366\n",
      "Iteration 968, loss = 0.69385130\n",
      "Iteration 969, loss = 0.69384849\n",
      "Iteration 970, loss = 0.69384618\n",
      "Iteration 971, loss = 0.69384289\n",
      "Iteration 972, loss = 0.69383979\n",
      "Iteration 973, loss = 0.69383740\n",
      "Iteration 974, loss = 0.69383526\n",
      "Iteration 975, loss = 0.69383198\n",
      "Iteration 976, loss = 0.69382942\n",
      "Iteration 977, loss = 0.69382591\n",
      "Iteration 978, loss = 0.69382413\n",
      "Iteration 979, loss = 0.69382141\n",
      "Iteration 980, loss = 0.69381978\n",
      "Iteration 981, loss = 0.69381695\n",
      "Iteration 982, loss = 0.69381408\n",
      "Iteration 983, loss = 0.69381113\n",
      "Iteration 984, loss = 0.69380910\n",
      "Iteration 985, loss = 0.69380668\n",
      "Iteration 986, loss = 0.69380429\n",
      "Iteration 987, loss = 0.69380100\n",
      "Iteration 988, loss = 0.69379813\n",
      "Iteration 989, loss = 0.69379576\n",
      "Iteration 990, loss = 0.69379368\n",
      "Iteration 991, loss = 0.69379135\n",
      "Iteration 992, loss = 0.69378857\n",
      "Iteration 993, loss = 0.69378582\n",
      "Iteration 994, loss = 0.69378365\n",
      "Iteration 995, loss = 0.69378062\n",
      "Iteration 996, loss = 0.69377759\n",
      "Iteration 997, loss = 0.69377485\n",
      "Iteration 998, loss = 0.69377132\n",
      "Iteration 999, loss = 0.69376886\n",
      "Iteration 1000, loss = 0.69376591\n",
      "Iteration 1001, loss = 0.69376303\n",
      "Iteration 1002, loss = 0.69376002\n",
      "Iteration 1003, loss = 0.69375648\n",
      "Iteration 1004, loss = 0.69375438\n",
      "Iteration 1005, loss = 0.69375190\n",
      "Iteration 1006, loss = 0.69374991\n",
      "Iteration 1007, loss = 0.69374722\n",
      "Iteration 1008, loss = 0.69374391\n",
      "Iteration 1009, loss = 0.69374174\n",
      "Iteration 1010, loss = 0.69373899\n",
      "Iteration 1011, loss = 0.69373720\n",
      "Iteration 1012, loss = 0.69373408\n",
      "Iteration 1013, loss = 0.69373124\n",
      "Iteration 1014, loss = 0.69372897\n",
      "Iteration 1015, loss = 0.69372717\n",
      "Iteration 1016, loss = 0.69372401\n",
      "Iteration 1017, loss = 0.69372183\n",
      "Iteration 1018, loss = 0.69371965\n",
      "Iteration 1019, loss = 0.69371719\n",
      "Iteration 1020, loss = 0.69371536\n",
      "Iteration 1021, loss = 0.69371221\n",
      "Iteration 1022, loss = 0.69371009\n",
      "Iteration 1023, loss = 0.69370716\n",
      "Iteration 1024, loss = 0.69370511\n",
      "Iteration 1025, loss = 0.69370280\n",
      "Iteration 1026, loss = 0.69370003\n",
      "Iteration 1027, loss = 0.69369767\n",
      "Iteration 1028, loss = 0.69369452\n",
      "Iteration 1029, loss = 0.69369200\n",
      "Iteration 1030, loss = 0.69369010\n",
      "Iteration 1031, loss = 0.69368752\n",
      "Iteration 1032, loss = 0.69368420\n",
      "Iteration 1033, loss = 0.69368205\n",
      "Iteration 1034, loss = 0.69367984\n",
      "Iteration 1035, loss = 0.69367710\n",
      "Iteration 1036, loss = 0.69367571\n",
      "Iteration 1037, loss = 0.69367280\n",
      "Iteration 1038, loss = 0.69367002\n",
      "Iteration 1039, loss = 0.69366807\n",
      "Iteration 1040, loss = 0.69366553\n",
      "Iteration 1041, loss = 0.69366302\n",
      "Iteration 1042, loss = 0.69366175\n",
      "Iteration 1043, loss = 0.69365888\n",
      "Iteration 1044, loss = 0.69365627\n",
      "Iteration 1045, loss = 0.69365377\n",
      "Iteration 1046, loss = 0.69365163\n",
      "Iteration 1047, loss = 0.69364925\n",
      "Iteration 1048, loss = 0.69364679\n",
      "Iteration 1049, loss = 0.69364444\n",
      "Iteration 1050, loss = 0.69364218\n",
      "Iteration 1051, loss = 0.69363993\n",
      "Iteration 1052, loss = 0.69363723\n",
      "Iteration 1053, loss = 0.69363440\n",
      "Iteration 1054, loss = 0.69363132\n",
      "Iteration 1055, loss = 0.69362878\n",
      "Iteration 1056, loss = 0.69362660\n",
      "Iteration 1057, loss = 0.69362467\n",
      "Iteration 1058, loss = 0.69362230\n",
      "Iteration 1059, loss = 0.69361940\n",
      "Iteration 1060, loss = 0.69361775\n",
      "Iteration 1061, loss = 0.69361534\n",
      "Iteration 1062, loss = 0.69361295\n",
      "Iteration 1063, loss = 0.69361043\n",
      "Iteration 1064, loss = 0.69360830\n",
      "Iteration 1065, loss = 0.69360581\n",
      "Iteration 1066, loss = 0.69360367\n",
      "Iteration 1067, loss = 0.69360167\n",
      "Iteration 1068, loss = 0.69359991\n",
      "Iteration 1069, loss = 0.69359731\n",
      "Iteration 1070, loss = 0.69359372\n",
      "Iteration 1071, loss = 0.69359170\n",
      "Iteration 1072, loss = 0.69358964\n",
      "Iteration 1073, loss = 0.69358714\n",
      "Iteration 1074, loss = 0.69358476\n",
      "Iteration 1075, loss = 0.69358222\n",
      "Iteration 1076, loss = 0.69358024\n",
      "Iteration 1077, loss = 0.69357808\n",
      "Iteration 1078, loss = 0.69357571\n",
      "Iteration 1079, loss = 0.69357341\n",
      "Iteration 1080, loss = 0.69357163\n",
      "Iteration 1081, loss = 0.69356902\n",
      "Iteration 1082, loss = 0.69356675\n",
      "Iteration 1083, loss = 0.69356415\n",
      "Iteration 1084, loss = 0.69356149\n",
      "Iteration 1085, loss = 0.69355927\n",
      "Iteration 1086, loss = 0.69355712\n",
      "Iteration 1087, loss = 0.69355527\n",
      "Iteration 1088, loss = 0.69355349\n",
      "Iteration 1089, loss = 0.69355160\n",
      "Iteration 1090, loss = 0.69354983\n",
      "Iteration 1091, loss = 0.69354716\n",
      "Iteration 1092, loss = 0.69354515\n",
      "Iteration 1093, loss = 0.69354270\n",
      "Iteration 1094, loss = 0.69354179\n",
      "Iteration 1095, loss = 0.69353930\n",
      "Iteration 1096, loss = 0.69353732\n",
      "Iteration 1097, loss = 0.69353515\n",
      "Iteration 1098, loss = 0.69353259\n",
      "Iteration 1099, loss = 0.69352997\n",
      "Iteration 1100, loss = 0.69352770\n",
      "Iteration 1101, loss = 0.69352565\n",
      "Iteration 1102, loss = 0.69352288\n",
      "Iteration 1103, loss = 0.69352112\n",
      "Iteration 1104, loss = 0.69351819\n",
      "Iteration 1105, loss = 0.69351698\n",
      "Iteration 1106, loss = 0.69351427\n",
      "Iteration 1107, loss = 0.69351192\n",
      "Iteration 1108, loss = 0.69350988\n",
      "Iteration 1109, loss = 0.69350780\n",
      "Iteration 1110, loss = 0.69350640\n",
      "Iteration 1111, loss = 0.69350386\n",
      "Iteration 1112, loss = 0.69350222\n",
      "Iteration 1113, loss = 0.69349999\n",
      "Iteration 1114, loss = 0.69349791\n",
      "Iteration 1115, loss = 0.69349501\n",
      "Iteration 1116, loss = 0.69349323\n",
      "Iteration 1117, loss = 0.69349217\n",
      "Iteration 1118, loss = 0.69348865\n",
      "Iteration 1119, loss = 0.69348621\n",
      "Iteration 1120, loss = 0.69348388\n",
      "Iteration 1121, loss = 0.69348187\n",
      "Iteration 1122, loss = 0.69347949\n",
      "Iteration 1123, loss = 0.69347737\n",
      "Iteration 1124, loss = 0.69347500\n",
      "Iteration 1125, loss = 0.69347286\n",
      "Iteration 1126, loss = 0.69347037\n",
      "Iteration 1127, loss = 0.69346846\n",
      "Iteration 1128, loss = 0.69346617\n",
      "Iteration 1129, loss = 0.69346373\n",
      "Iteration 1130, loss = 0.69346151\n",
      "Iteration 1131, loss = 0.69346005\n",
      "Iteration 1132, loss = 0.69345717\n",
      "Iteration 1133, loss = 0.69345542\n",
      "Iteration 1134, loss = 0.69345284\n",
      "Iteration 1135, loss = 0.69345047\n",
      "Iteration 1136, loss = 0.69344797\n",
      "Iteration 1137, loss = 0.69344589\n",
      "Iteration 1138, loss = 0.69344397\n",
      "Iteration 1139, loss = 0.69344199\n",
      "Iteration 1140, loss = 0.69343931\n",
      "Iteration 1141, loss = 0.69343675\n",
      "Iteration 1142, loss = 0.69343469\n",
      "Iteration 1143, loss = 0.69343248\n",
      "Iteration 1144, loss = 0.69343052\n",
      "Iteration 1145, loss = 0.69342806\n",
      "Iteration 1146, loss = 0.69342604\n",
      "Iteration 1147, loss = 0.69342392\n",
      "Iteration 1148, loss = 0.69342109\n",
      "Iteration 1149, loss = 0.69341912\n",
      "Iteration 1150, loss = 0.69341679\n",
      "Iteration 1151, loss = 0.69341486\n",
      "Iteration 1152, loss = 0.69341243\n",
      "Iteration 1153, loss = 0.69341036\n",
      "Iteration 1154, loss = 0.69340839\n",
      "Iteration 1155, loss = 0.69340616\n",
      "Iteration 1156, loss = 0.69340413\n",
      "Iteration 1157, loss = 0.69340214\n",
      "Iteration 1158, loss = 0.69340030\n",
      "Iteration 1159, loss = 0.69339885\n",
      "Iteration 1160, loss = 0.69339652\n",
      "Iteration 1161, loss = 0.69339401\n",
      "Iteration 1162, loss = 0.69339200\n",
      "Iteration 1163, loss = 0.69338971\n",
      "Iteration 1164, loss = 0.69338821\n",
      "Iteration 1165, loss = 0.69338525\n",
      "Iteration 1166, loss = 0.69338308\n",
      "Iteration 1167, loss = 0.69338114\n",
      "Iteration 1168, loss = 0.69337892\n",
      "Iteration 1169, loss = 0.69337670\n",
      "Iteration 1170, loss = 0.69337501\n",
      "Iteration 1171, loss = 0.69337201\n",
      "Iteration 1172, loss = 0.69336989\n",
      "Iteration 1173, loss = 0.69336749\n",
      "Iteration 1174, loss = 0.69336555\n",
      "Iteration 1175, loss = 0.69336366\n",
      "Iteration 1176, loss = 0.69336147\n",
      "Iteration 1177, loss = 0.69335928\n",
      "Iteration 1178, loss = 0.69335719\n",
      "Iteration 1179, loss = 0.69335513\n",
      "Iteration 1180, loss = 0.69335329\n",
      "Iteration 1181, loss = 0.69335073\n",
      "Iteration 1182, loss = 0.69334881\n",
      "Iteration 1183, loss = 0.69334721\n",
      "Iteration 1184, loss = 0.69334506\n",
      "Iteration 1185, loss = 0.69334357\n",
      "Iteration 1186, loss = 0.69334101\n",
      "Iteration 1187, loss = 0.69333890\n",
      "Iteration 1188, loss = 0.69333742\n",
      "Iteration 1189, loss = 0.69333554\n",
      "Iteration 1190, loss = 0.69333399\n",
      "Iteration 1191, loss = 0.69333073\n",
      "Iteration 1192, loss = 0.69332888\n",
      "Iteration 1193, loss = 0.69332657\n",
      "Iteration 1194, loss = 0.69332446\n",
      "Iteration 1195, loss = 0.69332221\n",
      "Iteration 1196, loss = 0.69332025\n",
      "Iteration 1197, loss = 0.69331940\n",
      "Iteration 1198, loss = 0.69331580\n",
      "Iteration 1199, loss = 0.69331388\n",
      "Iteration 1200, loss = 0.69331159\n",
      "Iteration 1201, loss = 0.69330927\n",
      "Iteration 1202, loss = 0.69330703\n",
      "Iteration 1203, loss = 0.69330492\n",
      "Iteration 1204, loss = 0.69330367\n",
      "Iteration 1205, loss = 0.69330024\n",
      "Iteration 1206, loss = 0.69329839\n",
      "Iteration 1207, loss = 0.69329601\n",
      "Iteration 1208, loss = 0.69329430\n",
      "Iteration 1209, loss = 0.69329251\n",
      "Iteration 1210, loss = 0.69328994\n",
      "Iteration 1211, loss = 0.69328807\n",
      "Iteration 1212, loss = 0.69328574\n",
      "Iteration 1213, loss = 0.69328407\n",
      "Iteration 1214, loss = 0.69328196\n",
      "Iteration 1215, loss = 0.69327959\n",
      "Iteration 1216, loss = 0.69327739\n",
      "Iteration 1217, loss = 0.69327604\n",
      "Iteration 1218, loss = 0.69327389\n",
      "Iteration 1219, loss = 0.69327187\n",
      "Iteration 1220, loss = 0.69326965\n",
      "Iteration 1221, loss = 0.69326778\n",
      "Iteration 1222, loss = 0.69326625\n",
      "Iteration 1223, loss = 0.69326336\n",
      "Iteration 1224, loss = 0.69326135\n",
      "Iteration 1225, loss = 0.69325939\n",
      "Iteration 1226, loss = 0.69325744\n",
      "Iteration 1227, loss = 0.69325528\n",
      "Iteration 1228, loss = 0.69325313\n",
      "Iteration 1229, loss = 0.69325102\n",
      "Iteration 1230, loss = 0.69324887\n",
      "Iteration 1231, loss = 0.69324697\n",
      "Iteration 1232, loss = 0.69324567\n",
      "Iteration 1233, loss = 0.69324329\n",
      "Iteration 1234, loss = 0.69324140\n",
      "Iteration 1235, loss = 0.69323941\n",
      "Iteration 1236, loss = 0.69323758\n",
      "Iteration 1237, loss = 0.69323531\n",
      "Iteration 1238, loss = 0.69323347\n",
      "Iteration 1239, loss = 0.69323145\n",
      "Iteration 1240, loss = 0.69323005\n",
      "Iteration 1241, loss = 0.69322790\n",
      "Iteration 1242, loss = 0.69322583\n",
      "Iteration 1243, loss = 0.69322416\n",
      "Iteration 1244, loss = 0.69322193\n",
      "Iteration 1245, loss = 0.69321994\n",
      "Iteration 1246, loss = 0.69321829\n",
      "Iteration 1247, loss = 0.69321637\n",
      "Iteration 1248, loss = 0.69321451\n",
      "Iteration 1249, loss = 0.69321234\n",
      "Iteration 1250, loss = 0.69321094\n",
      "Iteration 1251, loss = 0.69320885\n",
      "Iteration 1252, loss = 0.69320676\n",
      "Iteration 1253, loss = 0.69320545\n",
      "Iteration 1254, loss = 0.69320270\n",
      "Iteration 1255, loss = 0.69320108\n",
      "Iteration 1256, loss = 0.69320029\n",
      "Iteration 1257, loss = 0.69319731\n",
      "Iteration 1258, loss = 0.69319530\n",
      "Iteration 1259, loss = 0.69319366\n",
      "Iteration 1260, loss = 0.69319162\n",
      "Iteration 1261, loss = 0.69318996\n",
      "Iteration 1262, loss = 0.69318826\n",
      "Iteration 1263, loss = 0.69318582\n",
      "Iteration 1264, loss = 0.69318382\n",
      "Iteration 1265, loss = 0.69318177\n",
      "Iteration 1266, loss = 0.69318002\n",
      "Iteration 1267, loss = 0.69317855\n",
      "Iteration 1268, loss = 0.69317592\n",
      "Iteration 1269, loss = 0.69317523\n",
      "Iteration 1270, loss = 0.69317317\n",
      "Iteration 1271, loss = 0.69317145\n",
      "Iteration 1272, loss = 0.69316889\n",
      "Iteration 1273, loss = 0.69316686\n",
      "Iteration 1274, loss = 0.69316464\n",
      "Iteration 1275, loss = 0.69316228\n",
      "Iteration 1276, loss = 0.69316075\n",
      "Iteration 1277, loss = 0.69315849\n",
      "Iteration 1278, loss = 0.69315638\n",
      "Iteration 1279, loss = 0.69315487\n",
      "Iteration 1280, loss = 0.69315295\n",
      "Iteration 1281, loss = 0.69315097\n",
      "Iteration 1282, loss = 0.69314850\n",
      "Iteration 1283, loss = 0.69314734\n",
      "Iteration 1284, loss = 0.69314574\n",
      "Iteration 1285, loss = 0.69314292\n",
      "Iteration 1286, loss = 0.69314115\n",
      "Iteration 1287, loss = 0.69313906\n",
      "Iteration 1288, loss = 0.69313686\n",
      "Iteration 1289, loss = 0.69313554\n",
      "Iteration 1290, loss = 0.69313374\n",
      "Iteration 1291, loss = 0.69313163\n",
      "Iteration 1292, loss = 0.69313029\n",
      "Iteration 1293, loss = 0.69312801\n",
      "Iteration 1294, loss = 0.69312586\n",
      "Iteration 1295, loss = 0.69312459\n",
      "Iteration 1296, loss = 0.69312246\n",
      "Iteration 1297, loss = 0.69312106\n",
      "Iteration 1298, loss = 0.69311868\n",
      "Iteration 1299, loss = 0.69311615\n",
      "Iteration 1300, loss = 0.69311407\n",
      "Iteration 1301, loss = 0.69311250\n",
      "Iteration 1302, loss = 0.69310987\n",
      "Iteration 1303, loss = 0.69310839\n",
      "Iteration 1304, loss = 0.69310664\n",
      "Iteration 1305, loss = 0.69310444\n",
      "Iteration 1306, loss = 0.69310178\n",
      "Iteration 1307, loss = 0.69310028\n",
      "Iteration 1308, loss = 0.69309853\n",
      "Iteration 1309, loss = 0.69309733\n",
      "Iteration 1310, loss = 0.69309534\n",
      "Iteration 1311, loss = 0.69309323\n",
      "Iteration 1312, loss = 0.69309180\n",
      "Iteration 1313, loss = 0.69309000\n",
      "Iteration 1314, loss = 0.69308848\n",
      "Iteration 1315, loss = 0.69308665\n",
      "Iteration 1316, loss = 0.69308481\n",
      "Iteration 1317, loss = 0.69308324\n",
      "Iteration 1318, loss = 0.69308139\n",
      "Iteration 1319, loss = 0.69307941\n",
      "Iteration 1320, loss = 0.69307763\n",
      "Iteration 1321, loss = 0.69307596\n",
      "Iteration 1322, loss = 0.69307491\n",
      "Iteration 1323, loss = 0.69307221\n",
      "Iteration 1324, loss = 0.69307046\n",
      "Iteration 1325, loss = 0.69306896\n",
      "Iteration 1326, loss = 0.69306756\n",
      "Iteration 1327, loss = 0.69306527\n",
      "Iteration 1328, loss = 0.69306384\n",
      "Iteration 1329, loss = 0.69306157\n",
      "Iteration 1330, loss = 0.69305968\n",
      "Iteration 1331, loss = 0.69305787\n",
      "Iteration 1332, loss = 0.69305603\n",
      "Iteration 1333, loss = 0.69305448\n",
      "Iteration 1334, loss = 0.69305296\n",
      "Iteration 1335, loss = 0.69305087\n",
      "Iteration 1336, loss = 0.69304839\n",
      "Iteration 1337, loss = 0.69304700\n",
      "Iteration 1338, loss = 0.69304514\n",
      "Iteration 1339, loss = 0.69304321\n",
      "Iteration 1340, loss = 0.69304182\n",
      "Iteration 1341, loss = 0.69304035\n",
      "Iteration 1342, loss = 0.69303844\n",
      "Iteration 1343, loss = 0.69303687\n",
      "Iteration 1344, loss = 0.69303492\n",
      "Iteration 1345, loss = 0.69303343\n",
      "Iteration 1346, loss = 0.69303125\n",
      "Iteration 1347, loss = 0.69302970\n",
      "Iteration 1348, loss = 0.69302815\n",
      "Iteration 1349, loss = 0.69302576\n",
      "Iteration 1350, loss = 0.69302437\n",
      "Iteration 1351, loss = 0.69302245\n",
      "Iteration 1352, loss = 0.69302052\n",
      "Iteration 1353, loss = 0.69301922\n",
      "Iteration 1354, loss = 0.69301734\n",
      "Iteration 1355, loss = 0.69301608\n",
      "Iteration 1356, loss = 0.69301344\n",
      "Iteration 1357, loss = 0.69301177\n",
      "Iteration 1358, loss = 0.69300950\n",
      "Iteration 1359, loss = 0.69300748\n",
      "Iteration 1360, loss = 0.69300607\n",
      "Iteration 1361, loss = 0.69300462\n",
      "Iteration 1362, loss = 0.69300315\n",
      "Iteration 1363, loss = 0.69300059\n",
      "Iteration 1364, loss = 0.69299882\n",
      "Iteration 1365, loss = 0.69299715\n",
      "Iteration 1366, loss = 0.69299581\n",
      "Iteration 1367, loss = 0.69299405\n",
      "Iteration 1368, loss = 0.69299292\n",
      "Iteration 1369, loss = 0.69299083\n",
      "Iteration 1370, loss = 0.69298869\n",
      "Iteration 1371, loss = 0.69298719\n",
      "Iteration 1372, loss = 0.69298517\n",
      "Iteration 1373, loss = 0.69298387\n",
      "Iteration 1374, loss = 0.69298236\n",
      "Iteration 1375, loss = 0.69298006\n",
      "Iteration 1376, loss = 0.69297878\n",
      "Iteration 1377, loss = 0.69297678\n",
      "Iteration 1378, loss = 0.69297550\n",
      "Iteration 1379, loss = 0.69297426\n",
      "Iteration 1380, loss = 0.69297201\n",
      "Iteration 1381, loss = 0.69297003\n",
      "Iteration 1382, loss = 0.69296836\n",
      "Iteration 1383, loss = 0.69296707\n",
      "Iteration 1384, loss = 0.69296493\n",
      "Iteration 1385, loss = 0.69296313\n",
      "Iteration 1386, loss = 0.69296163\n",
      "Iteration 1387, loss = 0.69296014\n",
      "Iteration 1388, loss = 0.69295796\n",
      "Iteration 1389, loss = 0.69295625\n",
      "Iteration 1390, loss = 0.69295436\n",
      "Iteration 1391, loss = 0.69295295\n",
      "Iteration 1392, loss = 0.69295167\n",
      "Iteration 1393, loss = 0.69295057\n",
      "Iteration 1394, loss = 0.69294828\n",
      "Iteration 1395, loss = 0.69294735\n",
      "Iteration 1396, loss = 0.69294413\n",
      "Iteration 1397, loss = 0.69294276\n",
      "Iteration 1398, loss = 0.69294120\n",
      "Iteration 1399, loss = 0.69293965\n",
      "Iteration 1400, loss = 0.69293802\n",
      "Iteration 1401, loss = 0.69293642\n",
      "Iteration 1402, loss = 0.69293504\n",
      "Iteration 1403, loss = 0.69293384\n",
      "Iteration 1404, loss = 0.69293203\n",
      "Iteration 1405, loss = 0.69293044\n",
      "Iteration 1406, loss = 0.69292845\n",
      "Iteration 1407, loss = 0.69292668\n",
      "Iteration 1408, loss = 0.69292520\n",
      "Iteration 1409, loss = 0.69292417\n",
      "Iteration 1410, loss = 0.69292194\n",
      "Iteration 1411, loss = 0.69292026\n",
      "Iteration 1412, loss = 0.69291863\n",
      "Iteration 1413, loss = 0.69291717\n",
      "Iteration 1414, loss = 0.69291546\n",
      "Iteration 1415, loss = 0.69291383\n",
      "Iteration 1416, loss = 0.69291224\n",
      "Iteration 1417, loss = 0.69291052\n",
      "Iteration 1418, loss = 0.69290867\n",
      "Iteration 1419, loss = 0.69290713\n",
      "Iteration 1420, loss = 0.69290541\n",
      "Iteration 1421, loss = 0.69290416\n",
      "Iteration 1422, loss = 0.69290222\n",
      "Iteration 1423, loss = 0.69290116\n",
      "Iteration 1424, loss = 0.69289919\n",
      "Iteration 1425, loss = 0.69289746\n",
      "Iteration 1426, loss = 0.69289540\n",
      "Iteration 1427, loss = 0.69289399\n",
      "Iteration 1428, loss = 0.69289255\n",
      "Iteration 1429, loss = 0.69289073\n",
      "Iteration 1430, loss = 0.69288884\n",
      "Iteration 1431, loss = 0.69288775\n",
      "Iteration 1432, loss = 0.69288591\n",
      "Iteration 1433, loss = 0.69288402\n",
      "Iteration 1434, loss = 0.69288204\n",
      "Iteration 1435, loss = 0.69288041\n",
      "Iteration 1436, loss = 0.69287924\n",
      "Iteration 1437, loss = 0.69287736\n",
      "Iteration 1438, loss = 0.69287544\n",
      "Iteration 1439, loss = 0.69287426\n",
      "Iteration 1440, loss = 0.69287258\n",
      "Iteration 1441, loss = 0.69287039\n",
      "Iteration 1442, loss = 0.69286884\n",
      "Iteration 1443, loss = 0.69286737\n",
      "Iteration 1444, loss = 0.69286523\n",
      "Iteration 1445, loss = 0.69286407\n",
      "Iteration 1446, loss = 0.69286246\n",
      "Iteration 1447, loss = 0.69286084\n",
      "Iteration 1448, loss = 0.69285940\n",
      "Iteration 1449, loss = 0.69285833\n",
      "Iteration 1450, loss = 0.69285610\n",
      "Iteration 1451, loss = 0.69285441\n",
      "Iteration 1452, loss = 0.69285249\n",
      "Iteration 1453, loss = 0.69285098\n",
      "Iteration 1454, loss = 0.69284948\n",
      "Iteration 1455, loss = 0.69284765\n",
      "Iteration 1456, loss = 0.69284602\n",
      "Iteration 1457, loss = 0.69284477\n",
      "Iteration 1458, loss = 0.69284301\n",
      "Iteration 1459, loss = 0.69284135\n",
      "Iteration 1460, loss = 0.69283943\n",
      "Iteration 1461, loss = 0.69283791\n",
      "Iteration 1462, loss = 0.69283626\n",
      "Iteration 1463, loss = 0.69283513\n",
      "Iteration 1464, loss = 0.69283344\n",
      "Iteration 1465, loss = 0.69283157\n",
      "Iteration 1466, loss = 0.69283041\n",
      "Iteration 1467, loss = 0.69282927\n",
      "Iteration 1468, loss = 0.69282730\n",
      "Iteration 1469, loss = 0.69282544\n",
      "Iteration 1470, loss = 0.69282333\n",
      "Iteration 1471, loss = 0.69282197\n",
      "Iteration 1472, loss = 0.69282021\n",
      "Iteration 1473, loss = 0.69281833\n",
      "Iteration 1474, loss = 0.69281671\n",
      "Iteration 1475, loss = 0.69281507\n",
      "Iteration 1476, loss = 0.69281319\n",
      "Iteration 1477, loss = 0.69281180\n",
      "Iteration 1478, loss = 0.69281056\n",
      "Iteration 1479, loss = 0.69280881\n",
      "Iteration 1480, loss = 0.69280735\n",
      "Iteration 1481, loss = 0.69280545\n",
      "Iteration 1482, loss = 0.69280389\n",
      "Iteration 1483, loss = 0.69280256\n",
      "Iteration 1484, loss = 0.69280071\n",
      "Iteration 1485, loss = 0.69279900\n",
      "Iteration 1486, loss = 0.69279761\n",
      "Iteration 1487, loss = 0.69279540\n",
      "Iteration 1488, loss = 0.69279418\n",
      "Iteration 1489, loss = 0.69279270\n",
      "Iteration 1490, loss = 0.69279043\n",
      "Iteration 1491, loss = 0.69278903\n",
      "Iteration 1492, loss = 0.69278686\n",
      "Iteration 1493, loss = 0.69278535\n",
      "Iteration 1494, loss = 0.69278405\n",
      "Iteration 1495, loss = 0.69278212\n",
      "Iteration 1496, loss = 0.69278070\n",
      "Iteration 1497, loss = 0.69277908\n",
      "Iteration 1498, loss = 0.69277887\n",
      "Iteration 1499, loss = 0.69277585\n",
      "Iteration 1500, loss = 0.69277453\n",
      "Iteration 1501, loss = 0.69277286\n",
      "Iteration 1502, loss = 0.69277233\n",
      "Iteration 1503, loss = 0.69276979\n",
      "Iteration 1504, loss = 0.69276797\n",
      "Iteration 1505, loss = 0.69276645\n",
      "Iteration 1506, loss = 0.69276516\n",
      "Iteration 1507, loss = 0.69276351\n",
      "Iteration 1508, loss = 0.69276164\n",
      "Iteration 1509, loss = 0.69276010\n",
      "Iteration 1510, loss = 0.69275888\n",
      "Iteration 1511, loss = 0.69275688\n",
      "Iteration 1512, loss = 0.69275547\n",
      "Iteration 1513, loss = 0.69275376\n",
      "Iteration 1514, loss = 0.69275143\n",
      "Iteration 1515, loss = 0.69275000\n",
      "Iteration 1516, loss = 0.69274869\n",
      "Iteration 1517, loss = 0.69274716\n",
      "Iteration 1518, loss = 0.69274552\n",
      "Iteration 1519, loss = 0.69274416\n",
      "Iteration 1520, loss = 0.69274204\n",
      "Iteration 1521, loss = 0.69274083\n",
      "Iteration 1522, loss = 0.69273958\n",
      "Iteration 1523, loss = 0.69273794\n",
      "Iteration 1524, loss = 0.69273585\n",
      "Iteration 1525, loss = 0.69273445\n",
      "Iteration 1526, loss = 0.69273274\n",
      "Iteration 1527, loss = 0.69273180\n",
      "Iteration 1528, loss = 0.69272973\n",
      "Iteration 1529, loss = 0.69272844\n",
      "Iteration 1530, loss = 0.69272760\n",
      "Iteration 1531, loss = 0.69272567\n",
      "Iteration 1532, loss = 0.69272398\n",
      "Iteration 1533, loss = 0.69272236\n",
      "Iteration 1534, loss = 0.69272071\n",
      "Iteration 1535, loss = 0.69271943\n",
      "Iteration 1536, loss = 0.69271762\n",
      "Iteration 1537, loss = 0.69271607\n",
      "Iteration 1538, loss = 0.69271468\n",
      "Iteration 1539, loss = 0.69271307\n",
      "Iteration 1540, loss = 0.69271153\n",
      "Iteration 1541, loss = 0.69271029\n",
      "Iteration 1542, loss = 0.69270894\n",
      "Iteration 1543, loss = 0.69270696\n",
      "Iteration 1544, loss = 0.69270482\n",
      "Iteration 1545, loss = 0.69270398\n",
      "Iteration 1546, loss = 0.69270231\n",
      "Iteration 1547, loss = 0.69270072\n",
      "Iteration 1548, loss = 0.69269947\n",
      "Iteration 1549, loss = 0.69269860\n",
      "Iteration 1550, loss = 0.69269616\n",
      "Iteration 1551, loss = 0.69269441\n",
      "Iteration 1552, loss = 0.69269288\n",
      "Iteration 1553, loss = 0.69269124\n",
      "Iteration 1554, loss = 0.69268950\n",
      "Iteration 1555, loss = 0.69268789\n",
      "Iteration 1556, loss = 0.69268577\n",
      "Iteration 1557, loss = 0.69268397\n",
      "Iteration 1558, loss = 0.69268268\n",
      "Iteration 1559, loss = 0.69268107\n",
      "Iteration 1560, loss = 0.69267971\n",
      "Iteration 1561, loss = 0.69267918\n",
      "Iteration 1562, loss = 0.69267683\n",
      "Iteration 1563, loss = 0.69267556\n",
      "Iteration 1564, loss = 0.69267339\n",
      "Iteration 1565, loss = 0.69267213\n",
      "Iteration 1566, loss = 0.69267070\n",
      "Iteration 1567, loss = 0.69266937\n",
      "Iteration 1568, loss = 0.69266772\n",
      "Iteration 1569, loss = 0.69266693\n",
      "Iteration 1570, loss = 0.69266490\n",
      "Iteration 1571, loss = 0.69266315\n",
      "Iteration 1572, loss = 0.69266158\n",
      "Iteration 1573, loss = 0.69266019\n",
      "Iteration 1574, loss = 0.69265901\n",
      "Iteration 1575, loss = 0.69265732\n",
      "Iteration 1576, loss = 0.69265614\n",
      "Iteration 1577, loss = 0.69265439\n",
      "Iteration 1578, loss = 0.69265304\n",
      "Iteration 1579, loss = 0.69265159\n",
      "Iteration 1580, loss = 0.69265008\n",
      "Iteration 1581, loss = 0.69264877\n",
      "Iteration 1582, loss = 0.69264809\n",
      "Iteration 1583, loss = 0.69264558\n",
      "Iteration 1584, loss = 0.69264403\n",
      "Iteration 1585, loss = 0.69264290\n",
      "Iteration 1586, loss = 0.69264146\n",
      "Iteration 1587, loss = 0.69263986\n",
      "Iteration 1588, loss = 0.69263812\n",
      "Iteration 1589, loss = 0.69263705\n",
      "Iteration 1590, loss = 0.69263596\n",
      "Iteration 1591, loss = 0.69263399\n",
      "Iteration 1592, loss = 0.69263290\n",
      "Iteration 1593, loss = 0.69263165\n",
      "Iteration 1594, loss = 0.69263006\n",
      "Iteration 1595, loss = 0.69262968\n",
      "Iteration 1596, loss = 0.69262763\n",
      "Iteration 1597, loss = 0.69262583\n",
      "Iteration 1598, loss = 0.69262433\n",
      "Iteration 1599, loss = 0.69262346\n",
      "Iteration 1600, loss = 0.69262177\n",
      "Iteration 1601, loss = 0.69262011\n",
      "Iteration 1602, loss = 0.69261844\n",
      "Iteration 1603, loss = 0.69261727\n",
      "Iteration 1604, loss = 0.69261630\n",
      "Iteration 1605, loss = 0.69261450\n",
      "Iteration 1606, loss = 0.69261309\n",
      "Iteration 1607, loss = 0.69261087\n",
      "Iteration 1608, loss = 0.69260945\n",
      "Iteration 1609, loss = 0.69260788\n",
      "Iteration 1610, loss = 0.69260644\n",
      "Iteration 1611, loss = 0.69260570\n",
      "Iteration 1612, loss = 0.69260371\n",
      "Iteration 1613, loss = 0.69260218\n",
      "Iteration 1614, loss = 0.69260093\n",
      "Iteration 1615, loss = 0.69259960\n",
      "Iteration 1616, loss = 0.69259820\n",
      "Iteration 1617, loss = 0.69259674\n",
      "Iteration 1618, loss = 0.69259530\n",
      "Iteration 1619, loss = 0.69259368\n",
      "Iteration 1620, loss = 0.69259189\n",
      "Iteration 1621, loss = 0.69259112\n",
      "Iteration 1622, loss = 0.69259014\n",
      "Iteration 1623, loss = 0.69258834\n",
      "Iteration 1624, loss = 0.69258683\n",
      "Iteration 1625, loss = 0.69258515\n",
      "Iteration 1626, loss = 0.69258346\n",
      "Iteration 1627, loss = 0.69258231\n",
      "Iteration 1628, loss = 0.69258087\n",
      "Iteration 1629, loss = 0.69257926\n",
      "Iteration 1630, loss = 0.69257803\n",
      "Iteration 1631, loss = 0.69257680\n",
      "Iteration 1632, loss = 0.69257581\n",
      "Iteration 1633, loss = 0.69257421\n",
      "Iteration 1634, loss = 0.69257281\n",
      "Iteration 1635, loss = 0.69257130\n",
      "Iteration 1636, loss = 0.69257079\n",
      "Iteration 1637, loss = 0.69256875\n",
      "Iteration 1638, loss = 0.69256743\n",
      "Iteration 1639, loss = 0.69256587\n",
      "Iteration 1640, loss = 0.69256405\n",
      "Iteration 1641, loss = 0.69256257\n",
      "Iteration 1642, loss = 0.69256066\n",
      "Iteration 1643, loss = 0.69255929\n",
      "Iteration 1644, loss = 0.69255831\n",
      "Iteration 1645, loss = 0.69255696\n",
      "Iteration 1646, loss = 0.69255521\n",
      "Iteration 1647, loss = 0.69255407\n",
      "Iteration 1648, loss = 0.69255261\n",
      "Iteration 1649, loss = 0.69255131\n",
      "Iteration 1650, loss = 0.69254992\n",
      "Iteration 1651, loss = 0.69254840\n",
      "Iteration 1652, loss = 0.69254718\n",
      "Iteration 1653, loss = 0.69254545\n",
      "Iteration 1654, loss = 0.69254410\n",
      "Iteration 1655, loss = 0.69254266\n",
      "Iteration 1656, loss = 0.69254122\n",
      "Iteration 1657, loss = 0.69254010\n",
      "Iteration 1658, loss = 0.69253835\n",
      "Iteration 1659, loss = 0.69253730\n",
      "Iteration 1660, loss = 0.69253571\n",
      "Iteration 1661, loss = 0.69253505\n",
      "Iteration 1662, loss = 0.69253344\n",
      "Iteration 1663, loss = 0.69253200\n",
      "Iteration 1664, loss = 0.69252998\n",
      "Iteration 1665, loss = 0.69252886\n",
      "Iteration 1666, loss = 0.69252736\n",
      "Iteration 1667, loss = 0.69252547\n",
      "Iteration 1668, loss = 0.69252439\n",
      "Iteration 1669, loss = 0.69252301\n",
      "Iteration 1670, loss = 0.69252152\n",
      "Iteration 1671, loss = 0.69251990\n",
      "Iteration 1672, loss = 0.69251915\n",
      "Iteration 1673, loss = 0.69251712\n",
      "Iteration 1674, loss = 0.69251630\n",
      "Iteration 1675, loss = 0.69251453\n",
      "Iteration 1676, loss = 0.69251345\n",
      "Iteration 1677, loss = 0.69251241\n",
      "Iteration 1678, loss = 0.69251105\n",
      "Iteration 1679, loss = 0.69250931\n",
      "Iteration 1680, loss = 0.69250821\n",
      "Iteration 1681, loss = 0.69250646\n",
      "Iteration 1682, loss = 0.69250511\n",
      "Iteration 1683, loss = 0.69250398\n",
      "Iteration 1684, loss = 0.69250234\n",
      "Iteration 1685, loss = 0.69250140\n",
      "Iteration 1686, loss = 0.69249947\n",
      "Iteration 1687, loss = 0.69249871\n",
      "Iteration 1688, loss = 0.69249708\n",
      "Iteration 1689, loss = 0.69249594\n",
      "Iteration 1690, loss = 0.69249429\n",
      "Iteration 1691, loss = 0.69249278\n",
      "Iteration 1692, loss = 0.69249156\n",
      "Iteration 1693, loss = 0.69249059\n",
      "Iteration 1694, loss = 0.69248877\n",
      "Iteration 1695, loss = 0.69248742\n",
      "Iteration 1696, loss = 0.69248613\n",
      "Iteration 1697, loss = 0.69248502\n",
      "Iteration 1698, loss = 0.69248340\n",
      "Iteration 1699, loss = 0.69248183\n",
      "Iteration 1700, loss = 0.69248087\n",
      "Iteration 1701, loss = 0.69247902\n",
      "Iteration 1702, loss = 0.69247747\n",
      "Iteration 1703, loss = 0.69247593\n",
      "Iteration 1704, loss = 0.69247466\n",
      "Iteration 1705, loss = 0.69247314\n",
      "Iteration 1706, loss = 0.69247197\n",
      "Iteration 1707, loss = 0.69247077\n",
      "Iteration 1708, loss = 0.69246989\n",
      "Iteration 1709, loss = 0.69246835\n",
      "Iteration 1710, loss = 0.69246789\n",
      "Iteration 1711, loss = 0.69246577\n",
      "Iteration 1712, loss = 0.69246403\n",
      "Iteration 1713, loss = 0.69246281\n",
      "Iteration 1714, loss = 0.69246147\n",
      "Iteration 1715, loss = 0.69246029\n",
      "Iteration 1716, loss = 0.69245852\n",
      "Iteration 1717, loss = 0.69245711\n",
      "Iteration 1718, loss = 0.69245575\n",
      "Iteration 1719, loss = 0.69245406\n",
      "Iteration 1720, loss = 0.69245288\n",
      "Iteration 1721, loss = 0.69245194\n",
      "Iteration 1722, loss = 0.69245024\n",
      "Iteration 1723, loss = 0.69244871\n",
      "Iteration 1724, loss = 0.69244797\n",
      "Iteration 1725, loss = 0.69244633\n",
      "Iteration 1726, loss = 0.69244460\n",
      "Iteration 1727, loss = 0.69244363\n",
      "Iteration 1728, loss = 0.69244177\n",
      "Iteration 1729, loss = 0.69244057\n",
      "Iteration 1730, loss = 0.69243907\n",
      "Iteration 1731, loss = 0.69243823\n",
      "Iteration 1732, loss = 0.69243596\n",
      "Iteration 1733, loss = 0.69243516\n",
      "Iteration 1734, loss = 0.69243319\n",
      "Iteration 1735, loss = 0.69243200\n",
      "Iteration 1736, loss = 0.69243073\n",
      "Iteration 1737, loss = 0.69242926\n",
      "Iteration 1738, loss = 0.69242788\n",
      "Iteration 1739, loss = 0.69242662\n",
      "Iteration 1740, loss = 0.69242513\n",
      "Iteration 1741, loss = 0.69242409\n",
      "Iteration 1742, loss = 0.69242253\n",
      "Iteration 1743, loss = 0.69242084\n",
      "Iteration 1744, loss = 0.69241995\n",
      "Iteration 1745, loss = 0.69241844\n",
      "Iteration 1746, loss = 0.69241698\n",
      "Iteration 1747, loss = 0.69241569\n",
      "Iteration 1748, loss = 0.69241424\n",
      "Iteration 1749, loss = 0.69241284\n",
      "Iteration 1750, loss = 0.69241221\n",
      "Iteration 1751, loss = 0.69240992\n",
      "Iteration 1752, loss = 0.69240891\n",
      "Iteration 1753, loss = 0.69240742\n",
      "Iteration 1754, loss = 0.69240594\n",
      "Iteration 1755, loss = 0.69240465\n",
      "Iteration 1756, loss = 0.69240348\n",
      "Iteration 1757, loss = 0.69240216\n",
      "Iteration 1758, loss = 0.69240098\n",
      "Iteration 1759, loss = 0.69239928\n",
      "Iteration 1760, loss = 0.69239857\n",
      "Iteration 1761, loss = 0.69239654\n",
      "Iteration 1762, loss = 0.69239527\n",
      "Iteration 1763, loss = 0.69239387\n",
      "Iteration 1764, loss = 0.69239224\n",
      "Iteration 1765, loss = 0.69239130\n",
      "Iteration 1766, loss = 0.69238993\n",
      "Iteration 1767, loss = 0.69238854\n",
      "Iteration 1768, loss = 0.69238714\n",
      "Iteration 1769, loss = 0.69238611\n",
      "Iteration 1770, loss = 0.69238461\n",
      "Iteration 1771, loss = 0.69238362\n",
      "Iteration 1772, loss = 0.69238209\n",
      "Iteration 1773, loss = 0.69238054\n",
      "Iteration 1774, loss = 0.69238045\n",
      "Iteration 1775, loss = 0.69237800\n",
      "Iteration 1776, loss = 0.69237680\n",
      "Iteration 1777, loss = 0.69237508\n",
      "Iteration 1778, loss = 0.69237432\n",
      "Iteration 1779, loss = 0.69237246\n",
      "Iteration 1780, loss = 0.69237107\n",
      "Iteration 1781, loss = 0.69237007\n",
      "Iteration 1782, loss = 0.69236836\n",
      "Iteration 1783, loss = 0.69236716\n",
      "Iteration 1784, loss = 0.69236590\n",
      "Iteration 1785, loss = 0.69236459\n",
      "Iteration 1786, loss = 0.69236331\n",
      "Iteration 1787, loss = 0.69236189\n",
      "Iteration 1788, loss = 0.69236057\n",
      "Iteration 1789, loss = 0.69235956\n",
      "Iteration 1790, loss = 0.69235836\n",
      "Iteration 1791, loss = 0.69235676\n",
      "Iteration 1792, loss = 0.69235553\n",
      "Iteration 1793, loss = 0.69235421\n",
      "Iteration 1794, loss = 0.69235371\n",
      "Iteration 1795, loss = 0.69235174\n",
      "Iteration 1796, loss = 0.69235001\n",
      "Iteration 1797, loss = 0.69234924\n",
      "Iteration 1798, loss = 0.69234672\n",
      "Iteration 1799, loss = 0.69234576\n",
      "Iteration 1800, loss = 0.69234505\n",
      "Iteration 1801, loss = 0.69234326\n",
      "Iteration 1802, loss = 0.69234169\n",
      "Iteration 1803, loss = 0.69234075\n",
      "Iteration 1804, loss = 0.69233908\n",
      "Iteration 1805, loss = 0.69233800\n",
      "Iteration 1806, loss = 0.69233652\n",
      "Iteration 1807, loss = 0.69233485\n",
      "Iteration 1808, loss = 0.69233355\n",
      "Iteration 1809, loss = 0.69233263\n",
      "Iteration 1810, loss = 0.69233115\n",
      "Iteration 1811, loss = 0.69233024\n",
      "Iteration 1812, loss = 0.69232944\n",
      "Iteration 1813, loss = 0.69232713\n",
      "Iteration 1814, loss = 0.69232585\n",
      "Iteration 1815, loss = 0.69232428\n",
      "Iteration 1816, loss = 0.69232314\n",
      "Iteration 1817, loss = 0.69232149\n",
      "Iteration 1818, loss = 0.69232043\n",
      "Iteration 1819, loss = 0.69231938\n",
      "Iteration 1820, loss = 0.69231778\n",
      "Iteration 1821, loss = 0.69231661\n",
      "Iteration 1822, loss = 0.69231521\n",
      "Iteration 1823, loss = 0.69231407\n",
      "Iteration 1824, loss = 0.69231314\n",
      "Iteration 1825, loss = 0.69231221\n",
      "Iteration 1826, loss = 0.69231060\n",
      "Iteration 1827, loss = 0.69230938\n",
      "Iteration 1828, loss = 0.69230746\n",
      "Iteration 1829, loss = 0.69230631\n",
      "Iteration 1830, loss = 0.69230529\n",
      "Iteration 1831, loss = 0.69230378\n",
      "Iteration 1832, loss = 0.69230257\n",
      "Iteration 1833, loss = 0.69230093\n",
      "Iteration 1834, loss = 0.69230036\n",
      "Iteration 1835, loss = 0.69229810\n",
      "Iteration 1836, loss = 0.69229709\n",
      "Iteration 1837, loss = 0.69229552\n",
      "Iteration 1838, loss = 0.69229415\n",
      "Iteration 1839, loss = 0.69229308\n",
      "Iteration 1840, loss = 0.69229219\n",
      "Iteration 1841, loss = 0.69229105\n",
      "Iteration 1842, loss = 0.69229003\n",
      "Iteration 1843, loss = 0.69228866\n",
      "Iteration 1844, loss = 0.69228670\n",
      "Iteration 1845, loss = 0.69228635\n",
      "Iteration 1846, loss = 0.69228415\n",
      "Iteration 1847, loss = 0.69228303\n",
      "Iteration 1848, loss = 0.69228175\n",
      "Iteration 1849, loss = 0.69228043\n",
      "Iteration 1850, loss = 0.69227960\n",
      "Iteration 1851, loss = 0.69227751\n",
      "Iteration 1852, loss = 0.69227627\n",
      "Iteration 1853, loss = 0.69227456\n",
      "Iteration 1854, loss = 0.69227312\n",
      "Iteration 1855, loss = 0.69227311\n",
      "Iteration 1856, loss = 0.69227096\n",
      "Iteration 1857, loss = 0.69226982\n",
      "Iteration 1858, loss = 0.69226847\n",
      "Iteration 1859, loss = 0.69226720\n",
      "Iteration 1860, loss = 0.69226568\n",
      "Iteration 1861, loss = 0.69226454\n",
      "Iteration 1862, loss = 0.69226325\n",
      "Iteration 1863, loss = 0.69226227\n",
      "Iteration 1864, loss = 0.69226085\n",
      "Iteration 1865, loss = 0.69226000\n",
      "Iteration 1866, loss = 0.69225848\n",
      "Iteration 1867, loss = 0.69225673\n",
      "Iteration 1868, loss = 0.69225586\n",
      "Iteration 1869, loss = 0.69225476\n",
      "Iteration 1870, loss = 0.69225314\n",
      "Iteration 1871, loss = 0.69225193\n",
      "Iteration 1872, loss = 0.69225065\n",
      "Iteration 1873, loss = 0.69224939\n",
      "Iteration 1874, loss = 0.69224807\n",
      "Iteration 1875, loss = 0.69224665\n",
      "Iteration 1876, loss = 0.69224538\n",
      "Iteration 1877, loss = 0.69224403\n",
      "Iteration 1878, loss = 0.69224279\n",
      "Iteration 1879, loss = 0.69224168\n",
      "Iteration 1880, loss = 0.69224002\n",
      "Iteration 1881, loss = 0.69223883\n",
      "Iteration 1882, loss = 0.69223751\n",
      "Iteration 1883, loss = 0.69223652\n",
      "Iteration 1884, loss = 0.69223579\n",
      "Iteration 1885, loss = 0.69223345\n",
      "Iteration 1886, loss = 0.69223231\n",
      "Iteration 1887, loss = 0.69223098\n",
      "Iteration 1888, loss = 0.69223010\n",
      "Iteration 1889, loss = 0.69222875\n",
      "Iteration 1890, loss = 0.69222742\n",
      "Iteration 1891, loss = 0.69222603\n",
      "Iteration 1892, loss = 0.69222508\n",
      "Iteration 1893, loss = 0.69222412\n",
      "Iteration 1894, loss = 0.69222228\n",
      "Iteration 1895, loss = 0.69222127\n",
      "Iteration 1896, loss = 0.69222002\n",
      "Iteration 1897, loss = 0.69221897\n",
      "Iteration 1898, loss = 0.69221815\n",
      "Iteration 1899, loss = 0.69221651\n",
      "Iteration 1900, loss = 0.69221513\n",
      "Iteration 1901, loss = 0.69221405\n",
      "Iteration 1902, loss = 0.69221326\n",
      "Iteration 1903, loss = 0.69221153\n",
      "Iteration 1904, loss = 0.69221059\n",
      "Iteration 1905, loss = 0.69220949\n",
      "Iteration 1906, loss = 0.69220856\n",
      "Iteration 1907, loss = 0.69220703\n",
      "Iteration 1908, loss = 0.69220558\n",
      "Iteration 1909, loss = 0.69220461\n",
      "Iteration 1910, loss = 0.69220296\n",
      "Iteration 1911, loss = 0.69220195\n",
      "Iteration 1912, loss = 0.69220101\n",
      "Iteration 1913, loss = 0.69219991\n",
      "Iteration 1914, loss = 0.69219894\n",
      "Iteration 1915, loss = 0.69219740\n",
      "Iteration 1916, loss = 0.69219664\n",
      "Iteration 1917, loss = 0.69219521\n",
      "Iteration 1918, loss = 0.69219419\n",
      "Iteration 1919, loss = 0.69219286\n",
      "Iteration 1920, loss = 0.69219208\n",
      "Iteration 1921, loss = 0.69219033\n",
      "Iteration 1922, loss = 0.69218913\n",
      "Iteration 1923, loss = 0.69218813\n",
      "Iteration 1924, loss = 0.69218683\n",
      "Iteration 1925, loss = 0.69218553\n",
      "Iteration 1926, loss = 0.69218447\n",
      "Iteration 1927, loss = 0.69218368\n",
      "Iteration 1928, loss = 0.69218211\n",
      "Iteration 1929, loss = 0.69218077\n",
      "Iteration 1930, loss = 0.69217985\n",
      "Iteration 1931, loss = 0.69217850\n",
      "Iteration 1932, loss = 0.69217733\n",
      "Iteration 1933, loss = 0.69217632\n",
      "Iteration 1934, loss = 0.69217569\n",
      "Iteration 1935, loss = 0.69217366\n",
      "Iteration 1936, loss = 0.69217258\n",
      "Iteration 1937, loss = 0.69217158\n",
      "Iteration 1938, loss = 0.69217083\n",
      "Iteration 1939, loss = 0.69216913\n",
      "Iteration 1940, loss = 0.69216793\n",
      "Iteration 1941, loss = 0.69216743\n",
      "Iteration 1942, loss = 0.69216544\n",
      "Iteration 1943, loss = 0.69216534\n",
      "Iteration 1944, loss = 0.69216339\n",
      "Iteration 1945, loss = 0.69216256\n",
      "Iteration 1946, loss = 0.69216085\n",
      "Iteration 1947, loss = 0.69215930\n",
      "Iteration 1948, loss = 0.69215856\n",
      "Iteration 1949, loss = 0.69215750\n",
      "Iteration 1950, loss = 0.69215621\n",
      "Iteration 1951, loss = 0.69215491\n",
      "Iteration 1952, loss = 0.69215390\n",
      "Iteration 1953, loss = 0.69215278\n",
      "Iteration 1954, loss = 0.69215154\n",
      "Iteration 1955, loss = 0.69215015\n",
      "Iteration 1956, loss = 0.69214875\n",
      "Iteration 1957, loss = 0.69214747\n",
      "Iteration 1958, loss = 0.69214666\n",
      "Iteration 1959, loss = 0.69214509\n",
      "Iteration 1960, loss = 0.69214421\n",
      "Iteration 1961, loss = 0.69214365\n",
      "Iteration 1962, loss = 0.69214176\n",
      "Iteration 1963, loss = 0.69213996\n",
      "Iteration 1964, loss = 0.69213882\n",
      "Iteration 1965, loss = 0.69213781\n",
      "Iteration 1966, loss = 0.69213669\n",
      "Iteration 1967, loss = 0.69213566\n",
      "Iteration 1968, loss = 0.69213434\n",
      "Iteration 1969, loss = 0.69213346\n",
      "Iteration 1970, loss = 0.69213272\n",
      "Iteration 1971, loss = 0.69213108\n",
      "Iteration 1972, loss = 0.69212982\n",
      "Iteration 1973, loss = 0.69212859\n",
      "Iteration 1974, loss = 0.69212753\n",
      "Iteration 1975, loss = 0.69212623\n",
      "Iteration 1976, loss = 0.69212558\n",
      "Iteration 1977, loss = 0.69212416\n",
      "Iteration 1978, loss = 0.69212276\n",
      "Iteration 1979, loss = 0.69212140\n",
      "Iteration 1980, loss = 0.69212012\n",
      "Iteration 1981, loss = 0.69211954\n",
      "Iteration 1982, loss = 0.69211801\n",
      "Iteration 1983, loss = 0.69211723\n",
      "Iteration 1984, loss = 0.69211596\n",
      "Iteration 1985, loss = 0.69211481\n",
      "Iteration 1986, loss = 0.69211344\n",
      "Iteration 1987, loss = 0.69211199\n",
      "Iteration 1988, loss = 0.69211040\n",
      "Iteration 1989, loss = 0.69211018\n",
      "Iteration 1990, loss = 0.69210839\n",
      "Iteration 1991, loss = 0.69210696\n",
      "Iteration 1992, loss = 0.69210567\n",
      "Iteration 1993, loss = 0.69210452\n",
      "Iteration 1994, loss = 0.69210347\n",
      "Iteration 1995, loss = 0.69210246\n",
      "Iteration 1996, loss = 0.69210096\n",
      "Iteration 1997, loss = 0.69209982\n",
      "Iteration 1998, loss = 0.69209871\n",
      "Iteration 1999, loss = 0.69209703\n",
      "Iteration 2000, loss = 0.69209597\n",
      "Iteration 2001, loss = 0.69209515\n",
      "Iteration 2002, loss = 0.69209366\n",
      "Iteration 2003, loss = 0.69209243\n",
      "Iteration 2004, loss = 0.69209131\n",
      "Iteration 2005, loss = 0.69209005\n",
      "Iteration 2006, loss = 0.69208895\n",
      "Iteration 2007, loss = 0.69208756\n",
      "Iteration 2008, loss = 0.69208728\n",
      "Iteration 2009, loss = 0.69208559\n",
      "Iteration 2010, loss = 0.69208595\n",
      "Iteration 2011, loss = 0.69208306\n",
      "Iteration 2012, loss = 0.69208176\n",
      "Iteration 2013, loss = 0.69208063\n",
      "Iteration 2014, loss = 0.69207976\n",
      "Iteration 2015, loss = 0.69207799\n",
      "Iteration 2016, loss = 0.69207726\n",
      "Iteration 2017, loss = 0.69207595\n",
      "Iteration 2018, loss = 0.69207422\n",
      "Iteration 2019, loss = 0.69207394\n",
      "Iteration 2020, loss = 0.69207211\n",
      "Iteration 2021, loss = 0.69207078\n",
      "Iteration 2022, loss = 0.69206988\n",
      "Iteration 2023, loss = 0.69206839\n",
      "Iteration 2024, loss = 0.69206762\n",
      "Iteration 2025, loss = 0.69206624\n",
      "Iteration 2026, loss = 0.69206482\n",
      "Iteration 2027, loss = 0.69206446\n",
      "Iteration 2028, loss = 0.69206234\n",
      "Iteration 2029, loss = 0.69206128\n",
      "Iteration 2030, loss = 0.69206114\n",
      "Iteration 2031, loss = 0.69205944\n",
      "Iteration 2032, loss = 0.69205822\n",
      "Iteration 2033, loss = 0.69205697\n",
      "Iteration 2034, loss = 0.69205557\n",
      "Iteration 2035, loss = 0.69205422\n",
      "Iteration 2036, loss = 0.69205310\n",
      "Iteration 2037, loss = 0.69205238\n",
      "Iteration 2038, loss = 0.69205107\n",
      "Iteration 2039, loss = 0.69204989\n",
      "Iteration 2040, loss = 0.69204872\n",
      "Iteration 2041, loss = 0.69204794\n",
      "Iteration 2042, loss = 0.69204657\n",
      "Iteration 2043, loss = 0.69204503\n",
      "Iteration 2044, loss = 0.69204408\n",
      "Iteration 2045, loss = 0.69204352\n",
      "Iteration 2046, loss = 0.69204196\n",
      "Iteration 2047, loss = 0.69204051\n",
      "Iteration 2048, loss = 0.69203949\n",
      "Iteration 2049, loss = 0.69203849\n",
      "Iteration 2050, loss = 0.69203699\n",
      "Iteration 2051, loss = 0.69203614\n",
      "Iteration 2052, loss = 0.69203502\n",
      "Iteration 2053, loss = 0.69203351\n",
      "Iteration 2054, loss = 0.69203242\n",
      "Iteration 2055, loss = 0.69203168\n",
      "Iteration 2056, loss = 0.69203047\n",
      "Iteration 2057, loss = 0.69202949\n",
      "Iteration 2058, loss = 0.69202824\n",
      "Iteration 2059, loss = 0.69202715\n",
      "Iteration 2060, loss = 0.69202667\n",
      "Iteration 2061, loss = 0.69202491\n",
      "Iteration 2062, loss = 0.69202389\n",
      "Iteration 2063, loss = 0.69202261\n",
      "Iteration 2064, loss = 0.69202170\n",
      "Iteration 2065, loss = 0.69201998\n",
      "Iteration 2066, loss = 0.69201928\n",
      "Iteration 2067, loss = 0.69201805\n",
      "Iteration 2068, loss = 0.69201734\n",
      "Iteration 2069, loss = 0.69201577\n",
      "Iteration 2070, loss = 0.69201478\n",
      "Iteration 2071, loss = 0.69201358\n",
      "Iteration 2072, loss = 0.69201261\n",
      "Iteration 2073, loss = 0.69201125\n",
      "Iteration 2074, loss = 0.69201040\n",
      "Iteration 2075, loss = 0.69200941\n",
      "Iteration 2076, loss = 0.69200762\n",
      "Iteration 2077, loss = 0.69200651\n",
      "Iteration 2078, loss = 0.69200611\n",
      "Iteration 2079, loss = 0.69200436\n",
      "Iteration 2080, loss = 0.69200330\n",
      "Iteration 2081, loss = 0.69200224\n",
      "Iteration 2082, loss = 0.69200290\n",
      "Iteration 2083, loss = 0.69200020\n",
      "Iteration 2084, loss = 0.69199901\n",
      "Iteration 2085, loss = 0.69199755\n",
      "Iteration 2086, loss = 0.69199643\n",
      "Iteration 2087, loss = 0.69199562\n",
      "Iteration 2088, loss = 0.69199420\n",
      "Iteration 2089, loss = 0.69199310\n",
      "Iteration 2090, loss = 0.69199152\n",
      "Iteration 2091, loss = 0.69199096\n",
      "Iteration 2092, loss = 0.69198936\n",
      "Iteration 2093, loss = 0.69198814\n",
      "Iteration 2094, loss = 0.69198706\n",
      "Iteration 2095, loss = 0.69198638\n",
      "Iteration 2096, loss = 0.69198463\n",
      "Iteration 2097, loss = 0.69198413\n",
      "Iteration 2098, loss = 0.69198262\n",
      "Iteration 2099, loss = 0.69198130\n",
      "Iteration 2100, loss = 0.69198051\n",
      "Iteration 2101, loss = 0.69197944\n",
      "Iteration 2102, loss = 0.69197754\n",
      "Iteration 2103, loss = 0.69197668\n",
      "Iteration 2104, loss = 0.69197601\n",
      "Iteration 2105, loss = 0.69197465\n",
      "Iteration 2106, loss = 0.69197323\n",
      "Iteration 2107, loss = 0.69197218\n",
      "Iteration 2108, loss = 0.69197071\n",
      "Iteration 2109, loss = 0.69196943\n",
      "Iteration 2110, loss = 0.69196866\n",
      "Iteration 2111, loss = 0.69196737\n",
      "Iteration 2112, loss = 0.69196653\n",
      "Iteration 2113, loss = 0.69196527\n",
      "Iteration 2114, loss = 0.69196394\n",
      "Iteration 2115, loss = 0.69196310\n",
      "Iteration 2116, loss = 0.69196189\n",
      "Iteration 2117, loss = 0.69196040\n",
      "Iteration 2118, loss = 0.69195979\n",
      "Iteration 2119, loss = 0.69195831\n",
      "Iteration 2120, loss = 0.69195742\n",
      "Iteration 2121, loss = 0.69195627\n",
      "Iteration 2122, loss = 0.69195561\n",
      "Iteration 2123, loss = 0.69195372\n",
      "Iteration 2124, loss = 0.69195274\n",
      "Iteration 2125, loss = 0.69195131\n",
      "Iteration 2126, loss = 0.69195033\n",
      "Iteration 2127, loss = 0.69194871\n",
      "Iteration 2128, loss = 0.69194769\n",
      "Iteration 2129, loss = 0.69194685\n",
      "Iteration 2130, loss = 0.69194539\n",
      "Iteration 2131, loss = 0.69194440\n",
      "Iteration 2132, loss = 0.69194294\n",
      "Iteration 2133, loss = 0.69194209\n",
      "Iteration 2134, loss = 0.69194082\n",
      "Iteration 2135, loss = 0.69193976\n",
      "Iteration 2136, loss = 0.69193892\n",
      "Iteration 2137, loss = 0.69193755\n",
      "Iteration 2138, loss = 0.69193637\n",
      "Iteration 2139, loss = 0.69193541\n",
      "Iteration 2140, loss = 0.69193391\n",
      "Iteration 2141, loss = 0.69193308\n",
      "Iteration 2142, loss = 0.69193160\n",
      "Iteration 2143, loss = 0.69193058\n",
      "Iteration 2144, loss = 0.69192949\n",
      "Iteration 2145, loss = 0.69192861\n",
      "Iteration 2146, loss = 0.69192744\n",
      "Iteration 2147, loss = 0.69192609\n",
      "Iteration 2148, loss = 0.69192495\n",
      "Iteration 2149, loss = 0.69192409\n",
      "Iteration 2150, loss = 0.69192259\n",
      "Iteration 2151, loss = 0.69192184\n",
      "Iteration 2152, loss = 0.69192058\n",
      "Iteration 2153, loss = 0.69191975\n",
      "Iteration 2154, loss = 0.69191922\n",
      "Iteration 2155, loss = 0.69191736\n",
      "Iteration 2156, loss = 0.69191627\n",
      "Iteration 2157, loss = 0.69191506\n",
      "Iteration 2158, loss = 0.69191473\n",
      "Iteration 2159, loss = 0.69191293\n",
      "Iteration 2160, loss = 0.69191203\n",
      "Iteration 2161, loss = 0.69191144\n",
      "Iteration 2162, loss = 0.69190979\n",
      "Iteration 2163, loss = 0.69190883\n",
      "Iteration 2164, loss = 0.69190774\n",
      "Iteration 2165, loss = 0.69190673\n",
      "Iteration 2166, loss = 0.69190588\n",
      "Iteration 2167, loss = 0.69190473\n",
      "Iteration 2168, loss = 0.69190405\n",
      "Iteration 2169, loss = 0.69190269\n",
      "Iteration 2170, loss = 0.69190133\n",
      "Iteration 2171, loss = 0.69190024\n",
      "Iteration 2172, loss = 0.69189960\n",
      "Iteration 2173, loss = 0.69189799\n",
      "Iteration 2174, loss = 0.69189693\n",
      "Iteration 2175, loss = 0.69189583\n",
      "Iteration 2176, loss = 0.69189433\n",
      "Iteration 2177, loss = 0.69189364\n",
      "Iteration 2178, loss = 0.69189235\n",
      "Iteration 2179, loss = 0.69189108\n",
      "Iteration 2180, loss = 0.69189031\n",
      "Iteration 2181, loss = 0.69188913\n",
      "Iteration 2182, loss = 0.69188795\n",
      "Iteration 2183, loss = 0.69188709\n",
      "Iteration 2184, loss = 0.69188585\n",
      "Iteration 2185, loss = 0.69188397\n",
      "Iteration 2186, loss = 0.69188380\n",
      "Iteration 2187, loss = 0.69188228\n",
      "Iteration 2188, loss = 0.69188141\n",
      "Iteration 2189, loss = 0.69188029\n",
      "Iteration 2190, loss = 0.69187922\n",
      "Iteration 2191, loss = 0.69187861\n",
      "Iteration 2192, loss = 0.69187714\n",
      "Iteration 2193, loss = 0.69187652\n",
      "Iteration 2194, loss = 0.69187507\n",
      "Iteration 2195, loss = 0.69187417\n",
      "Iteration 2196, loss = 0.69187302\n",
      "Iteration 2197, loss = 0.69187168\n",
      "Iteration 2198, loss = 0.69187123\n",
      "Iteration 2199, loss = 0.69186998\n",
      "Iteration 2200, loss = 0.69186873\n",
      "Iteration 2201, loss = 0.69186792\n",
      "Iteration 2202, loss = 0.69186663\n",
      "Iteration 2203, loss = 0.69186551\n",
      "Iteration 2204, loss = 0.69186464\n",
      "Iteration 2205, loss = 0.69186349\n",
      "Iteration 2206, loss = 0.69186280\n",
      "Iteration 2207, loss = 0.69186131\n",
      "Iteration 2208, loss = 0.69186052\n",
      "Iteration 2209, loss = 0.69185942\n",
      "Iteration 2210, loss = 0.69185841\n",
      "Iteration 2211, loss = 0.69185772\n",
      "Iteration 2212, loss = 0.69185573\n",
      "Iteration 2213, loss = 0.69185448\n",
      "Iteration 2214, loss = 0.69185344\n",
      "Iteration 2215, loss = 0.69185264\n",
      "Iteration 2216, loss = 0.69185133\n",
      "Iteration 2217, loss = 0.69185102\n",
      "Iteration 2218, loss = 0.69184922\n",
      "Iteration 2219, loss = 0.69184884\n",
      "Iteration 2220, loss = 0.69184685\n",
      "Iteration 2221, loss = 0.69184621\n",
      "Iteration 2222, loss = 0.69184497\n",
      "Iteration 2223, loss = 0.69184400\n",
      "Iteration 2224, loss = 0.69184310\n",
      "Iteration 2225, loss = 0.69184270\n",
      "Iteration 2226, loss = 0.69184146\n",
      "Iteration 2227, loss = 0.69184012\n",
      "Iteration 2228, loss = 0.69183895\n",
      "Iteration 2229, loss = 0.69183763\n",
      "Iteration 2230, loss = 0.69183642\n",
      "Iteration 2231, loss = 0.69183566\n",
      "Iteration 2232, loss = 0.69183430\n",
      "Iteration 2233, loss = 0.69183356\n",
      "Iteration 2234, loss = 0.69183236\n",
      "Iteration 2235, loss = 0.69183220\n",
      "Iteration 2236, loss = 0.69183046\n",
      "Iteration 2237, loss = 0.69182970\n",
      "Iteration 2238, loss = 0.69182835\n",
      "Iteration 2239, loss = 0.69182772\n",
      "Iteration 2240, loss = 0.69182668\n",
      "Iteration 2241, loss = 0.69182573\n",
      "Iteration 2242, loss = 0.69182423\n",
      "Iteration 2243, loss = 0.69182397\n",
      "Iteration 2244, loss = 0.69182274\n",
      "Iteration 2245, loss = 0.69182168\n",
      "Iteration 2246, loss = 0.69182037\n",
      "Iteration 2247, loss = 0.69181958\n",
      "Iteration 2248, loss = 0.69181846\n",
      "Iteration 2249, loss = 0.69181809\n",
      "Iteration 2250, loss = 0.69181651\n",
      "Iteration 2251, loss = 0.69181510\n",
      "Iteration 2252, loss = 0.69181482\n",
      "Iteration 2253, loss = 0.69181291\n",
      "Iteration 2254, loss = 0.69181191\n",
      "Iteration 2255, loss = 0.69181099\n",
      "Iteration 2256, loss = 0.69180974\n",
      "Iteration 2257, loss = 0.69180890\n",
      "Iteration 2258, loss = 0.69180785\n",
      "Iteration 2259, loss = 0.69180685\n",
      "Iteration 2260, loss = 0.69180619\n",
      "Iteration 2261, loss = 0.69180538\n",
      "Iteration 2262, loss = 0.69180392\n",
      "Iteration 2263, loss = 0.69180335\n",
      "Iteration 2264, loss = 0.69180155\n",
      "Iteration 2265, loss = 0.69180049\n",
      "Iteration 2266, loss = 0.69179891\n",
      "Iteration 2267, loss = 0.69179792\n",
      "Iteration 2268, loss = 0.69179726\n",
      "Iteration 2269, loss = 0.69179592\n",
      "Iteration 2270, loss = 0.69179481\n",
      "Iteration 2271, loss = 0.69179445\n",
      "Iteration 2272, loss = 0.69179322\n",
      "Iteration 2273, loss = 0.69179175\n",
      "Iteration 2274, loss = 0.69179108\n",
      "Iteration 2275, loss = 0.69178978\n",
      "Iteration 2276, loss = 0.69178873\n",
      "Iteration 2277, loss = 0.69178796\n",
      "Iteration 2278, loss = 0.69178660\n",
      "Iteration 2279, loss = 0.69178579\n",
      "Iteration 2280, loss = 0.69178507\n",
      "Iteration 2281, loss = 0.69178337\n",
      "Iteration 2282, loss = 0.69178253\n",
      "Iteration 2283, loss = 0.69178158\n",
      "Iteration 2284, loss = 0.69178121\n",
      "Iteration 2285, loss = 0.69177951\n",
      "Iteration 2286, loss = 0.69177835\n",
      "Iteration 2287, loss = 0.69177754\n",
      "Iteration 2288, loss = 0.69177659\n",
      "Iteration 2289, loss = 0.69177511\n",
      "Iteration 2290, loss = 0.69177448\n",
      "Iteration 2291, loss = 0.69177336\n",
      "Iteration 2292, loss = 0.69177248\n",
      "Iteration 2293, loss = 0.69177155\n",
      "Iteration 2294, loss = 0.69177036\n",
      "Iteration 2295, loss = 0.69176934\n",
      "Iteration 2296, loss = 0.69176829\n",
      "Iteration 2297, loss = 0.69176724\n",
      "Iteration 2298, loss = 0.69176650\n",
      "Iteration 2299, loss = 0.69176475\n",
      "Iteration 2300, loss = 0.69176349\n",
      "Iteration 2301, loss = 0.69176262\n",
      "Iteration 2302, loss = 0.69176257\n",
      "Iteration 2303, loss = 0.69176076\n",
      "Iteration 2304, loss = 0.69175963\n",
      "Iteration 2305, loss = 0.69175906\n",
      "Iteration 2306, loss = 0.69175795\n",
      "Iteration 2307, loss = 0.69175650\n",
      "Iteration 2308, loss = 0.69175541\n",
      "Iteration 2309, loss = 0.69175513\n",
      "Iteration 2310, loss = 0.69175387\n",
      "Iteration 2311, loss = 0.69175288\n",
      "Iteration 2312, loss = 0.69175185\n",
      "Iteration 2313, loss = 0.69175034\n",
      "Iteration 2314, loss = 0.69174963\n",
      "Iteration 2315, loss = 0.69174854\n",
      "Iteration 2316, loss = 0.69174752\n",
      "Iteration 2317, loss = 0.69174673\n",
      "Iteration 2318, loss = 0.69174619\n",
      "Iteration 2319, loss = 0.69174533\n",
      "Iteration 2320, loss = 0.69174365\n",
      "Iteration 2321, loss = 0.69174261\n",
      "Iteration 2322, loss = 0.69174151\n",
      "Iteration 2323, loss = 0.69174063\n",
      "Iteration 2324, loss = 0.69173930\n",
      "Iteration 2325, loss = 0.69173865\n",
      "Iteration 2326, loss = 0.69173732\n",
      "Iteration 2327, loss = 0.69173644\n",
      "Iteration 2328, loss = 0.69173576\n",
      "Iteration 2329, loss = 0.69173478\n",
      "Iteration 2330, loss = 0.69173345\n",
      "Iteration 2331, loss = 0.69173267\n",
      "Iteration 2332, loss = 0.69173144\n",
      "Iteration 2333, loss = 0.69173086\n",
      "Iteration 2334, loss = 0.69172971\n",
      "Iteration 2335, loss = 0.69172866\n",
      "Iteration 2336, loss = 0.69172756\n",
      "Iteration 2337, loss = 0.69172701\n",
      "Iteration 2338, loss = 0.69172589\n",
      "Iteration 2339, loss = 0.69172504\n",
      "Iteration 2340, loss = 0.69172368\n",
      "Iteration 2341, loss = 0.69172330\n",
      "Iteration 2342, loss = 0.69172143\n",
      "Iteration 2343, loss = 0.69172082\n",
      "Iteration 2344, loss = 0.69171984\n",
      "Iteration 2345, loss = 0.69171900\n",
      "Iteration 2346, loss = 0.69171805\n",
      "Iteration 2347, loss = 0.69171704\n",
      "Iteration 2348, loss = 0.69171651\n",
      "Iteration 2349, loss = 0.69171506\n",
      "Iteration 2350, loss = 0.69171407\n",
      "Iteration 2351, loss = 0.69171334\n",
      "Iteration 2352, loss = 0.69171309\n",
      "Iteration 2353, loss = 0.69171113\n",
      "Iteration 2354, loss = 0.69171065\n",
      "Iteration 2355, loss = 0.69170949\n",
      "Iteration 2356, loss = 0.69170890\n",
      "Iteration 2357, loss = 0.69170765\n",
      "Iteration 2358, loss = 0.69170671\n",
      "Iteration 2359, loss = 0.69170583\n",
      "Iteration 2360, loss = 0.69170527\n",
      "Iteration 2361, loss = 0.69170439\n",
      "Iteration 2362, loss = 0.69170309\n",
      "Iteration 2363, loss = 0.69170253\n",
      "Iteration 2364, loss = 0.69170114\n",
      "Iteration 2365, loss = 0.69170013\n",
      "Iteration 2366, loss = 0.69169930\n",
      "Iteration 2367, loss = 0.69169836\n",
      "Iteration 2368, loss = 0.69169704\n",
      "Iteration 2369, loss = 0.69169611\n",
      "Iteration 2370, loss = 0.69169496\n",
      "Iteration 2371, loss = 0.69169445\n",
      "Iteration 2372, loss = 0.69169390\n",
      "Iteration 2373, loss = 0.69169252\n",
      "Iteration 2374, loss = 0.69169216\n",
      "Iteration 2375, loss = 0.69169133\n",
      "Iteration 2376, loss = 0.69168942\n",
      "Iteration 2377, loss = 0.69168862\n",
      "Iteration 2378, loss = 0.69168770\n",
      "Iteration 2379, loss = 0.69168677\n",
      "Iteration 2380, loss = 0.69168580\n",
      "Iteration 2381, loss = 0.69168498\n",
      "Iteration 2382, loss = 0.69168379\n",
      "Iteration 2383, loss = 0.69168281\n",
      "Iteration 2384, loss = 0.69168201\n",
      "Iteration 2385, loss = 0.69168117\n",
      "Iteration 2386, loss = 0.69168021\n",
      "Iteration 2387, loss = 0.69167897\n",
      "Iteration 2388, loss = 0.69167808\n",
      "Iteration 2389, loss = 0.69167774\n",
      "Iteration 2390, loss = 0.69167639\n",
      "Iteration 2391, loss = 0.69167528\n",
      "Iteration 2392, loss = 0.69167429\n",
      "Iteration 2393, loss = 0.69167338\n",
      "Iteration 2394, loss = 0.69167266\n",
      "Iteration 2395, loss = 0.69167134\n",
      "Iteration 2396, loss = 0.69167086\n",
      "Iteration 2397, loss = 0.69166999\n",
      "Iteration 2398, loss = 0.69166862\n",
      "Iteration 2399, loss = 0.69166754\n",
      "Iteration 2400, loss = 0.69166685\n",
      "Iteration 2401, loss = 0.69166574\n",
      "Iteration 2402, loss = 0.69166488\n",
      "Iteration 2403, loss = 0.69166356\n",
      "Iteration 2404, loss = 0.69166279\n",
      "Iteration 2405, loss = 0.69166213\n",
      "Iteration 2406, loss = 0.69166119\n",
      "Iteration 2407, loss = 0.69166036\n",
      "Iteration 2408, loss = 0.69165953\n",
      "Iteration 2409, loss = 0.69165862\n",
      "Iteration 2410, loss = 0.69165735\n",
      "Iteration 2411, loss = 0.69165658\n",
      "Iteration 2412, loss = 0.69165516\n",
      "Iteration 2413, loss = 0.69165410\n",
      "Iteration 2414, loss = 0.69165349\n",
      "Iteration 2415, loss = 0.69165213\n",
      "Iteration 2416, loss = 0.69165160\n",
      "Iteration 2417, loss = 0.69164985\n",
      "Iteration 2418, loss = 0.69164949\n",
      "Iteration 2419, loss = 0.69164786\n",
      "Iteration 2420, loss = 0.69164688\n",
      "Iteration 2421, loss = 0.69164629\n",
      "Iteration 2422, loss = 0.69164643\n",
      "Iteration 2423, loss = 0.69164440\n",
      "Iteration 2424, loss = 0.69164328\n",
      "Iteration 2425, loss = 0.69164251\n",
      "Iteration 2426, loss = 0.69164162\n",
      "Iteration 2427, loss = 0.69164114\n",
      "Iteration 2428, loss = 0.69163952\n",
      "Iteration 2429, loss = 0.69163867\n",
      "Iteration 2430, loss = 0.69163819\n",
      "Iteration 2431, loss = 0.69163681\n",
      "Iteration 2432, loss = 0.69163613\n",
      "Iteration 2433, loss = 0.69163544\n",
      "Iteration 2434, loss = 0.69163462\n",
      "Iteration 2435, loss = 0.69163358\n",
      "Iteration 2436, loss = 0.69163294\n",
      "Iteration 2437, loss = 0.69163180\n",
      "Iteration 2438, loss = 0.69163085\n",
      "Iteration 2439, loss = 0.69163001\n",
      "Iteration 2440, loss = 0.69162936\n",
      "Iteration 2441, loss = 0.69162843\n",
      "Iteration 2442, loss = 0.69162755\n",
      "Iteration 2443, loss = 0.69162680\n",
      "Iteration 2444, loss = 0.69162615\n",
      "Iteration 2445, loss = 0.69162510\n",
      "Iteration 2446, loss = 0.69162453\n",
      "Iteration 2447, loss = 0.69162363\n",
      "Iteration 2448, loss = 0.69162352\n",
      "Iteration 2449, loss = 0.69162114\n",
      "Iteration 2450, loss = 0.69162031\n",
      "Iteration 2451, loss = 0.69162015\n",
      "Iteration 2452, loss = 0.69161869\n",
      "Iteration 2453, loss = 0.69161810\n",
      "Iteration 2454, loss = 0.69161762\n",
      "Iteration 2455, loss = 0.69161598\n",
      "Iteration 2456, loss = 0.69161574\n",
      "Iteration 2457, loss = 0.69161457\n",
      "Iteration 2458, loss = 0.69161396\n",
      "Iteration 2459, loss = 0.69161289\n",
      "Iteration 2460, loss = 0.69161200\n",
      "Iteration 2461, loss = 0.69161149\n",
      "Iteration 2462, loss = 0.69161067\n",
      "Iteration 2463, loss = 0.69160999\n",
      "Iteration 2464, loss = 0.69160886\n",
      "Iteration 2465, loss = 0.69160790\n",
      "Iteration 2466, loss = 0.69160752\n",
      "Iteration 2467, loss = 0.69160652\n",
      "Iteration 2468, loss = 0.69160580\n",
      "Iteration 2469, loss = 0.69160470\n",
      "Iteration 2470, loss = 0.69160449\n",
      "Iteration 2471, loss = 0.69160298\n",
      "Iteration 2472, loss = 0.69160194\n",
      "Iteration 2473, loss = 0.69160125\n",
      "Iteration 2474, loss = 0.69160047\n",
      "Iteration 2475, loss = 0.69160022\n",
      "Iteration 2476, loss = 0.69159936\n",
      "Iteration 2477, loss = 0.69159805\n",
      "Iteration 2478, loss = 0.69159748\n",
      "Iteration 2479, loss = 0.69159654\n",
      "Iteration 2480, loss = 0.69159559\n",
      "Iteration 2481, loss = 0.69159499\n",
      "Iteration 2482, loss = 0.69159440\n",
      "Iteration 2483, loss = 0.69159302\n",
      "Iteration 2484, loss = 0.69159240\n",
      "Iteration 2485, loss = 0.69159123\n",
      "Iteration 2486, loss = 0.69159088\n",
      "Iteration 2487, loss = 0.69158984\n",
      "Iteration 2488, loss = 0.69158892\n",
      "Iteration 2489, loss = 0.69158806\n",
      "Iteration 2490, loss = 0.69158738\n",
      "Iteration 2491, loss = 0.69158596\n",
      "Iteration 2492, loss = 0.69158597\n",
      "Iteration 2493, loss = 0.69158537\n",
      "Iteration 2494, loss = 0.69158366\n",
      "Iteration 2495, loss = 0.69158260\n",
      "Iteration 2496, loss = 0.69158218\n",
      "Iteration 2497, loss = 0.69158095\n",
      "Iteration 2498, loss = 0.69158054\n",
      "Iteration 2499, loss = 0.69157953\n",
      "Iteration 2500, loss = 0.69157877\n",
      "Iteration 2501, loss = 0.69157813\n",
      "Iteration 2502, loss = 0.69157683\n",
      "Iteration 2503, loss = 0.69157633\n",
      "Iteration 2504, loss = 0.69157507\n",
      "Iteration 2505, loss = 0.69157474\n",
      "Iteration 2506, loss = 0.69157373\n",
      "Iteration 2507, loss = 0.69157259\n",
      "Iteration 2508, loss = 0.69157169\n",
      "Iteration 2509, loss = 0.69157072\n",
      "Iteration 2510, loss = 0.69157017\n",
      "Iteration 2511, loss = 0.69156923\n",
      "Iteration 2512, loss = 0.69156827\n",
      "Iteration 2513, loss = 0.69156733\n",
      "Iteration 2514, loss = 0.69156651\n",
      "Iteration 2515, loss = 0.69156589\n",
      "Iteration 2516, loss = 0.69156521\n",
      "Iteration 2517, loss = 0.69156428\n",
      "Iteration 2518, loss = 0.69156346\n",
      "Iteration 2519, loss = 0.69156286\n",
      "Iteration 2520, loss = 0.69156179\n",
      "Iteration 2521, loss = 0.69156141\n",
      "Iteration 2522, loss = 0.69156021\n",
      "Iteration 2523, loss = 0.69155914\n",
      "Iteration 2524, loss = 0.69155832\n",
      "Iteration 2525, loss = 0.69155761\n",
      "Iteration 2526, loss = 0.69155642\n",
      "Iteration 2527, loss = 0.69155572\n",
      "Iteration 2528, loss = 0.69155498\n",
      "Iteration 2529, loss = 0.69155404\n",
      "Iteration 2530, loss = 0.69155305\n",
      "Iteration 2531, loss = 0.69155210\n",
      "Iteration 2532, loss = 0.69155119\n",
      "Iteration 2533, loss = 0.69155018\n",
      "Iteration 2534, loss = 0.69154923\n",
      "Iteration 2535, loss = 0.69154846\n",
      "Iteration 2536, loss = 0.69154782\n",
      "Iteration 2537, loss = 0.69154682\n",
      "Iteration 2538, loss = 0.69154625\n",
      "Iteration 2539, loss = 0.69154554\n",
      "Iteration 2540, loss = 0.69154445\n",
      "Iteration 2541, loss = 0.69154347\n",
      "Iteration 2542, loss = 0.69154279\n",
      "Iteration 2543, loss = 0.69154201\n",
      "Iteration 2544, loss = 0.69154111\n",
      "Iteration 2545, loss = 0.69154026\n",
      "Iteration 2546, loss = 0.69153943\n",
      "Iteration 2547, loss = 0.69153852\n",
      "Iteration 2548, loss = 0.69153780\n",
      "Iteration 2549, loss = 0.69153663\n",
      "Iteration 2550, loss = 0.69153621\n",
      "Iteration 2551, loss = 0.69153566\n",
      "Iteration 2552, loss = 0.69153414\n",
      "Iteration 2553, loss = 0.69153392\n",
      "Iteration 2554, loss = 0.69153279\n",
      "Iteration 2555, loss = 0.69153195\n",
      "Iteration 2556, loss = 0.69153106\n",
      "Iteration 2557, loss = 0.69153017\n",
      "Iteration 2558, loss = 0.69152962\n",
      "Iteration 2559, loss = 0.69152876\n",
      "Iteration 2560, loss = 0.69152778\n",
      "Iteration 2561, loss = 0.69152683\n",
      "Iteration 2562, loss = 0.69152674\n",
      "Iteration 2563, loss = 0.69152539\n",
      "Iteration 2564, loss = 0.69152454\n",
      "Iteration 2565, loss = 0.69152387\n",
      "Iteration 2566, loss = 0.69152278\n",
      "Iteration 2567, loss = 0.69152235\n",
      "Iteration 2568, loss = 0.69152127\n",
      "Iteration 2569, loss = 0.69152024\n",
      "Iteration 2570, loss = 0.69151969\n",
      "Iteration 2571, loss = 0.69151891\n",
      "Iteration 2572, loss = 0.69151824\n",
      "Iteration 2573, loss = 0.69151705\n",
      "Iteration 2574, loss = 0.69151615\n",
      "Iteration 2575, loss = 0.69151553\n",
      "Iteration 2576, loss = 0.69151466\n",
      "Iteration 2577, loss = 0.69151391\n",
      "Iteration 2578, loss = 0.69151337\n",
      "Iteration 2579, loss = 0.69151255\n",
      "Iteration 2580, loss = 0.69151160\n",
      "Iteration 2581, loss = 0.69151049\n",
      "Iteration 2582, loss = 0.69151041\n",
      "Iteration 2583, loss = 0.69151015\n",
      "Iteration 2584, loss = 0.69150818\n",
      "Iteration 2585, loss = 0.69150770\n",
      "Iteration 2586, loss = 0.69150645\n",
      "Iteration 2587, loss = 0.69150611\n",
      "Iteration 2588, loss = 0.69150511\n",
      "Iteration 2589, loss = 0.69150453\n",
      "Iteration 2590, loss = 0.69150454\n",
      "Iteration 2591, loss = 0.69150297\n",
      "Iteration 2592, loss = 0.69150172\n",
      "Iteration 2593, loss = 0.69150098\n",
      "Iteration 2594, loss = 0.69150033\n",
      "Iteration 2595, loss = 0.69149971\n",
      "Iteration 2596, loss = 0.69149859\n",
      "Iteration 2597, loss = 0.69149808\n",
      "Iteration 2598, loss = 0.69149770\n",
      "Iteration 2599, loss = 0.69149631\n",
      "Iteration 2600, loss = 0.69149497\n",
      "Iteration 2601, loss = 0.69149418\n",
      "Iteration 2602, loss = 0.69149329\n",
      "Iteration 2603, loss = 0.69149263\n",
      "Iteration 2604, loss = 0.69149195\n",
      "Iteration 2605, loss = 0.69149094\n",
      "Iteration 2606, loss = 0.69149042\n",
      "Iteration 2607, loss = 0.69148964\n",
      "Iteration 2608, loss = 0.69148878\n",
      "Iteration 2609, loss = 0.69148809\n",
      "Iteration 2610, loss = 0.69148795\n",
      "Iteration 2611, loss = 0.69148783\n",
      "Iteration 2612, loss = 0.69148586\n",
      "Iteration 2613, loss = 0.69148500\n",
      "Iteration 2614, loss = 0.69148451\n",
      "Iteration 2615, loss = 0.69148327\n",
      "Iteration 2616, loss = 0.69148331\n",
      "Iteration 2617, loss = 0.69148190\n",
      "Iteration 2618, loss = 0.69148156\n",
      "Iteration 2619, loss = 0.69148025\n",
      "Iteration 2620, loss = 0.69148051\n",
      "Iteration 2621, loss = 0.69147908\n",
      "Iteration 2622, loss = 0.69147817\n",
      "Iteration 2623, loss = 0.69147721\n",
      "Iteration 2624, loss = 0.69147635\n",
      "Iteration 2625, loss = 0.69147568\n",
      "Iteration 2626, loss = 0.69147480\n",
      "Iteration 2627, loss = 0.69147398\n",
      "Iteration 2628, loss = 0.69147326\n",
      "Iteration 2629, loss = 0.69147243\n",
      "Iteration 2630, loss = 0.69147205\n",
      "Iteration 2631, loss = 0.69147121\n",
      "Iteration 2632, loss = 0.69147013\n",
      "Iteration 2633, loss = 0.69146950\n",
      "Iteration 2634, loss = 0.69146887\n",
      "Iteration 2635, loss = 0.69146764\n",
      "Iteration 2636, loss = 0.69146714\n",
      "Iteration 2637, loss = 0.69146559\n",
      "Iteration 2638, loss = 0.69146559\n",
      "Iteration 2639, loss = 0.69146422\n",
      "Iteration 2640, loss = 0.69146407\n",
      "Iteration 2641, loss = 0.69146332\n",
      "Iteration 2642, loss = 0.69146188\n",
      "Iteration 2643, loss = 0.69146137\n",
      "Iteration 2644, loss = 0.69146073\n",
      "Iteration 2645, loss = 0.69145953\n",
      "Iteration 2646, loss = 0.69145912\n",
      "Iteration 2647, loss = 0.69145801\n",
      "Iteration 2648, loss = 0.69145764\n",
      "Iteration 2649, loss = 0.69145662\n",
      "Iteration 2650, loss = 0.69145579\n",
      "Iteration 2651, loss = 0.69145483\n",
      "Iteration 2652, loss = 0.69145409\n",
      "Iteration 2653, loss = 0.69145328\n",
      "Iteration 2654, loss = 0.69145239\n",
      "Iteration 2655, loss = 0.69145178\n",
      "Iteration 2656, loss = 0.69145101\n",
      "Iteration 2657, loss = 0.69145007\n",
      "Iteration 2658, loss = 0.69144923\n",
      "Iteration 2659, loss = 0.69144840\n",
      "Iteration 2660, loss = 0.69144808\n",
      "Iteration 2661, loss = 0.69144670\n",
      "Iteration 2662, loss = 0.69144599\n",
      "Iteration 2663, loss = 0.69144542\n",
      "Iteration 2664, loss = 0.69144412\n",
      "Iteration 2665, loss = 0.69144355\n",
      "Iteration 2666, loss = 0.69144351\n",
      "Iteration 2667, loss = 0.69144197\n",
      "Iteration 2668, loss = 0.69144186\n",
      "Iteration 2669, loss = 0.69144097\n",
      "Iteration 2670, loss = 0.69143994\n",
      "Iteration 2671, loss = 0.69143877\n",
      "Iteration 2672, loss = 0.69143769\n",
      "Iteration 2673, loss = 0.69143741\n",
      "Iteration 2674, loss = 0.69143638\n",
      "Iteration 2675, loss = 0.69143670\n",
      "Iteration 2676, loss = 0.69143467\n",
      "Iteration 2677, loss = 0.69143454\n",
      "Iteration 2678, loss = 0.69143337\n",
      "Iteration 2679, loss = 0.69143303\n",
      "Iteration 2680, loss = 0.69143158\n",
      "Iteration 2681, loss = 0.69143083\n",
      "Iteration 2682, loss = 0.69143005\n",
      "Iteration 2683, loss = 0.69143012\n",
      "Iteration 2684, loss = 0.69142872\n",
      "Iteration 2685, loss = 0.69142768\n",
      "Iteration 2686, loss = 0.69142710\n",
      "Iteration 2687, loss = 0.69142564\n",
      "Iteration 2688, loss = 0.69142481\n",
      "Iteration 2689, loss = 0.69142418\n",
      "Iteration 2690, loss = 0.69142319\n",
      "Iteration 2691, loss = 0.69142275\n",
      "Iteration 2692, loss = 0.69142166\n",
      "Iteration 2693, loss = 0.69142131\n",
      "Iteration 2694, loss = 0.69142016\n",
      "Iteration 2695, loss = 0.69141912\n",
      "Iteration 2696, loss = 0.69141871\n",
      "Iteration 2697, loss = 0.69141860\n",
      "Iteration 2698, loss = 0.69141766\n",
      "Iteration 2699, loss = 0.69141619\n",
      "Iteration 2700, loss = 0.69141554\n",
      "Iteration 2701, loss = 0.69141482\n",
      "Iteration 2702, loss = 0.69141394\n",
      "Iteration 2703, loss = 0.69141340\n",
      "Iteration 2704, loss = 0.69141253\n",
      "Iteration 2705, loss = 0.69141134\n",
      "Iteration 2706, loss = 0.69141054\n",
      "Iteration 2707, loss = 0.69140973\n",
      "Iteration 2708, loss = 0.69140965\n",
      "Iteration 2709, loss = 0.69140858\n",
      "Iteration 2710, loss = 0.69140775\n",
      "Iteration 2711, loss = 0.69140660\n",
      "Iteration 2712, loss = 0.69140657\n",
      "Iteration 2713, loss = 0.69140545\n",
      "Iteration 2714, loss = 0.69140451\n",
      "Iteration 2715, loss = 0.69140375\n",
      "Iteration 2716, loss = 0.69140300\n",
      "Iteration 2717, loss = 0.69140227\n",
      "Iteration 2718, loss = 0.69140177\n",
      "Iteration 2719, loss = 0.69140047\n",
      "Iteration 2720, loss = 0.69139999\n",
      "Iteration 2721, loss = 0.69139915\n",
      "Iteration 2722, loss = 0.69139829\n",
      "Iteration 2723, loss = 0.69139793\n",
      "Iteration 2724, loss = 0.69139695\n",
      "Iteration 2725, loss = 0.69139635\n",
      "Iteration 2726, loss = 0.69139508\n",
      "Iteration 2727, loss = 0.69139425\n",
      "Iteration 2728, loss = 0.69139390\n",
      "Iteration 2729, loss = 0.69139282\n",
      "Iteration 2730, loss = 0.69139219\n",
      "Iteration 2731, loss = 0.69139135\n",
      "Iteration 2732, loss = 0.69139077\n",
      "Iteration 2733, loss = 0.69139023\n",
      "Iteration 2734, loss = 0.69139066\n",
      "Iteration 2735, loss = 0.69138795\n",
      "Iteration 2736, loss = 0.69138723\n",
      "Iteration 2737, loss = 0.69138646\n",
      "Iteration 2738, loss = 0.69138561\n",
      "Iteration 2739, loss = 0.69138596\n",
      "Iteration 2740, loss = 0.69138404\n",
      "Iteration 2741, loss = 0.69138351\n",
      "Iteration 2742, loss = 0.69138277\n",
      "Iteration 2743, loss = 0.69138195\n",
      "Iteration 2744, loss = 0.69138150\n",
      "Iteration 2745, loss = 0.69138081\n",
      "Iteration 2746, loss = 0.69137985\n",
      "Iteration 2747, loss = 0.69137922\n",
      "Iteration 2748, loss = 0.69137869\n",
      "Iteration 2749, loss = 0.69137792\n",
      "Iteration 2750, loss = 0.69137698\n",
      "Iteration 2751, loss = 0.69137619\n",
      "Iteration 2752, loss = 0.69137535\n",
      "Iteration 2753, loss = 0.69137472\n",
      "Iteration 2754, loss = 0.69137396\n",
      "Iteration 2755, loss = 0.69137349\n",
      "Iteration 2756, loss = 0.69137307\n",
      "Iteration 2757, loss = 0.69137155\n",
      "Iteration 2758, loss = 0.69137080\n",
      "Iteration 2759, loss = 0.69137016\n",
      "Iteration 2760, loss = 0.69136938\n",
      "Iteration 2761, loss = 0.69136944\n",
      "Iteration 2762, loss = 0.69136853\n",
      "Iteration 2763, loss = 0.69136749\n",
      "Iteration 2764, loss = 0.69136690\n",
      "Iteration 2765, loss = 0.69136604\n",
      "Iteration 2766, loss = 0.69136528\n",
      "Iteration 2767, loss = 0.69136447\n",
      "Iteration 2768, loss = 0.69136460\n",
      "Iteration 2769, loss = 0.69136307\n",
      "Iteration 2770, loss = 0.69136233\n",
      "Iteration 2771, loss = 0.69136145\n",
      "Iteration 2772, loss = 0.69136026\n",
      "Iteration 2773, loss = 0.69135963\n",
      "Iteration 2774, loss = 0.69135913\n",
      "Iteration 2775, loss = 0.69135818\n",
      "Iteration 2776, loss = 0.69135749\n",
      "Iteration 2777, loss = 0.69135651\n",
      "Iteration 2778, loss = 0.69135590\n",
      "Iteration 2779, loss = 0.69135542\n",
      "Iteration 2780, loss = 0.69135435\n",
      "Iteration 2781, loss = 0.69135393\n",
      "Iteration 2782, loss = 0.69135316\n",
      "Iteration 2783, loss = 0.69135264\n",
      "Iteration 2784, loss = 0.69135158\n",
      "Iteration 2785, loss = 0.69135135\n",
      "Iteration 2786, loss = 0.69135063\n",
      "Iteration 2787, loss = 0.69134969\n",
      "Iteration 2788, loss = 0.69134866\n",
      "Iteration 2789, loss = 0.69134841\n",
      "Iteration 2790, loss = 0.69134790\n",
      "Iteration 2791, loss = 0.69134721\n",
      "Iteration 2792, loss = 0.69134593\n",
      "Iteration 2793, loss = 0.69134636\n",
      "Iteration 2794, loss = 0.69134451\n",
      "Iteration 2795, loss = 0.69134394\n",
      "Iteration 2796, loss = 0.69134322\n",
      "Iteration 2797, loss = 0.69134287\n",
      "Iteration 2798, loss = 0.69134166\n",
      "Iteration 2799, loss = 0.69134101\n",
      "Iteration 2800, loss = 0.69134062\n",
      "Iteration 2801, loss = 0.69133944\n",
      "Iteration 2802, loss = 0.69133873\n",
      "Iteration 2803, loss = 0.69133804\n",
      "Iteration 2804, loss = 0.69133765\n",
      "Iteration 2805, loss = 0.69133738\n",
      "Iteration 2806, loss = 0.69133620\n",
      "Iteration 2807, loss = 0.69133521\n",
      "Iteration 2808, loss = 0.69133458\n",
      "Iteration 2809, loss = 0.69133405\n",
      "Iteration 2810, loss = 0.69133362\n",
      "Iteration 2811, loss = 0.69133240\n",
      "Iteration 2812, loss = 0.69133177\n",
      "Iteration 2813, loss = 0.69133125\n",
      "Iteration 2814, loss = 0.69133042\n",
      "Iteration 2815, loss = 0.69132974\n",
      "Iteration 2816, loss = 0.69132901\n",
      "Iteration 2817, loss = 0.69132787\n",
      "Iteration 2818, loss = 0.69132735\n",
      "Iteration 2819, loss = 0.69132709\n",
      "Iteration 2820, loss = 0.69132605\n",
      "Iteration 2821, loss = 0.69132545\n",
      "Iteration 2822, loss = 0.69132498\n",
      "Iteration 2823, loss = 0.69132408\n",
      "Iteration 2824, loss = 0.69132324\n",
      "Iteration 2825, loss = 0.69132335\n",
      "Iteration 2826, loss = 0.69132212\n",
      "Iteration 2827, loss = 0.69132109\n",
      "Iteration 2828, loss = 0.69132090\n",
      "Iteration 2829, loss = 0.69131983\n",
      "Iteration 2830, loss = 0.69131921\n",
      "Iteration 2831, loss = 0.69131847\n",
      "Iteration 2832, loss = 0.69131796\n",
      "Iteration 2833, loss = 0.69131777\n",
      "Iteration 2834, loss = 0.69131663\n",
      "Iteration 2835, loss = 0.69131583\n",
      "Iteration 2836, loss = 0.69131509\n",
      "Iteration 2837, loss = 0.69131425\n",
      "Iteration 2838, loss = 0.69131382\n",
      "Iteration 2839, loss = 0.69131305\n",
      "Iteration 2840, loss = 0.69131334\n",
      "Iteration 2841, loss = 0.69131188\n",
      "Iteration 2842, loss = 0.69131197\n",
      "Iteration 2843, loss = 0.69131083\n",
      "Iteration 2844, loss = 0.69130949\n",
      "Iteration 2845, loss = 0.69130959\n",
      "Iteration 2846, loss = 0.69130885\n",
      "Iteration 2847, loss = 0.69130771\n",
      "Iteration 2848, loss = 0.69130688\n",
      "Iteration 2849, loss = 0.69130685\n",
      "Iteration 2850, loss = 0.69130596\n",
      "Iteration 2851, loss = 0.69130491\n",
      "Iteration 2852, loss = 0.69130442\n",
      "Iteration 2853, loss = 0.69130392\n",
      "Iteration 2854, loss = 0.69130280\n",
      "Iteration 2855, loss = 0.69130184\n",
      "Iteration 2856, loss = 0.69130132\n",
      "Iteration 2857, loss = 0.69130043\n",
      "Iteration 2858, loss = 0.69129970\n",
      "Iteration 2859, loss = 0.69129941\n",
      "Iteration 2860, loss = 0.69129838\n",
      "Iteration 2861, loss = 0.69129771\n",
      "Iteration 2862, loss = 0.69129684\n",
      "Iteration 2863, loss = 0.69129624\n",
      "Iteration 2864, loss = 0.69129535\n",
      "Iteration 2865, loss = 0.69129484\n",
      "Iteration 2866, loss = 0.69129413\n",
      "Iteration 2867, loss = 0.69129340\n",
      "Iteration 2868, loss = 0.69129277\n",
      "Iteration 2869, loss = 0.69129277\n",
      "Iteration 2870, loss = 0.69129130\n",
      "Iteration 2871, loss = 0.69129084\n",
      "Iteration 2872, loss = 0.69129046\n",
      "Iteration 2873, loss = 0.69128953\n",
      "Iteration 2874, loss = 0.69128849\n",
      "Iteration 2875, loss = 0.69128763\n",
      "Iteration 2876, loss = 0.69128715\n",
      "Iteration 2877, loss = 0.69128637\n",
      "Iteration 2878, loss = 0.69128539\n",
      "Iteration 2879, loss = 0.69128443\n",
      "Iteration 2880, loss = 0.69128357\n",
      "Iteration 2881, loss = 0.69128336\n",
      "Iteration 2882, loss = 0.69128260\n",
      "Iteration 2883, loss = 0.69128190\n",
      "Iteration 2884, loss = 0.69128124\n",
      "Iteration 2885, loss = 0.69128022\n",
      "Iteration 2886, loss = 0.69127963\n",
      "Iteration 2887, loss = 0.69127955\n",
      "Iteration 2888, loss = 0.69127836\n",
      "Iteration 2889, loss = 0.69127778\n",
      "Iteration 2890, loss = 0.69127693\n",
      "Iteration 2891, loss = 0.69127643\n",
      "Iteration 2892, loss = 0.69127566\n",
      "Iteration 2893, loss = 0.69127471\n",
      "Iteration 2894, loss = 0.69127382\n",
      "Iteration 2895, loss = 0.69127365\n",
      "Iteration 2896, loss = 0.69127237\n",
      "Iteration 2897, loss = 0.69127200\n",
      "Iteration 2898, loss = 0.69127138\n",
      "Iteration 2899, loss = 0.69127060\n",
      "Iteration 2900, loss = 0.69126990\n",
      "Iteration 2901, loss = 0.69126944\n",
      "Iteration 2902, loss = 0.69126846\n",
      "Iteration 2903, loss = 0.69126784\n",
      "Iteration 2904, loss = 0.69126755\n",
      "Iteration 2905, loss = 0.69126681\n",
      "Iteration 2906, loss = 0.69126612\n",
      "Iteration 2907, loss = 0.69126591\n",
      "Iteration 2908, loss = 0.69126528\n",
      "Iteration 2909, loss = 0.69126397\n",
      "Iteration 2910, loss = 0.69126350\n",
      "Iteration 2911, loss = 0.69126304\n",
      "Iteration 2912, loss = 0.69126226\n",
      "Iteration 2913, loss = 0.69126154\n",
      "Iteration 2914, loss = 0.69126132\n",
      "Iteration 2915, loss = 0.69126087\n",
      "Iteration 2916, loss = 0.69125996\n",
      "Iteration 2917, loss = 0.69125936\n",
      "Iteration 2918, loss = 0.69125840\n",
      "Iteration 2919, loss = 0.69125777\n",
      "Iteration 2920, loss = 0.69125732\n",
      "Iteration 2921, loss = 0.69125616\n",
      "Iteration 2922, loss = 0.69125568\n",
      "Iteration 2923, loss = 0.69125480\n",
      "Iteration 2924, loss = 0.69125446\n",
      "Iteration 2925, loss = 0.69125358\n",
      "Iteration 2926, loss = 0.69125310\n",
      "Iteration 2927, loss = 0.69125231\n",
      "Iteration 2928, loss = 0.69125140\n",
      "Iteration 2929, loss = 0.69125077\n",
      "Iteration 2930, loss = 0.69125011\n",
      "Iteration 2931, loss = 0.69124950\n",
      "Iteration 2932, loss = 0.69124885\n",
      "Iteration 2933, loss = 0.69124816\n",
      "Iteration 2934, loss = 0.69124772\n",
      "Iteration 2935, loss = 0.69124666\n",
      "Iteration 2936, loss = 0.69124622\n",
      "Iteration 2937, loss = 0.69124585\n",
      "Iteration 2938, loss = 0.69124497\n",
      "Iteration 2939, loss = 0.69124382\n",
      "Iteration 2940, loss = 0.69124399\n",
      "Iteration 2941, loss = 0.69124302\n",
      "Iteration 2942, loss = 0.69124278\n",
      "Iteration 2943, loss = 0.69124190\n",
      "Iteration 2944, loss = 0.69124117\n",
      "Iteration 2945, loss = 0.69124042\n",
      "Iteration 2946, loss = 0.69123978\n",
      "Iteration 2947, loss = 0.69123899\n",
      "Iteration 2948, loss = 0.69123819\n",
      "Iteration 2949, loss = 0.69123794\n",
      "Iteration 2950, loss = 0.69123677\n",
      "Iteration 2951, loss = 0.69123684\n",
      "Iteration 2952, loss = 0.69123561\n",
      "Iteration 2953, loss = 0.69123483\n",
      "Iteration 2954, loss = 0.69123467\n",
      "Iteration 2955, loss = 0.69123426\n",
      "Iteration 2956, loss = 0.69123299\n",
      "Iteration 2957, loss = 0.69123259\n",
      "Iteration 2958, loss = 0.69123207\n",
      "Iteration 2959, loss = 0.69123084\n",
      "Iteration 2960, loss = 0.69123038\n",
      "Iteration 2961, loss = 0.69123005\n",
      "Iteration 2962, loss = 0.69122911\n",
      "Iteration 2963, loss = 0.69122870\n",
      "Iteration 2964, loss = 0.69122759\n",
      "Iteration 2965, loss = 0.69122714\n",
      "Iteration 2966, loss = 0.69122644\n",
      "Iteration 2967, loss = 0.69122521\n",
      "Iteration 2968, loss = 0.69122467\n",
      "Iteration 2969, loss = 0.69122418\n",
      "Iteration 2970, loss = 0.69122372\n",
      "Iteration 2971, loss = 0.69122269\n",
      "Iteration 2972, loss = 0.69122229\n",
      "Iteration 2973, loss = 0.69122137\n",
      "Iteration 2974, loss = 0.69122041\n",
      "Iteration 2975, loss = 0.69121971\n",
      "Iteration 2976, loss = 0.69121926\n",
      "Iteration 2977, loss = 0.69121853\n",
      "Iteration 2978, loss = 0.69121777\n",
      "Iteration 2979, loss = 0.69121786\n",
      "Iteration 2980, loss = 0.69121613\n",
      "Iteration 2981, loss = 0.69121563\n",
      "Iteration 2982, loss = 0.69121460\n",
      "Iteration 2983, loss = 0.69121407\n",
      "Iteration 2984, loss = 0.69121353\n",
      "Iteration 2985, loss = 0.69121303\n",
      "Iteration 2986, loss = 0.69121296\n",
      "Iteration 2987, loss = 0.69121128\n",
      "Iteration 2988, loss = 0.69121063\n",
      "Iteration 2989, loss = 0.69121084\n",
      "Iteration 2990, loss = 0.69121096\n",
      "Iteration 2991, loss = 0.69120897\n",
      "Iteration 2992, loss = 0.69120818\n",
      "Iteration 2993, loss = 0.69120777\n",
      "Iteration 2994, loss = 0.69120690\n",
      "Iteration 2995, loss = 0.69120674\n",
      "Iteration 2996, loss = 0.69120568\n",
      "Iteration 2997, loss = 0.69120517\n",
      "Iteration 2998, loss = 0.69120449\n",
      "Iteration 2999, loss = 0.69120376\n",
      "Iteration 3000, loss = 0.69120318\n",
      "Iteration 3001, loss = 0.69120230\n",
      "Iteration 3002, loss = 0.69120179\n",
      "Iteration 3003, loss = 0.69120128\n",
      "Iteration 3004, loss = 0.69120067\n",
      "Iteration 3005, loss = 0.69120011\n",
      "Iteration 3006, loss = 0.69119983\n",
      "Iteration 3007, loss = 0.69119884\n",
      "Iteration 3008, loss = 0.69119855\n",
      "Iteration 3009, loss = 0.69119769\n",
      "Iteration 3010, loss = 0.69119753\n",
      "Iteration 3011, loss = 0.69119648\n",
      "Iteration 3012, loss = 0.69119574\n",
      "Iteration 3013, loss = 0.69119540\n",
      "Iteration 3014, loss = 0.69119446\n",
      "Iteration 3015, loss = 0.69119396\n",
      "Iteration 3016, loss = 0.69119374\n",
      "Iteration 3017, loss = 0.69119265\n",
      "Iteration 3018, loss = 0.69119232\n",
      "Iteration 3019, loss = 0.69119155\n",
      "Iteration 3020, loss = 0.69119090\n",
      "Iteration 3021, loss = 0.69119014\n",
      "Iteration 3022, loss = 0.69118930\n",
      "Iteration 3023, loss = 0.69118903\n",
      "Iteration 3024, loss = 0.69118874\n",
      "Iteration 3025, loss = 0.69118768\n",
      "Iteration 3026, loss = 0.69118722\n",
      "Iteration 3027, loss = 0.69118673\n",
      "Iteration 3028, loss = 0.69118693\n",
      "Iteration 3029, loss = 0.69118535\n",
      "Iteration 3030, loss = 0.69118519\n",
      "Iteration 3031, loss = 0.69118428\n",
      "Iteration 3032, loss = 0.69118359\n",
      "Iteration 3033, loss = 0.69118291\n",
      "Iteration 3034, loss = 0.69118229\n",
      "Iteration 3035, loss = 0.69118172\n",
      "Iteration 3036, loss = 0.69118104\n",
      "Iteration 3037, loss = 0.69118053\n",
      "Iteration 3038, loss = 0.69117976\n",
      "Iteration 3039, loss = 0.69117916\n",
      "Iteration 3040, loss = 0.69117844\n",
      "Iteration 3041, loss = 0.69117796\n",
      "Iteration 3042, loss = 0.69117736\n",
      "Iteration 3043, loss = 0.69117674\n",
      "Iteration 3044, loss = 0.69117610\n",
      "Iteration 3045, loss = 0.69117560\n",
      "Iteration 3046, loss = 0.69117478\n",
      "Iteration 3047, loss = 0.69117448\n",
      "Iteration 3048, loss = 0.69117421\n",
      "Iteration 3049, loss = 0.69117326\n",
      "Iteration 3050, loss = 0.69117207\n",
      "Iteration 3051, loss = 0.69117134\n",
      "Iteration 3052, loss = 0.69117065\n",
      "Iteration 3053, loss = 0.69117007\n",
      "Iteration 3054, loss = 0.69116930\n",
      "Iteration 3055, loss = 0.69116868\n",
      "Iteration 3056, loss = 0.69116818\n",
      "Iteration 3057, loss = 0.69116745\n",
      "Iteration 3058, loss = 0.69116665\n",
      "Iteration 3059, loss = 0.69116622\n",
      "Iteration 3060, loss = 0.69116518\n",
      "Iteration 3061, loss = 0.69116466\n",
      "Iteration 3062, loss = 0.69116412\n",
      "Iteration 3063, loss = 0.69116337\n",
      "Iteration 3064, loss = 0.69116282\n",
      "Iteration 3065, loss = 0.69116298\n",
      "Iteration 3066, loss = 0.69116134\n",
      "Iteration 3067, loss = 0.69116090\n",
      "Iteration 3068, loss = 0.69116038\n",
      "Iteration 3069, loss = 0.69116003\n",
      "Iteration 3070, loss = 0.69115900\n",
      "Iteration 3071, loss = 0.69115815\n",
      "Iteration 3072, loss = 0.69115762\n",
      "Iteration 3073, loss = 0.69115692\n",
      "Iteration 3074, loss = 0.69115615\n",
      "Iteration 3075, loss = 0.69115530\n",
      "Iteration 3076, loss = 0.69115532\n",
      "Iteration 3077, loss = 0.69115434\n",
      "Iteration 3078, loss = 0.69115350\n",
      "Iteration 3079, loss = 0.69115317\n",
      "Iteration 3080, loss = 0.69115245\n",
      "Iteration 3081, loss = 0.69115254\n",
      "Iteration 3082, loss = 0.69115193\n",
      "Iteration 3083, loss = 0.69115051\n",
      "Iteration 3084, loss = 0.69114996\n",
      "Iteration 3085, loss = 0.69114967\n",
      "Iteration 3086, loss = 0.69114868\n",
      "Iteration 3087, loss = 0.69114807\n",
      "Iteration 3088, loss = 0.69114765\n",
      "Iteration 3089, loss = 0.69114715\n",
      "Iteration 3090, loss = 0.69114615\n",
      "Iteration 3091, loss = 0.69114600\n",
      "Iteration 3092, loss = 0.69114506\n",
      "Iteration 3093, loss = 0.69114439\n",
      "Iteration 3094, loss = 0.69114479\n",
      "Iteration 3095, loss = 0.69114293\n",
      "Iteration 3096, loss = 0.69114250\n",
      "Iteration 3097, loss = 0.69114192\n",
      "Iteration 3098, loss = 0.69114122\n",
      "Iteration 3099, loss = 0.69114089\n",
      "Iteration 3100, loss = 0.69114031\n",
      "Iteration 3101, loss = 0.69113923\n",
      "Iteration 3102, loss = 0.69113904\n",
      "Iteration 3103, loss = 0.69113852\n",
      "Iteration 3104, loss = 0.69113749\n",
      "Iteration 3105, loss = 0.69113712\n",
      "Iteration 3106, loss = 0.69113638\n",
      "Iteration 3107, loss = 0.69113581\n",
      "Iteration 3108, loss = 0.69113560\n",
      "Iteration 3109, loss = 0.69113463\n",
      "Iteration 3110, loss = 0.69113453\n",
      "Iteration 3111, loss = 0.69113392\n",
      "Iteration 3112, loss = 0.69113290\n",
      "Iteration 3113, loss = 0.69113195\n",
      "Iteration 3114, loss = 0.69113175\n",
      "Iteration 3115, loss = 0.69113071\n",
      "Iteration 3116, loss = 0.69113114\n",
      "Iteration 3117, loss = 0.69113024\n",
      "Iteration 3118, loss = 0.69112917\n",
      "Iteration 3119, loss = 0.69112878\n",
      "Iteration 3120, loss = 0.69112813\n",
      "Iteration 3121, loss = 0.69112730\n",
      "Iteration 3122, loss = 0.69112668\n",
      "Iteration 3123, loss = 0.69112702\n",
      "Iteration 3124, loss = 0.69112530\n",
      "Iteration 3125, loss = 0.69112450\n",
      "Iteration 3126, loss = 0.69112429\n",
      "Iteration 3127, loss = 0.69112417\n",
      "Iteration 3128, loss = 0.69112356\n",
      "Iteration 3129, loss = 0.69112223\n",
      "Iteration 3130, loss = 0.69112149\n",
      "Iteration 3131, loss = 0.69112110\n",
      "Iteration 3132, loss = 0.69112044\n",
      "Iteration 3133, loss = 0.69111969\n",
      "Iteration 3134, loss = 0.69111894\n",
      "Iteration 3135, loss = 0.69111795\n",
      "Iteration 3136, loss = 0.69111780\n",
      "Iteration 3137, loss = 0.69111672\n",
      "Iteration 3138, loss = 0.69111617\n",
      "Iteration 3139, loss = 0.69111553\n",
      "Iteration 3140, loss = 0.69111506\n",
      "Iteration 3141, loss = 0.69111481\n",
      "Iteration 3142, loss = 0.69111437\n",
      "Iteration 3143, loss = 0.69111301\n",
      "Iteration 3144, loss = 0.69111252\n",
      "Iteration 3145, loss = 0.69111178\n",
      "Iteration 3146, loss = 0.69111104\n",
      "Iteration 3147, loss = 0.69111048\n",
      "Iteration 3148, loss = 0.69110992\n",
      "Iteration 3149, loss = 0.69110924\n",
      "Iteration 3150, loss = 0.69110867\n",
      "Iteration 3151, loss = 0.69110863\n",
      "Iteration 3152, loss = 0.69110756\n",
      "Iteration 3153, loss = 0.69110683\n",
      "Iteration 3154, loss = 0.69110617\n",
      "Iteration 3155, loss = 0.69110563\n",
      "Iteration 3156, loss = 0.69110490\n",
      "Iteration 3157, loss = 0.69110444\n",
      "Iteration 3158, loss = 0.69110350\n",
      "Iteration 3159, loss = 0.69110353\n",
      "Iteration 3160, loss = 0.69110299\n",
      "Iteration 3161, loss = 0.69110232\n",
      "Iteration 3162, loss = 0.69110167\n",
      "Iteration 3163, loss = 0.69110110\n",
      "Iteration 3164, loss = 0.69110023\n",
      "Iteration 3165, loss = 0.69109938\n",
      "Iteration 3166, loss = 0.69109872\n",
      "Iteration 3167, loss = 0.69109885\n",
      "Iteration 3168, loss = 0.69109782\n",
      "Iteration 3169, loss = 0.69109683\n",
      "Iteration 3170, loss = 0.69109650\n",
      "Iteration 3171, loss = 0.69109587\n",
      "Iteration 3172, loss = 0.69109504\n",
      "Iteration 3173, loss = 0.69109454\n",
      "Iteration 3174, loss = 0.69109369\n",
      "Iteration 3175, loss = 0.69109314\n",
      "Iteration 3176, loss = 0.69109244\n",
      "Iteration 3177, loss = 0.69109233\n",
      "Iteration 3178, loss = 0.69109161\n",
      "Iteration 3179, loss = 0.69109125\n",
      "Iteration 3180, loss = 0.69109031\n",
      "Iteration 3181, loss = 0.69108974\n",
      "Iteration 3182, loss = 0.69108902\n",
      "Iteration 3183, loss = 0.69108831\n",
      "Iteration 3184, loss = 0.69108804\n",
      "Iteration 3185, loss = 0.69108715\n",
      "Iteration 3186, loss = 0.69108647\n",
      "Iteration 3187, loss = 0.69108596\n",
      "Iteration 3188, loss = 0.69108519\n",
      "Iteration 3189, loss = 0.69108472\n",
      "Iteration 3190, loss = 0.69108452\n",
      "Iteration 3191, loss = 0.69108363\n",
      "Iteration 3192, loss = 0.69108281\n",
      "Iteration 3193, loss = 0.69108266\n",
      "Iteration 3194, loss = 0.69108156\n",
      "Iteration 3195, loss = 0.69108136\n",
      "Iteration 3196, loss = 0.69108078\n",
      "Iteration 3197, loss = 0.69107972\n",
      "Iteration 3198, loss = 0.69107909\n",
      "Iteration 3199, loss = 0.69107818\n",
      "Iteration 3200, loss = 0.69107757\n",
      "Iteration 3201, loss = 0.69107789\n",
      "Iteration 3202, loss = 0.69107694\n",
      "Iteration 3203, loss = 0.69107612\n",
      "Iteration 3204, loss = 0.69107543\n",
      "Iteration 3205, loss = 0.69107466\n",
      "Iteration 3206, loss = 0.69107425\n",
      "Iteration 3207, loss = 0.69107350\n",
      "Iteration 3208, loss = 0.69107261\n",
      "Iteration 3209, loss = 0.69107224\n",
      "Iteration 3210, loss = 0.69107202\n",
      "Iteration 3211, loss = 0.69107155\n",
      "Iteration 3212, loss = 0.69107030\n",
      "Iteration 3213, loss = 0.69107023\n",
      "Iteration 3214, loss = 0.69106922\n",
      "Iteration 3215, loss = 0.69106863\n",
      "Iteration 3216, loss = 0.69106833\n",
      "Iteration 3217, loss = 0.69106734\n",
      "Iteration 3218, loss = 0.69106646\n",
      "Iteration 3219, loss = 0.69106603\n",
      "Iteration 3220, loss = 0.69106541\n",
      "Iteration 3221, loss = 0.69106488\n",
      "Iteration 3222, loss = 0.69106491\n",
      "Iteration 3223, loss = 0.69106408\n",
      "Iteration 3224, loss = 0.69106286\n",
      "Iteration 3225, loss = 0.69106226\n",
      "Iteration 3226, loss = 0.69106213\n",
      "Iteration 3227, loss = 0.69106118\n",
      "Iteration 3228, loss = 0.69106063\n",
      "Iteration 3229, loss = 0.69105992\n",
      "Iteration 3230, loss = 0.69105935\n",
      "Iteration 3231, loss = 0.69105914\n",
      "Iteration 3232, loss = 0.69105928\n",
      "Iteration 3233, loss = 0.69105759\n",
      "Iteration 3234, loss = 0.69105767\n",
      "Iteration 3235, loss = 0.69105633\n",
      "Iteration 3236, loss = 0.69105613\n",
      "Iteration 3237, loss = 0.69105498\n",
      "Iteration 3238, loss = 0.69105515\n",
      "Iteration 3239, loss = 0.69105393\n",
      "Iteration 3240, loss = 0.69105342\n",
      "Iteration 3241, loss = 0.69105287\n",
      "Iteration 3242, loss = 0.69105232\n",
      "Iteration 3243, loss = 0.69105162\n",
      "Iteration 3244, loss = 0.69105122\n",
      "Iteration 3245, loss = 0.69105055\n",
      "Iteration 3246, loss = 0.69104991\n",
      "Iteration 3247, loss = 0.69104950\n",
      "Iteration 3248, loss = 0.69104898\n",
      "Iteration 3249, loss = 0.69104814\n",
      "Iteration 3250, loss = 0.69104763\n",
      "Iteration 3251, loss = 0.69104686\n",
      "Iteration 3252, loss = 0.69104669\n",
      "Iteration 3253, loss = 0.69104598\n",
      "Iteration 3254, loss = 0.69104544\n",
      "Iteration 3255, loss = 0.69104480\n",
      "Iteration 3256, loss = 0.69104420\n",
      "Iteration 3257, loss = 0.69104351\n",
      "Iteration 3258, loss = 0.69104277\n",
      "Iteration 3259, loss = 0.69104323\n",
      "Iteration 3260, loss = 0.69104178\n",
      "Iteration 3261, loss = 0.69104134\n",
      "Iteration 3262, loss = 0.69104102\n",
      "Iteration 3263, loss = 0.69104066\n",
      "Iteration 3264, loss = 0.69103955\n",
      "Iteration 3265, loss = 0.69103862\n",
      "Iteration 3266, loss = 0.69103853\n",
      "Iteration 3267, loss = 0.69103795\n",
      "Iteration 3268, loss = 0.69103815\n",
      "Iteration 3269, loss = 0.69103719\n",
      "Iteration 3270, loss = 0.69103647\n",
      "Iteration 3271, loss = 0.69103565\n",
      "Iteration 3272, loss = 0.69103546\n",
      "Iteration 3273, loss = 0.69103633\n",
      "Iteration 3274, loss = 0.69103422\n",
      "Iteration 3275, loss = 0.69103382\n",
      "Iteration 3276, loss = 0.69103306\n",
      "Iteration 3277, loss = 0.69103267\n",
      "Iteration 3278, loss = 0.69103173\n",
      "Iteration 3279, loss = 0.69103111\n",
      "Iteration 3280, loss = 0.69103061\n",
      "Iteration 3281, loss = 0.69102988\n",
      "Iteration 3282, loss = 0.69102981\n",
      "Iteration 3283, loss = 0.69102875\n",
      "Iteration 3284, loss = 0.69102824\n",
      "Iteration 3285, loss = 0.69102875\n",
      "Iteration 3286, loss = 0.69102707\n",
      "Iteration 3287, loss = 0.69102683\n",
      "Iteration 3288, loss = 0.69102619\n",
      "Iteration 3289, loss = 0.69102575\n",
      "Iteration 3290, loss = 0.69102475\n",
      "Iteration 3291, loss = 0.69102438\n",
      "Iteration 3292, loss = 0.69102415\n",
      "Iteration 3293, loss = 0.69102323\n",
      "Iteration 3294, loss = 0.69102262\n",
      "Iteration 3295, loss = 0.69102215\n",
      "Iteration 3296, loss = 0.69102129\n",
      "Iteration 3297, loss = 0.69102094\n",
      "Iteration 3298, loss = 0.69102040\n",
      "Iteration 3299, loss = 0.69101995\n",
      "Iteration 3300, loss = 0.69101932\n",
      "Iteration 3301, loss = 0.69101854\n",
      "Iteration 3302, loss = 0.69101824\n",
      "Iteration 3303, loss = 0.69101741\n",
      "Iteration 3304, loss = 0.69101689\n",
      "Iteration 3305, loss = 0.69101628\n",
      "Iteration 3306, loss = 0.69101647\n",
      "Iteration 3307, loss = 0.69101551\n",
      "Iteration 3308, loss = 0.69101455\n",
      "Iteration 3309, loss = 0.69101394\n",
      "Iteration 3310, loss = 0.69101388\n",
      "Iteration 3311, loss = 0.69101297\n",
      "Iteration 3312, loss = 0.69101234\n",
      "Iteration 3313, loss = 0.69101214\n",
      "Iteration 3314, loss = 0.69101154\n",
      "Iteration 3315, loss = 0.69101149\n",
      "Iteration 3316, loss = 0.69101018\n",
      "Iteration 3317, loss = 0.69100965\n",
      "Iteration 3318, loss = 0.69100908\n",
      "Iteration 3319, loss = 0.69100857\n",
      "Iteration 3320, loss = 0.69100807\n",
      "Iteration 3321, loss = 0.69100749\n",
      "Iteration 3322, loss = 0.69100681\n",
      "Iteration 3323, loss = 0.69100642\n",
      "Iteration 3324, loss = 0.69100546\n",
      "Iteration 3325, loss = 0.69100499\n",
      "Iteration 3326, loss = 0.69100467\n",
      "Iteration 3327, loss = 0.69100376\n",
      "Iteration 3328, loss = 0.69100328\n",
      "Iteration 3329, loss = 0.69100275\n",
      "Iteration 3330, loss = 0.69100224\n",
      "Iteration 3331, loss = 0.69100177\n",
      "Iteration 3332, loss = 0.69100128\n",
      "Iteration 3333, loss = 0.69100058\n",
      "Iteration 3334, loss = 0.69099989\n",
      "Iteration 3335, loss = 0.69099969\n",
      "Iteration 3336, loss = 0.69099949\n",
      "Iteration 3337, loss = 0.69099821\n",
      "Iteration 3338, loss = 0.69099788\n",
      "Iteration 3339, loss = 0.69099729\n",
      "Iteration 3340, loss = 0.69099710\n",
      "Iteration 3341, loss = 0.69099608\n",
      "Iteration 3342, loss = 0.69099537\n",
      "Iteration 3343, loss = 0.69099500\n",
      "Iteration 3344, loss = 0.69099399\n",
      "Iteration 3345, loss = 0.69099396\n",
      "Iteration 3346, loss = 0.69099330\n",
      "Iteration 3347, loss = 0.69099269\n",
      "Iteration 3348, loss = 0.69099215\n",
      "Iteration 3349, loss = 0.69099151\n",
      "Iteration 3350, loss = 0.69099108\n",
      "Iteration 3351, loss = 0.69098999\n",
      "Iteration 3352, loss = 0.69099022\n",
      "Iteration 3353, loss = 0.69098902\n",
      "Iteration 3354, loss = 0.69098896\n",
      "Iteration 3355, loss = 0.69098804\n",
      "Iteration 3356, loss = 0.69098756\n",
      "Iteration 3357, loss = 0.69098671\n",
      "Iteration 3358, loss = 0.69098655\n",
      "Iteration 3359, loss = 0.69098550\n",
      "Iteration 3360, loss = 0.69098528\n",
      "Iteration 3361, loss = 0.69098516\n",
      "Iteration 3362, loss = 0.69098416\n",
      "Iteration 3363, loss = 0.69098386\n",
      "Iteration 3364, loss = 0.69098300\n",
      "Iteration 3365, loss = 0.69098259\n",
      "Iteration 3366, loss = 0.69098291\n",
      "Iteration 3367, loss = 0.69098126\n",
      "Iteration 3368, loss = 0.69098090\n",
      "Iteration 3369, loss = 0.69098056\n",
      "Iteration 3370, loss = 0.69097995\n",
      "Iteration 3371, loss = 0.69097914\n",
      "Iteration 3372, loss = 0.69097855\n",
      "Iteration 3373, loss = 0.69097800\n",
      "Iteration 3374, loss = 0.69097758\n",
      "Iteration 3375, loss = 0.69097712\n",
      "Iteration 3376, loss = 0.69097647\n",
      "Iteration 3377, loss = 0.69097587\n",
      "Iteration 3378, loss = 0.69097552\n",
      "Iteration 3379, loss = 0.69097532\n",
      "Iteration 3380, loss = 0.69097461\n",
      "Iteration 3381, loss = 0.69097359\n",
      "Iteration 3382, loss = 0.69097304\n",
      "Iteration 3383, loss = 0.69097231\n",
      "Iteration 3384, loss = 0.69097252\n",
      "Iteration 3385, loss = 0.69097137\n",
      "Iteration 3386, loss = 0.69097093\n",
      "Iteration 3387, loss = 0.69097068\n",
      "Iteration 3388, loss = 0.69097001\n",
      "Iteration 3389, loss = 0.69096922\n",
      "Iteration 3390, loss = 0.69096887\n",
      "Iteration 3391, loss = 0.69096752\n",
      "Iteration 3392, loss = 0.69096750\n",
      "Iteration 3393, loss = 0.69096737\n",
      "Iteration 3394, loss = 0.69096671\n",
      "Iteration 3395, loss = 0.69096581\n",
      "Iteration 3396, loss = 0.69096640\n",
      "Iteration 3397, loss = 0.69096469\n",
      "Iteration 3398, loss = 0.69096444\n",
      "Iteration 3399, loss = 0.69096359\n",
      "Iteration 3400, loss = 0.69096314\n",
      "Iteration 3401, loss = 0.69096241\n",
      "Iteration 3402, loss = 0.69096222\n",
      "Iteration 3403, loss = 0.69096081\n",
      "Iteration 3404, loss = 0.69096058\n",
      "Iteration 3405, loss = 0.69096004\n",
      "Iteration 3406, loss = 0.69096015\n",
      "Iteration 3407, loss = 0.69095896\n",
      "Iteration 3408, loss = 0.69095810\n",
      "Iteration 3409, loss = 0.69095780\n",
      "Iteration 3410, loss = 0.69095729\n",
      "Iteration 3411, loss = 0.69095655\n",
      "Iteration 3412, loss = 0.69095594\n",
      "Iteration 3413, loss = 0.69095521\n",
      "Iteration 3414, loss = 0.69095474\n",
      "Iteration 3415, loss = 0.69095411\n",
      "Iteration 3416, loss = 0.69095365\n",
      "Iteration 3417, loss = 0.69095326\n",
      "Iteration 3418, loss = 0.69095246\n",
      "Iteration 3419, loss = 0.69095177\n",
      "Iteration 3420, loss = 0.69095154\n",
      "Iteration 3421, loss = 0.69095055\n",
      "Iteration 3422, loss = 0.69094983\n",
      "Iteration 3423, loss = 0.69094927\n",
      "Iteration 3424, loss = 0.69094877\n",
      "Iteration 3425, loss = 0.69094795\n",
      "Iteration 3426, loss = 0.69094850\n",
      "Iteration 3427, loss = 0.69094691\n",
      "Iteration 3428, loss = 0.69094628\n",
      "Iteration 3429, loss = 0.69094563\n",
      "Iteration 3430, loss = 0.69094503\n",
      "Iteration 3431, loss = 0.69094491\n",
      "Iteration 3432, loss = 0.69094391\n",
      "Iteration 3433, loss = 0.69094331\n",
      "Iteration 3434, loss = 0.69094346\n",
      "Iteration 3435, loss = 0.69094236\n",
      "Iteration 3436, loss = 0.69094154\n",
      "Iteration 3437, loss = 0.69094146\n",
      "Iteration 3438, loss = 0.69094050\n",
      "Iteration 3439, loss = 0.69094015\n",
      "Iteration 3440, loss = 0.69094036\n",
      "Iteration 3441, loss = 0.69093861\n",
      "Iteration 3442, loss = 0.69093882\n",
      "Iteration 3443, loss = 0.69093751\n",
      "Iteration 3444, loss = 0.69093713\n",
      "Iteration 3445, loss = 0.69093658\n",
      "Iteration 3446, loss = 0.69093598\n",
      "Iteration 3447, loss = 0.69093577\n",
      "Iteration 3448, loss = 0.69093526\n",
      "Iteration 3449, loss = 0.69093434\n",
      "Iteration 3450, loss = 0.69093403\n",
      "Iteration 3451, loss = 0.69093328\n",
      "Iteration 3452, loss = 0.69093265\n",
      "Iteration 3453, loss = 0.69093272\n",
      "Iteration 3454, loss = 0.69093243\n",
      "Iteration 3455, loss = 0.69093153\n",
      "Iteration 3456, loss = 0.69093090\n",
      "Iteration 3457, loss = 0.69092951\n",
      "Iteration 3458, loss = 0.69092934\n",
      "Iteration 3459, loss = 0.69092896\n",
      "Iteration 3460, loss = 0.69092844\n",
      "Iteration 3461, loss = 0.69092719\n",
      "Iteration 3462, loss = 0.69092696\n",
      "Iteration 3463, loss = 0.69092646\n",
      "Iteration 3464, loss = 0.69092635\n",
      "Iteration 3465, loss = 0.69092552\n",
      "Iteration 3466, loss = 0.69092468\n",
      "Iteration 3467, loss = 0.69092448\n",
      "Iteration 3468, loss = 0.69092344\n",
      "Iteration 3469, loss = 0.69092330\n",
      "Iteration 3470, loss = 0.69092237\n",
      "Iteration 3471, loss = 0.69092228\n",
      "Iteration 3472, loss = 0.69092143\n",
      "Iteration 3473, loss = 0.69092097\n",
      "Iteration 3474, loss = 0.69092065\n",
      "Iteration 3475, loss = 0.69091982\n",
      "Iteration 3476, loss = 0.69091938\n",
      "Iteration 3477, loss = 0.69091869\n",
      "Iteration 3478, loss = 0.69091866\n",
      "Iteration 3479, loss = 0.69091802\n",
      "Iteration 3480, loss = 0.69091763\n",
      "Iteration 3481, loss = 0.69091658\n",
      "Iteration 3482, loss = 0.69091629\n",
      "Iteration 3483, loss = 0.69091575\n",
      "Iteration 3484, loss = 0.69091467\n",
      "Iteration 3485, loss = 0.69091463\n",
      "Iteration 3486, loss = 0.69091362\n",
      "Iteration 3487, loss = 0.69091381\n",
      "Iteration 3488, loss = 0.69091288\n",
      "Iteration 3489, loss = 0.69091267\n",
      "Iteration 3490, loss = 0.69091204\n",
      "Iteration 3491, loss = 0.69091083\n",
      "Iteration 3492, loss = 0.69091047\n",
      "Iteration 3493, loss = 0.69090993\n",
      "Iteration 3494, loss = 0.69090946\n",
      "Iteration 3495, loss = 0.69090865\n",
      "Iteration 3496, loss = 0.69090814\n",
      "Iteration 3497, loss = 0.69090800\n",
      "Iteration 3498, loss = 0.69090700\n",
      "Iteration 3499, loss = 0.69090664\n",
      "Iteration 3500, loss = 0.69090575\n",
      "Iteration 3501, loss = 0.69090524\n",
      "Iteration 3502, loss = 0.69090508\n",
      "Iteration 3503, loss = 0.69090532\n",
      "Iteration 3504, loss = 0.69090379\n",
      "Iteration 3505, loss = 0.69090292\n",
      "Iteration 3506, loss = 0.69090274\n",
      "Iteration 3507, loss = 0.69090242\n",
      "Iteration 3508, loss = 0.69090124\n",
      "Iteration 3509, loss = 0.69090099\n",
      "Iteration 3510, loss = 0.69090063\n",
      "Iteration 3511, loss = 0.69089986\n",
      "Iteration 3512, loss = 0.69089955\n",
      "Iteration 3513, loss = 0.69089897\n",
      "Iteration 3514, loss = 0.69089823\n",
      "Iteration 3515, loss = 0.69089758\n",
      "Iteration 3516, loss = 0.69089700\n",
      "Iteration 3517, loss = 0.69089646\n",
      "Iteration 3518, loss = 0.69089704\n",
      "Iteration 3519, loss = 0.69089601\n",
      "Iteration 3520, loss = 0.69089537\n",
      "Iteration 3521, loss = 0.69089475\n",
      "Iteration 3522, loss = 0.69089411\n",
      "Iteration 3523, loss = 0.69089365\n",
      "Iteration 3524, loss = 0.69089286\n",
      "Iteration 3525, loss = 0.69089360\n",
      "Iteration 3526, loss = 0.69089169\n",
      "Iteration 3527, loss = 0.69089153\n",
      "Iteration 3528, loss = 0.69089081\n",
      "Iteration 3529, loss = 0.69089016\n",
      "Iteration 3530, loss = 0.69088964\n",
      "Iteration 3531, loss = 0.69088934\n",
      "Iteration 3532, loss = 0.69088897\n",
      "Iteration 3533, loss = 0.69088838\n",
      "Iteration 3534, loss = 0.69088808\n",
      "Iteration 3535, loss = 0.69088777\n",
      "Iteration 3536, loss = 0.69088639\n",
      "Iteration 3537, loss = 0.69088586\n",
      "Iteration 3538, loss = 0.69088573\n",
      "Iteration 3539, loss = 0.69088486\n",
      "Iteration 3540, loss = 0.69088473\n",
      "Iteration 3541, loss = 0.69088424\n",
      "Iteration 3542, loss = 0.69088349\n",
      "Iteration 3543, loss = 0.69088285\n",
      "Iteration 3544, loss = 0.69088196\n",
      "Iteration 3545, loss = 0.69088162\n",
      "Iteration 3546, loss = 0.69088144\n",
      "Iteration 3547, loss = 0.69088071\n",
      "Iteration 3548, loss = 0.69088018\n",
      "Iteration 3549, loss = 0.69087989\n",
      "Iteration 3550, loss = 0.69087948\n",
      "Iteration 3551, loss = 0.69087908\n",
      "Iteration 3552, loss = 0.69087786\n",
      "Iteration 3553, loss = 0.69087732\n",
      "Iteration 3554, loss = 0.69087673\n",
      "Iteration 3555, loss = 0.69087625\n",
      "Iteration 3556, loss = 0.69087578\n",
      "Iteration 3557, loss = 0.69087546\n",
      "Iteration 3558, loss = 0.69087533\n",
      "Iteration 3559, loss = 0.69087429\n",
      "Iteration 3560, loss = 0.69087377\n",
      "Iteration 3561, loss = 0.69087301\n",
      "Iteration 3562, loss = 0.69087249\n",
      "Iteration 3563, loss = 0.69087193\n",
      "Iteration 3564, loss = 0.69087126\n",
      "Iteration 3565, loss = 0.69087094\n",
      "Iteration 3566, loss = 0.69087097\n",
      "Iteration 3567, loss = 0.69087001\n",
      "Iteration 3568, loss = 0.69086992\n",
      "Iteration 3569, loss = 0.69086888\n",
      "Iteration 3570, loss = 0.69086847\n",
      "Iteration 3571, loss = 0.69086810\n",
      "Iteration 3572, loss = 0.69086757\n",
      "Iteration 3573, loss = 0.69086732\n",
      "Iteration 3574, loss = 0.69086646\n",
      "Iteration 3575, loss = 0.69086581\n",
      "Iteration 3576, loss = 0.69086512\n",
      "Iteration 3577, loss = 0.69086497\n",
      "Iteration 3578, loss = 0.69086368\n",
      "Iteration 3579, loss = 0.69086316\n",
      "Iteration 3580, loss = 0.69086281\n",
      "Iteration 3581, loss = 0.69086215\n",
      "Iteration 3582, loss = 0.69086277\n",
      "Iteration 3583, loss = 0.69086144\n",
      "Iteration 3584, loss = 0.69086040\n",
      "Iteration 3585, loss = 0.69085988\n",
      "Iteration 3586, loss = 0.69085945\n",
      "Iteration 3587, loss = 0.69085867\n",
      "Iteration 3588, loss = 0.69085820\n",
      "Iteration 3589, loss = 0.69085786\n",
      "Iteration 3590, loss = 0.69085736\n",
      "Iteration 3591, loss = 0.69085700\n",
      "Iteration 3592, loss = 0.69085629\n",
      "Iteration 3593, loss = 0.69085554\n",
      "Iteration 3594, loss = 0.69085544\n",
      "Iteration 3595, loss = 0.69085452\n",
      "Iteration 3596, loss = 0.69085394\n",
      "Iteration 3597, loss = 0.69085344\n",
      "Iteration 3598, loss = 0.69085302\n",
      "Iteration 3599, loss = 0.69085248\n",
      "Iteration 3600, loss = 0.69085191\n",
      "Iteration 3601, loss = 0.69085159\n",
      "Iteration 3602, loss = 0.69085091\n",
      "Iteration 3603, loss = 0.69085039\n",
      "Iteration 3604, loss = 0.69084998\n",
      "Iteration 3605, loss = 0.69084944\n",
      "Iteration 3606, loss = 0.69084892\n",
      "Iteration 3607, loss = 0.69084819\n",
      "Iteration 3608, loss = 0.69084796\n",
      "Iteration 3609, loss = 0.69084711\n",
      "Iteration 3610, loss = 0.69084669\n",
      "Iteration 3611, loss = 0.69084639\n",
      "Iteration 3612, loss = 0.69084576\n",
      "Iteration 3613, loss = 0.69084499\n",
      "Iteration 3614, loss = 0.69084439\n",
      "Iteration 3615, loss = 0.69084414\n",
      "Iteration 3616, loss = 0.69084450\n",
      "Iteration 3617, loss = 0.69084340\n",
      "Iteration 3618, loss = 0.69084277\n",
      "Iteration 3619, loss = 0.69084188\n",
      "Iteration 3620, loss = 0.69084158\n",
      "Iteration 3621, loss = 0.69084078\n",
      "Iteration 3622, loss = 0.69084021\n",
      "Iteration 3623, loss = 0.69084022\n",
      "Iteration 3624, loss = 0.69083937\n",
      "Iteration 3625, loss = 0.69083879\n",
      "Iteration 3626, loss = 0.69083775\n",
      "Iteration 3627, loss = 0.69083771\n",
      "Iteration 3628, loss = 0.69083726\n",
      "Iteration 3629, loss = 0.69083678\n",
      "Iteration 3630, loss = 0.69083584\n",
      "Iteration 3631, loss = 0.69083567\n",
      "Iteration 3632, loss = 0.69083515\n",
      "Iteration 3633, loss = 0.69083444\n",
      "Iteration 3634, loss = 0.69083413\n",
      "Iteration 3635, loss = 0.69083355\n",
      "Iteration 3636, loss = 0.69083303\n",
      "Iteration 3637, loss = 0.69083219\n",
      "Iteration 3638, loss = 0.69083255\n",
      "Iteration 3639, loss = 0.69083192\n",
      "Iteration 3640, loss = 0.69083105\n",
      "Iteration 3641, loss = 0.69083014\n",
      "Iteration 3642, loss = 0.69082998\n",
      "Iteration 3643, loss = 0.69082941\n",
      "Iteration 3644, loss = 0.69082872\n",
      "Iteration 3645, loss = 0.69082806\n",
      "Iteration 3646, loss = 0.69082772\n",
      "Iteration 3647, loss = 0.69082727\n",
      "Iteration 3648, loss = 0.69082681\n",
      "Iteration 3649, loss = 0.69082635\n",
      "Iteration 3650, loss = 0.69082528\n",
      "Iteration 3651, loss = 0.69082495\n",
      "Iteration 3652, loss = 0.69082485\n",
      "Iteration 3653, loss = 0.69082454\n",
      "Iteration 3654, loss = 0.69082373\n",
      "Iteration 3655, loss = 0.69082301\n",
      "Iteration 3656, loss = 0.69082209\n",
      "Iteration 3657, loss = 0.69082168\n",
      "Iteration 3658, loss = 0.69082158\n",
      "Iteration 3659, loss = 0.69082111\n",
      "Iteration 3660, loss = 0.69082058\n",
      "Iteration 3661, loss = 0.69082009\n",
      "Iteration 3662, loss = 0.69081908\n",
      "Iteration 3663, loss = 0.69081839\n",
      "Iteration 3664, loss = 0.69081848\n",
      "Iteration 3665, loss = 0.69081879\n",
      "Iteration 3666, loss = 0.69081699\n",
      "Iteration 3667, loss = 0.69081686\n",
      "Iteration 3668, loss = 0.69081690\n",
      "Iteration 3669, loss = 0.69081571\n",
      "Iteration 3670, loss = 0.69081515\n",
      "Iteration 3671, loss = 0.69081477\n",
      "Iteration 3672, loss = 0.69081414\n",
      "Iteration 3673, loss = 0.69081393\n",
      "Iteration 3674, loss = 0.69081349\n",
      "Iteration 3675, loss = 0.69081291\n",
      "Iteration 3676, loss = 0.69081294\n",
      "Iteration 3677, loss = 0.69081206\n",
      "Iteration 3678, loss = 0.69081161\n",
      "Iteration 3679, loss = 0.69081138\n",
      "Iteration 3680, loss = 0.69081070\n",
      "Iteration 3681, loss = 0.69081016\n",
      "Iteration 3682, loss = 0.69080960\n",
      "Iteration 3683, loss = 0.69080937\n",
      "Iteration 3684, loss = 0.69080855\n",
      "Iteration 3685, loss = 0.69080776\n",
      "Iteration 3686, loss = 0.69080773\n",
      "Iteration 3687, loss = 0.69080689\n",
      "Iteration 3688, loss = 0.69080628\n",
      "Iteration 3689, loss = 0.69080598\n",
      "Iteration 3690, loss = 0.69080524\n",
      "Iteration 3691, loss = 0.69080502\n",
      "Iteration 3692, loss = 0.69080476\n",
      "Iteration 3693, loss = 0.69080370\n",
      "Iteration 3694, loss = 0.69080421\n",
      "Iteration 3695, loss = 0.69080321\n",
      "Iteration 3696, loss = 0.69080266\n",
      "Iteration 3697, loss = 0.69080227\n",
      "Iteration 3698, loss = 0.69080174\n",
      "Iteration 3699, loss = 0.69080121\n",
      "Iteration 3700, loss = 0.69080059\n",
      "Iteration 3701, loss = 0.69080011\n",
      "Iteration 3702, loss = 0.69079963\n",
      "Iteration 3703, loss = 0.69079938\n",
      "Iteration 3704, loss = 0.69079919\n",
      "Iteration 3705, loss = 0.69079823\n",
      "Iteration 3706, loss = 0.69079769\n",
      "Iteration 3707, loss = 0.69079712\n",
      "Iteration 3708, loss = 0.69079683\n",
      "Iteration 3709, loss = 0.69079634\n",
      "Iteration 3710, loss = 0.69079590\n",
      "Iteration 3711, loss = 0.69079526\n",
      "Iteration 3712, loss = 0.69079481\n",
      "Iteration 3713, loss = 0.69079430\n",
      "Iteration 3714, loss = 0.69079420\n",
      "Iteration 3715, loss = 0.69079373\n",
      "Iteration 3716, loss = 0.69079315\n",
      "Iteration 3717, loss = 0.69079222\n",
      "Iteration 3718, loss = 0.69079194\n",
      "Iteration 3719, loss = 0.69079140\n",
      "Iteration 3720, loss = 0.69079153\n",
      "Iteration 3721, loss = 0.69079044\n",
      "Iteration 3722, loss = 0.69079010\n",
      "Iteration 3723, loss = 0.69078919\n",
      "Iteration 3724, loss = 0.69078903\n",
      "Iteration 3725, loss = 0.69078848\n",
      "Iteration 3726, loss = 0.69078770\n",
      "Iteration 3727, loss = 0.69078721\n",
      "Iteration 3728, loss = 0.69078649\n",
      "Iteration 3729, loss = 0.69078660\n",
      "Iteration 3730, loss = 0.69078568\n",
      "Iteration 3731, loss = 0.69078573\n",
      "Iteration 3732, loss = 0.69078480\n",
      "Iteration 3733, loss = 0.69078442\n",
      "Iteration 3734, loss = 0.69078384\n",
      "Iteration 3735, loss = 0.69078380\n",
      "Iteration 3736, loss = 0.69078297\n",
      "Iteration 3737, loss = 0.69078292\n",
      "Iteration 3738, loss = 0.69078233\n",
      "Iteration 3739, loss = 0.69078168\n",
      "Iteration 3740, loss = 0.69078123\n",
      "Iteration 3741, loss = 0.69078054\n",
      "Iteration 3742, loss = 0.69078008\n",
      "Iteration 3743, loss = 0.69078006\n",
      "Iteration 3744, loss = 0.69077933\n",
      "Iteration 3745, loss = 0.69077892\n",
      "Iteration 3746, loss = 0.69077844\n",
      "Iteration 3747, loss = 0.69077762\n",
      "Iteration 3748, loss = 0.69077727\n",
      "Iteration 3749, loss = 0.69077689\n",
      "Iteration 3750, loss = 0.69077638\n",
      "Iteration 3751, loss = 0.69077591\n",
      "Iteration 3752, loss = 0.69077536\n",
      "Iteration 3753, loss = 0.69077466\n",
      "Iteration 3754, loss = 0.69077502\n",
      "Iteration 3755, loss = 0.69077402\n",
      "Iteration 3756, loss = 0.69077354\n",
      "Iteration 3757, loss = 0.69077330\n",
      "Iteration 3758, loss = 0.69077272\n",
      "Iteration 3759, loss = 0.69077183\n",
      "Iteration 3760, loss = 0.69077167\n",
      "Iteration 3761, loss = 0.69077178\n",
      "Iteration 3762, loss = 0.69077125\n",
      "Iteration 3763, loss = 0.69077023\n",
      "Iteration 3764, loss = 0.69076972\n",
      "Iteration 3765, loss = 0.69076939\n",
      "Iteration 3766, loss = 0.69076883\n",
      "Iteration 3767, loss = 0.69076848\n",
      "Iteration 3768, loss = 0.69076789\n",
      "Iteration 3769, loss = 0.69076814\n",
      "Iteration 3770, loss = 0.69076688\n",
      "Iteration 3771, loss = 0.69076661\n",
      "Iteration 3772, loss = 0.69076587\n",
      "Iteration 3773, loss = 0.69076562\n",
      "Iteration 3774, loss = 0.69076464\n",
      "Iteration 3775, loss = 0.69076411\n",
      "Iteration 3776, loss = 0.69076386\n",
      "Iteration 3777, loss = 0.69076357\n",
      "Iteration 3778, loss = 0.69076288\n",
      "Iteration 3779, loss = 0.69076276\n",
      "Iteration 3780, loss = 0.69076167\n",
      "Iteration 3781, loss = 0.69076126\n",
      "Iteration 3782, loss = 0.69076078\n",
      "Iteration 3783, loss = 0.69076030\n",
      "Iteration 3784, loss = 0.69075979\n",
      "Iteration 3785, loss = 0.69075933\n",
      "Iteration 3786, loss = 0.69075872\n",
      "Iteration 3787, loss = 0.69075841\n",
      "Iteration 3788, loss = 0.69075846\n",
      "Iteration 3789, loss = 0.69075745\n",
      "Iteration 3790, loss = 0.69075686\n",
      "Iteration 3791, loss = 0.69075609\n",
      "Iteration 3792, loss = 0.69075556\n",
      "Iteration 3793, loss = 0.69075511\n",
      "Iteration 3794, loss = 0.69075467\n",
      "Iteration 3795, loss = 0.69075457\n",
      "Iteration 3796, loss = 0.69075387\n",
      "Iteration 3797, loss = 0.69075354\n",
      "Iteration 3798, loss = 0.69075307\n",
      "Iteration 3799, loss = 0.69075245\n",
      "Iteration 3800, loss = 0.69075196\n",
      "Iteration 3801, loss = 0.69075226\n",
      "Iteration 3802, loss = 0.69075116\n",
      "Iteration 3803, loss = 0.69075048\n",
      "Iteration 3804, loss = 0.69074992\n",
      "Iteration 3805, loss = 0.69074960\n",
      "Iteration 3806, loss = 0.69074883\n",
      "Iteration 3807, loss = 0.69074855\n",
      "Iteration 3808, loss = 0.69074792\n",
      "Iteration 3809, loss = 0.69074734\n",
      "Iteration 3810, loss = 0.69074754\n",
      "Iteration 3811, loss = 0.69074654\n",
      "Iteration 3812, loss = 0.69074619\n",
      "Iteration 3813, loss = 0.69074541\n",
      "Iteration 3814, loss = 0.69074518\n",
      "Iteration 3815, loss = 0.69074428\n",
      "Iteration 3816, loss = 0.69074389\n",
      "Iteration 3817, loss = 0.69074356\n",
      "Iteration 3818, loss = 0.69074383\n",
      "Iteration 3819, loss = 0.69074248\n",
      "Iteration 3820, loss = 0.69074189\n",
      "Iteration 3821, loss = 0.69074198\n",
      "Iteration 3822, loss = 0.69074097\n",
      "Iteration 3823, loss = 0.69074053\n",
      "Iteration 3824, loss = 0.69074030\n",
      "Iteration 3825, loss = 0.69073960\n",
      "Iteration 3826, loss = 0.69073877\n",
      "Iteration 3827, loss = 0.69073854\n",
      "Iteration 3828, loss = 0.69073827\n",
      "Iteration 3829, loss = 0.69073781\n",
      "Iteration 3830, loss = 0.69073710\n",
      "Iteration 3831, loss = 0.69073626\n",
      "Iteration 3832, loss = 0.69073589\n",
      "Iteration 3833, loss = 0.69073591\n",
      "Iteration 3834, loss = 0.69073518\n",
      "Iteration 3835, loss = 0.69073442\n",
      "Iteration 3836, loss = 0.69073413\n",
      "Iteration 3837, loss = 0.69073354\n",
      "Iteration 3838, loss = 0.69073291\n",
      "Iteration 3839, loss = 0.69073254\n",
      "Iteration 3840, loss = 0.69073191\n",
      "Iteration 3841, loss = 0.69073159\n",
      "Iteration 3842, loss = 0.69073146\n",
      "Iteration 3843, loss = 0.69073045\n",
      "Iteration 3844, loss = 0.69072981\n",
      "Iteration 3845, loss = 0.69072926\n",
      "Iteration 3846, loss = 0.69072882\n",
      "Iteration 3847, loss = 0.69072815\n",
      "Iteration 3848, loss = 0.69072782\n",
      "Iteration 3849, loss = 0.69072697\n",
      "Iteration 3850, loss = 0.69072716\n",
      "Iteration 3851, loss = 0.69072660\n",
      "Iteration 3852, loss = 0.69072573\n",
      "Iteration 3853, loss = 0.69072512\n",
      "Iteration 3854, loss = 0.69072490\n",
      "Iteration 3855, loss = 0.69072428\n",
      "Iteration 3856, loss = 0.69072391\n",
      "Iteration 3857, loss = 0.69072339\n",
      "Iteration 3858, loss = 0.69072300\n",
      "Iteration 3859, loss = 0.69072232\n",
      "Iteration 3860, loss = 0.69072166\n",
      "Iteration 3861, loss = 0.69072163\n",
      "Iteration 3862, loss = 0.69072100\n",
      "Iteration 3863, loss = 0.69072040\n",
      "Iteration 3864, loss = 0.69072017\n",
      "Iteration 3865, loss = 0.69071904\n",
      "Iteration 3866, loss = 0.69071876\n",
      "Iteration 3867, loss = 0.69071844\n",
      "Iteration 3868, loss = 0.69071835\n",
      "Iteration 3869, loss = 0.69071736\n",
      "Iteration 3870, loss = 0.69071686\n",
      "Iteration 3871, loss = 0.69071655\n",
      "Iteration 3872, loss = 0.69071583\n",
      "Iteration 3873, loss = 0.69071581\n",
      "Iteration 3874, loss = 0.69071488\n",
      "Iteration 3875, loss = 0.69071510\n",
      "Iteration 3876, loss = 0.69071409\n",
      "Iteration 3877, loss = 0.69071389\n",
      "Iteration 3878, loss = 0.69071341\n",
      "Iteration 3879, loss = 0.69071279\n",
      "Iteration 3880, loss = 0.69071243\n",
      "Iteration 3881, loss = 0.69071198\n",
      "Iteration 3882, loss = 0.69071162\n",
      "Iteration 3883, loss = 0.69071114\n",
      "Iteration 3884, loss = 0.69071060\n",
      "Iteration 3885, loss = 0.69071014\n",
      "Iteration 3886, loss = 0.69070955\n",
      "Iteration 3887, loss = 0.69070892\n",
      "Iteration 3888, loss = 0.69070851\n",
      "Iteration 3889, loss = 0.69070796\n",
      "Iteration 3890, loss = 0.69070769\n",
      "Iteration 3891, loss = 0.69070720\n",
      "Iteration 3892, loss = 0.69070689\n",
      "Iteration 3893, loss = 0.69070617\n",
      "Iteration 3894, loss = 0.69070573\n",
      "Iteration 3895, loss = 0.69070554\n",
      "Iteration 3896, loss = 0.69070468\n",
      "Iteration 3897, loss = 0.69070454\n",
      "Iteration 3898, loss = 0.69070407\n",
      "Iteration 3899, loss = 0.69070346\n",
      "Iteration 3900, loss = 0.69070316\n",
      "Iteration 3901, loss = 0.69070297\n",
      "Iteration 3902, loss = 0.69070239\n",
      "Iteration 3903, loss = 0.69070194\n",
      "Iteration 3904, loss = 0.69070139\n",
      "Iteration 3905, loss = 0.69070082\n",
      "Iteration 3906, loss = 0.69070139\n",
      "Iteration 3907, loss = 0.69070023\n",
      "Iteration 3908, loss = 0.69069942\n",
      "Iteration 3909, loss = 0.69069881\n",
      "Iteration 3910, loss = 0.69069858\n",
      "Iteration 3911, loss = 0.69069826\n",
      "Iteration 3912, loss = 0.69069777\n",
      "Iteration 3913, loss = 0.69069717\n",
      "Iteration 3914, loss = 0.69069662\n",
      "Iteration 3915, loss = 0.69069617\n",
      "Iteration 3916, loss = 0.69069614\n",
      "Iteration 3917, loss = 0.69069636\n",
      "Iteration 3918, loss = 0.69069488\n",
      "Iteration 3919, loss = 0.69069406\n",
      "Iteration 3920, loss = 0.69069400\n",
      "Iteration 3921, loss = 0.69069341\n",
      "Iteration 3922, loss = 0.69069279\n",
      "Iteration 3923, loss = 0.69069265\n",
      "Iteration 3924, loss = 0.69069225\n",
      "Iteration 3925, loss = 0.69069153\n",
      "Iteration 3926, loss = 0.69069082\n",
      "Iteration 3927, loss = 0.69069123\n",
      "Iteration 3928, loss = 0.69068967\n",
      "Iteration 3929, loss = 0.69068975\n",
      "Iteration 3930, loss = 0.69068995\n",
      "Iteration 3931, loss = 0.69068893\n",
      "Iteration 3932, loss = 0.69068877\n",
      "Iteration 3933, loss = 0.69068749\n",
      "Iteration 3934, loss = 0.69068742\n",
      "Iteration 3935, loss = 0.69068670\n",
      "Iteration 3936, loss = 0.69068613\n",
      "Iteration 3937, loss = 0.69068621\n",
      "Iteration 3938, loss = 0.69068513\n",
      "Iteration 3939, loss = 0.69068491\n",
      "Iteration 3940, loss = 0.69068422\n",
      "Iteration 3941, loss = 0.69068325\n",
      "Iteration 3942, loss = 0.69068373\n",
      "Iteration 3943, loss = 0.69068267\n",
      "Iteration 3944, loss = 0.69068248\n",
      "Iteration 3945, loss = 0.69068244\n",
      "Iteration 3946, loss = 0.69068175\n",
      "Iteration 3947, loss = 0.69068099\n",
      "Iteration 3948, loss = 0.69068041\n",
      "Iteration 3949, loss = 0.69068001\n",
      "Iteration 3950, loss = 0.69067946\n",
      "Iteration 3951, loss = 0.69067891\n",
      "Iteration 3952, loss = 0.69067836\n",
      "Iteration 3953, loss = 0.69067819\n",
      "Iteration 3954, loss = 0.69067756\n",
      "Iteration 3955, loss = 0.69067768\n",
      "Iteration 3956, loss = 0.69067684\n",
      "Iteration 3957, loss = 0.69067704\n",
      "Iteration 3958, loss = 0.69067645\n",
      "Iteration 3959, loss = 0.69067604\n",
      "Iteration 3960, loss = 0.69067463\n",
      "Iteration 3961, loss = 0.69067467\n",
      "Iteration 3962, loss = 0.69067526\n",
      "Iteration 3963, loss = 0.69067364\n",
      "Iteration 3964, loss = 0.69067319\n",
      "Iteration 3965, loss = 0.69067280\n",
      "Iteration 3966, loss = 0.69067205\n",
      "Iteration 3967, loss = 0.69067172\n",
      "Iteration 3968, loss = 0.69067126\n",
      "Iteration 3969, loss = 0.69067068\n",
      "Iteration 3970, loss = 0.69067006\n",
      "Iteration 3971, loss = 0.69067025\n",
      "Iteration 3972, loss = 0.69066951\n",
      "Iteration 3973, loss = 0.69066865\n",
      "Iteration 3974, loss = 0.69066868\n",
      "Iteration 3975, loss = 0.69066793\n",
      "Iteration 3976, loss = 0.69066773\n",
      "Iteration 3977, loss = 0.69066733\n",
      "Iteration 3978, loss = 0.69066699\n",
      "Iteration 3979, loss = 0.69066649\n",
      "Iteration 3980, loss = 0.69066650\n",
      "Iteration 3981, loss = 0.69066598\n",
      "Iteration 3982, loss = 0.69066507\n",
      "Iteration 3983, loss = 0.69066471\n",
      "Iteration 3984, loss = 0.69066437\n",
      "Iteration 3985, loss = 0.69066407\n",
      "Iteration 3986, loss = 0.69066341\n",
      "Iteration 3987, loss = 0.69066256\n",
      "Iteration 3988, loss = 0.69066235\n",
      "Iteration 3989, loss = 0.69066207\n",
      "Iteration 3990, loss = 0.69066163\n",
      "Iteration 3991, loss = 0.69066110\n",
      "Iteration 3992, loss = 0.69066025\n",
      "Iteration 3993, loss = 0.69066018\n",
      "Iteration 3994, loss = 0.69065964\n",
      "Iteration 3995, loss = 0.69065983\n",
      "Iteration 3996, loss = 0.69065868\n",
      "Iteration 3997, loss = 0.69065831\n",
      "Iteration 3998, loss = 0.69065789\n",
      "Iteration 3999, loss = 0.69065747\n",
      "Iteration 4000, loss = 0.69065725\n",
      "Iteration 4001, loss = 0.69065642\n",
      "Iteration 4002, loss = 0.69065604\n",
      "Iteration 4003, loss = 0.69065545\n",
      "Iteration 4004, loss = 0.69065518\n",
      "Iteration 4005, loss = 0.69065493\n",
      "Iteration 4006, loss = 0.69065404\n",
      "Iteration 4007, loss = 0.69065372\n",
      "Iteration 4008, loss = 0.69065337\n",
      "Iteration 4009, loss = 0.69065270\n",
      "Iteration 4010, loss = 0.69065237\n",
      "Iteration 4011, loss = 0.69065200\n",
      "Iteration 4012, loss = 0.69065187\n",
      "Iteration 4013, loss = 0.69065122\n",
      "Iteration 4014, loss = 0.69065031\n",
      "Iteration 4015, loss = 0.69064990\n",
      "Iteration 4016, loss = 0.69064955\n",
      "Iteration 4017, loss = 0.69064897\n",
      "Iteration 4018, loss = 0.69064851\n",
      "Iteration 4019, loss = 0.69064793\n",
      "Iteration 4020, loss = 0.69064771\n",
      "Iteration 4021, loss = 0.69064738\n",
      "Iteration 4022, loss = 0.69064758\n",
      "Iteration 4023, loss = 0.69064632\n",
      "Iteration 4024, loss = 0.69064649\n",
      "Iteration 4025, loss = 0.69064561\n",
      "Iteration 4026, loss = 0.69064519\n",
      "Iteration 4027, loss = 0.69064486\n",
      "Iteration 4028, loss = 0.69064487\n",
      "Iteration 4029, loss = 0.69064378\n",
      "Iteration 4030, loss = 0.69064345\n",
      "Iteration 4031, loss = 0.69064349\n",
      "Iteration 4032, loss = 0.69064263\n",
      "Iteration 4033, loss = 0.69064216\n",
      "Iteration 4034, loss = 0.69064212\n",
      "Iteration 4035, loss = 0.69064169\n",
      "Iteration 4036, loss = 0.69064083\n",
      "Iteration 4037, loss = 0.69064077\n",
      "Iteration 4038, loss = 0.69064006\n",
      "Iteration 4039, loss = 0.69063976\n",
      "Iteration 4040, loss = 0.69063921\n",
      "Iteration 4041, loss = 0.69063891\n",
      "Iteration 4042, loss = 0.69063826\n",
      "Iteration 4043, loss = 0.69063846\n",
      "Iteration 4044, loss = 0.69063756\n",
      "Iteration 4045, loss = 0.69063698\n",
      "Iteration 4046, loss = 0.69063663\n",
      "Iteration 4047, loss = 0.69063715\n",
      "Iteration 4048, loss = 0.69063617\n",
      "Iteration 4049, loss = 0.69063543\n",
      "Iteration 4050, loss = 0.69063553\n",
      "Iteration 4051, loss = 0.69063482\n",
      "Iteration 4052, loss = 0.69063448\n",
      "Iteration 4053, loss = 0.69063394\n",
      "Iteration 4054, loss = 0.69063340\n",
      "Iteration 4055, loss = 0.69063307\n",
      "Iteration 4056, loss = 0.69063306\n",
      "Iteration 4057, loss = 0.69063196\n",
      "Iteration 4058, loss = 0.69063192\n",
      "Iteration 4059, loss = 0.69063112\n",
      "Iteration 4060, loss = 0.69063123\n",
      "Iteration 4061, loss = 0.69063055\n",
      "Iteration 4062, loss = 0.69062991\n",
      "Iteration 4063, loss = 0.69062949\n",
      "Iteration 4064, loss = 0.69062915\n",
      "Iteration 4065, loss = 0.69062864\n",
      "Iteration 4066, loss = 0.69062840\n",
      "Iteration 4067, loss = 0.69062805\n",
      "Iteration 4068, loss = 0.69062738\n",
      "Iteration 4069, loss = 0.69062702\n",
      "Iteration 4070, loss = 0.69062631\n",
      "Iteration 4071, loss = 0.69062601\n",
      "Iteration 4072, loss = 0.69062536\n",
      "Iteration 4073, loss = 0.69062530\n",
      "Iteration 4074, loss = 0.69062468\n",
      "Iteration 4075, loss = 0.69062409\n",
      "Iteration 4076, loss = 0.69062360\n",
      "Iteration 4077, loss = 0.69062329\n",
      "Iteration 4078, loss = 0.69062308\n",
      "Iteration 4079, loss = 0.69062222\n",
      "Iteration 4080, loss = 0.69062178\n",
      "Iteration 4081, loss = 0.69062128\n",
      "Iteration 4082, loss = 0.69062102\n",
      "Iteration 4083, loss = 0.69062065\n",
      "Iteration 4084, loss = 0.69062005\n",
      "Iteration 4085, loss = 0.69061976\n",
      "Iteration 4086, loss = 0.69061898\n",
      "Iteration 4087, loss = 0.69061881\n",
      "Iteration 4088, loss = 0.69061818\n",
      "Iteration 4089, loss = 0.69061792\n",
      "Iteration 4090, loss = 0.69061758\n",
      "Iteration 4091, loss = 0.69061744\n",
      "Iteration 4092, loss = 0.69061641\n",
      "Iteration 4093, loss = 0.69061591\n",
      "Iteration 4094, loss = 0.69061568\n",
      "Iteration 4095, loss = 0.69061570\n",
      "Iteration 4096, loss = 0.69061494\n",
      "Iteration 4097, loss = 0.69061458\n",
      "Iteration 4098, loss = 0.69061392\n",
      "Iteration 4099, loss = 0.69061379\n",
      "Iteration 4100, loss = 0.69061346\n",
      "Iteration 4101, loss = 0.69061319\n",
      "Iteration 4102, loss = 0.69061246\n",
      "Iteration 4103, loss = 0.69061185\n",
      "Iteration 4104, loss = 0.69061144\n",
      "Iteration 4105, loss = 0.69061125\n",
      "Iteration 4106, loss = 0.69061114\n",
      "Iteration 4107, loss = 0.69061022\n",
      "Iteration 4108, loss = 0.69061035\n",
      "Iteration 4109, loss = 0.69060968\n",
      "Iteration 4110, loss = 0.69060887\n",
      "Iteration 4111, loss = 0.69060849\n",
      "Iteration 4112, loss = 0.69060815\n",
      "Iteration 4113, loss = 0.69060797\n",
      "Iteration 4114, loss = 0.69060738\n",
      "Iteration 4115, loss = 0.69060643\n",
      "Iteration 4116, loss = 0.69060659\n",
      "Iteration 4117, loss = 0.69060625\n",
      "Iteration 4118, loss = 0.69060637\n",
      "Iteration 4119, loss = 0.69060590\n",
      "Iteration 4120, loss = 0.69060467\n",
      "Iteration 4121, loss = 0.69060442\n",
      "Iteration 4122, loss = 0.69060397\n",
      "Iteration 4123, loss = 0.69060364\n",
      "Iteration 4124, loss = 0.69060339\n",
      "Iteration 4125, loss = 0.69060285\n",
      "Iteration 4126, loss = 0.69060250\n",
      "Iteration 4127, loss = 0.69060179\n",
      "Iteration 4128, loss = 0.69060144\n",
      "Iteration 4129, loss = 0.69060234\n",
      "Iteration 4130, loss = 0.69060091\n",
      "Iteration 4131, loss = 0.69060020\n",
      "Iteration 4132, loss = 0.69059973\n",
      "Iteration 4133, loss = 0.69059911\n",
      "Iteration 4134, loss = 0.69059921\n",
      "Iteration 4135, loss = 0.69059811\n",
      "Iteration 4136, loss = 0.69059827\n",
      "Iteration 4137, loss = 0.69059785\n",
      "Iteration 4138, loss = 0.69059725\n",
      "Iteration 4139, loss = 0.69059645\n",
      "Iteration 4140, loss = 0.69059642\n",
      "Iteration 4141, loss = 0.69059558\n",
      "Iteration 4142, loss = 0.69059532\n",
      "Iteration 4143, loss = 0.69059540\n",
      "Iteration 4144, loss = 0.69059433\n",
      "Iteration 4145, loss = 0.69059432\n",
      "Iteration 4146, loss = 0.69059439\n",
      "Iteration 4147, loss = 0.69059332\n",
      "Iteration 4148, loss = 0.69059247\n",
      "Iteration 4149, loss = 0.69059207\n",
      "Iteration 4150, loss = 0.69059191\n",
      "Iteration 4151, loss = 0.69059162\n",
      "Iteration 4152, loss = 0.69059137\n",
      "Iteration 4153, loss = 0.69059047\n",
      "Iteration 4154, loss = 0.69059057\n",
      "Iteration 4155, loss = 0.69058993\n",
      "Iteration 4156, loss = 0.69058915\n",
      "Iteration 4157, loss = 0.69058915\n",
      "Iteration 4158, loss = 0.69058818\n",
      "Iteration 4159, loss = 0.69058812\n",
      "Iteration 4160, loss = 0.69058764\n",
      "Iteration 4161, loss = 0.69058697\n",
      "Iteration 4162, loss = 0.69058660\n",
      "Iteration 4163, loss = 0.69058660\n",
      "Iteration 4164, loss = 0.69058620\n",
      "Iteration 4165, loss = 0.69058557\n",
      "Iteration 4166, loss = 0.69058531\n",
      "Iteration 4167, loss = 0.69058460\n",
      "Iteration 4168, loss = 0.69058406\n",
      "Iteration 4169, loss = 0.69058376\n",
      "Iteration 4170, loss = 0.69058313\n",
      "Iteration 4171, loss = 0.69058272\n",
      "Iteration 4172, loss = 0.69058310\n",
      "Iteration 4173, loss = 0.69058190\n",
      "Iteration 4174, loss = 0.69058125\n",
      "Iteration 4175, loss = 0.69058108\n",
      "Iteration 4176, loss = 0.69058057\n",
      "Iteration 4177, loss = 0.69058019\n",
      "Iteration 4178, loss = 0.69057963\n",
      "Iteration 4179, loss = 0.69057925\n",
      "Iteration 4180, loss = 0.69057889\n",
      "Iteration 4181, loss = 0.69057888\n",
      "Iteration 4182, loss = 0.69057800\n",
      "Iteration 4183, loss = 0.69057802\n",
      "Iteration 4184, loss = 0.69057751\n",
      "Iteration 4185, loss = 0.69057710\n",
      "Iteration 4186, loss = 0.69057657\n",
      "Iteration 4187, loss = 0.69057603\n",
      "Iteration 4188, loss = 0.69057640\n",
      "Iteration 4189, loss = 0.69057624\n",
      "Iteration 4190, loss = 0.69057503\n",
      "Iteration 4191, loss = 0.69057435\n",
      "Iteration 4192, loss = 0.69057429\n",
      "Iteration 4193, loss = 0.69057393\n",
      "Iteration 4194, loss = 0.69057326\n",
      "Iteration 4195, loss = 0.69057281\n",
      "Iteration 4196, loss = 0.69057249\n",
      "Iteration 4197, loss = 0.69057235\n",
      "Iteration 4198, loss = 0.69057144\n",
      "Iteration 4199, loss = 0.69057122\n",
      "Iteration 4200, loss = 0.69057106\n",
      "Iteration 4201, loss = 0.69057017\n",
      "Iteration 4202, loss = 0.69057114\n",
      "Iteration 4203, loss = 0.69056985\n",
      "Iteration 4204, loss = 0.69056987\n",
      "Iteration 4205, loss = 0.69056891\n",
      "Iteration 4206, loss = 0.69056853\n",
      "Iteration 4207, loss = 0.69056866\n",
      "Iteration 4208, loss = 0.69056775\n",
      "Iteration 4209, loss = 0.69056756\n",
      "Iteration 4210, loss = 0.69056712\n",
      "Iteration 4211, loss = 0.69056657\n",
      "Iteration 4212, loss = 0.69056617\n",
      "Iteration 4213, loss = 0.69056569\n",
      "Iteration 4214, loss = 0.69056501\n",
      "Iteration 4215, loss = 0.69056459\n",
      "Iteration 4216, loss = 0.69056448\n",
      "Iteration 4217, loss = 0.69056443\n",
      "Iteration 4218, loss = 0.69056363\n",
      "Iteration 4219, loss = 0.69056372\n",
      "Iteration 4220, loss = 0.69056340\n",
      "Iteration 4221, loss = 0.69056254\n",
      "Iteration 4222, loss = 0.69056233\n",
      "Iteration 4223, loss = 0.69056144\n",
      "Iteration 4224, loss = 0.69056164\n",
      "Iteration 4225, loss = 0.69056087\n",
      "Iteration 4226, loss = 0.69056054\n",
      "Iteration 4227, loss = 0.69056008\n",
      "Iteration 4228, loss = 0.69055950\n",
      "Iteration 4229, loss = 0.69055908\n",
      "Iteration 4230, loss = 0.69055908\n",
      "Iteration 4231, loss = 0.69055830\n",
      "Iteration 4232, loss = 0.69055784\n",
      "Iteration 4233, loss = 0.69055778\n",
      "Iteration 4234, loss = 0.69055726\n",
      "Iteration 4235, loss = 0.69055701\n",
      "Iteration 4236, loss = 0.69055617\n",
      "Iteration 4237, loss = 0.69055582\n",
      "Iteration 4238, loss = 0.69055548\n",
      "Iteration 4239, loss = 0.69055499\n",
      "Iteration 4240, loss = 0.69055446\n",
      "Iteration 4241, loss = 0.69055409\n",
      "Iteration 4242, loss = 0.69055403\n",
      "Iteration 4243, loss = 0.69055328\n",
      "Iteration 4244, loss = 0.69055321\n",
      "Iteration 4245, loss = 0.69055297\n",
      "Iteration 4246, loss = 0.69055193\n",
      "Iteration 4247, loss = 0.69055171\n",
      "Iteration 4248, loss = 0.69055151\n",
      "Iteration 4249, loss = 0.69055137\n",
      "Iteration 4250, loss = 0.69055025\n",
      "Iteration 4251, loss = 0.69055004\n",
      "Iteration 4252, loss = 0.69054966\n",
      "Iteration 4253, loss = 0.69054968\n",
      "Iteration 4254, loss = 0.69054882\n",
      "Iteration 4255, loss = 0.69054861\n",
      "Iteration 4256, loss = 0.69054855\n",
      "Iteration 4257, loss = 0.69054735\n",
      "Iteration 4258, loss = 0.69054715\n",
      "Iteration 4259, loss = 0.69054689\n",
      "Iteration 4260, loss = 0.69054653\n",
      "Iteration 4261, loss = 0.69054603\n",
      "Iteration 4262, loss = 0.69054555\n",
      "Iteration 4263, loss = 0.69054516\n",
      "Iteration 4264, loss = 0.69054520\n",
      "Iteration 4265, loss = 0.69054419\n",
      "Iteration 4266, loss = 0.69054403\n",
      "Iteration 4267, loss = 0.69054331\n",
      "Iteration 4268, loss = 0.69054283\n",
      "Iteration 4269, loss = 0.69054292\n",
      "Iteration 4270, loss = 0.69054224\n",
      "Iteration 4271, loss = 0.69054204\n",
      "Iteration 4272, loss = 0.69054171\n",
      "Iteration 4273, loss = 0.69054107\n",
      "Iteration 4274, loss = 0.69054072\n",
      "Iteration 4275, loss = 0.69054030\n",
      "Iteration 4276, loss = 0.69054088\n",
      "Iteration 4277, loss = 0.69053978\n",
      "Iteration 4278, loss = 0.69053947\n",
      "Iteration 4279, loss = 0.69053909\n",
      "Iteration 4280, loss = 0.69053850\n",
      "Iteration 4281, loss = 0.69053757\n",
      "Iteration 4282, loss = 0.69053781\n",
      "Iteration 4283, loss = 0.69053804\n",
      "Iteration 4284, loss = 0.69053687\n",
      "Iteration 4285, loss = 0.69053587\n",
      "Iteration 4286, loss = 0.69053539\n",
      "Iteration 4287, loss = 0.69053509\n",
      "Iteration 4288, loss = 0.69053470\n",
      "Iteration 4289, loss = 0.69053409\n",
      "Iteration 4290, loss = 0.69053355\n",
      "Iteration 4291, loss = 0.69053398\n",
      "Iteration 4292, loss = 0.69053284\n",
      "Iteration 4293, loss = 0.69053272\n",
      "Iteration 4294, loss = 0.69053207\n",
      "Iteration 4295, loss = 0.69053174\n",
      "Iteration 4296, loss = 0.69053114\n",
      "Iteration 4297, loss = 0.69053127\n",
      "Iteration 4298, loss = 0.69053083\n",
      "Iteration 4299, loss = 0.69053054\n",
      "Iteration 4300, loss = 0.69053024\n",
      "Iteration 4301, loss = 0.69052921\n",
      "Iteration 4302, loss = 0.69052884\n",
      "Iteration 4303, loss = 0.69052847\n",
      "Iteration 4304, loss = 0.69052790\n",
      "Iteration 4305, loss = 0.69052751\n",
      "Iteration 4306, loss = 0.69052698\n",
      "Iteration 4307, loss = 0.69052709\n",
      "Iteration 4308, loss = 0.69052681\n",
      "Iteration 4309, loss = 0.69052585\n",
      "Iteration 4310, loss = 0.69052573\n",
      "Iteration 4311, loss = 0.69052538\n",
      "Iteration 4312, loss = 0.69052475\n",
      "Iteration 4313, loss = 0.69052439\n",
      "Iteration 4314, loss = 0.69052407\n",
      "Iteration 4315, loss = 0.69052375\n",
      "Iteration 4316, loss = 0.69052282\n",
      "Iteration 4317, loss = 0.69052265\n",
      "Iteration 4318, loss = 0.69052235\n",
      "Iteration 4319, loss = 0.69052196\n",
      "Iteration 4320, loss = 0.69052123\n",
      "Iteration 4321, loss = 0.69052137\n",
      "Iteration 4322, loss = 0.69052079\n",
      "Iteration 4323, loss = 0.69052023\n",
      "Iteration 4324, loss = 0.69052079\n",
      "Iteration 4325, loss = 0.69051923\n",
      "Iteration 4326, loss = 0.69051882\n",
      "Iteration 4327, loss = 0.69051873\n",
      "Iteration 4328, loss = 0.69051804\n",
      "Iteration 4329, loss = 0.69051854\n",
      "Iteration 4330, loss = 0.69051725\n",
      "Iteration 4331, loss = 0.69051728\n",
      "Iteration 4332, loss = 0.69051671\n",
      "Iteration 4333, loss = 0.69051650\n",
      "Iteration 4334, loss = 0.69051579\n",
      "Iteration 4335, loss = 0.69051545\n",
      "Iteration 4336, loss = 0.69051504\n",
      "Iteration 4337, loss = 0.69051477\n",
      "Iteration 4338, loss = 0.69051414\n",
      "Iteration 4339, loss = 0.69051388\n",
      "Iteration 4340, loss = 0.69051350\n",
      "Iteration 4341, loss = 0.69051339\n",
      "Iteration 4342, loss = 0.69051260\n",
      "Iteration 4343, loss = 0.69051234\n",
      "Iteration 4344, loss = 0.69051190\n",
      "Iteration 4345, loss = 0.69051160\n",
      "Iteration 4346, loss = 0.69051103\n",
      "Iteration 4347, loss = 0.69051074\n",
      "Iteration 4348, loss = 0.69051063\n",
      "Iteration 4349, loss = 0.69051002\n",
      "Iteration 4350, loss = 0.69050966\n",
      "Iteration 4351, loss = 0.69050957\n",
      "Iteration 4352, loss = 0.69050882\n",
      "Iteration 4353, loss = 0.69050854\n",
      "Iteration 4354, loss = 0.69050772\n",
      "Iteration 4355, loss = 0.69050775\n",
      "Iteration 4356, loss = 0.69050744\n",
      "Iteration 4357, loss = 0.69050656\n",
      "Iteration 4358, loss = 0.69050708\n",
      "Iteration 4359, loss = 0.69050595\n",
      "Iteration 4360, loss = 0.69050585\n",
      "Iteration 4361, loss = 0.69050531\n",
      "Iteration 4362, loss = 0.69050504\n",
      "Iteration 4363, loss = 0.69050461\n",
      "Iteration 4364, loss = 0.69050466\n",
      "Iteration 4365, loss = 0.69050373\n",
      "Iteration 4366, loss = 0.69050328\n",
      "Iteration 4367, loss = 0.69050299\n",
      "Iteration 4368, loss = 0.69050285\n",
      "Iteration 4369, loss = 0.69050229\n",
      "Iteration 4370, loss = 0.69050208\n",
      "Iteration 4371, loss = 0.69050139\n",
      "Iteration 4372, loss = 0.69050132\n",
      "Iteration 4373, loss = 0.69050093\n",
      "Iteration 4374, loss = 0.69050050\n",
      "Iteration 4375, loss = 0.69050025\n",
      "Iteration 4376, loss = 0.69050029\n",
      "Iteration 4377, loss = 0.69050024\n",
      "Iteration 4378, loss = 0.69049896\n",
      "Iteration 4379, loss = 0.69049844\n",
      "Iteration 4380, loss = 0.69049782\n",
      "Iteration 4381, loss = 0.69049781\n",
      "Iteration 4382, loss = 0.69049738\n",
      "Iteration 4383, loss = 0.69049752\n",
      "Iteration 4384, loss = 0.69049692\n",
      "Iteration 4385, loss = 0.69049595\n",
      "Iteration 4386, loss = 0.69049579\n",
      "Iteration 4387, loss = 0.69049506\n",
      "Iteration 4388, loss = 0.69049472\n",
      "Iteration 4389, loss = 0.69049423\n",
      "Iteration 4390, loss = 0.69049402\n",
      "Iteration 4391, loss = 0.69049369\n",
      "Iteration 4392, loss = 0.69049301\n",
      "Iteration 4393, loss = 0.69049298\n",
      "Iteration 4394, loss = 0.69049236\n",
      "Iteration 4395, loss = 0.69049198\n",
      "Iteration 4396, loss = 0.69049211\n",
      "Iteration 4397, loss = 0.69049138\n",
      "Iteration 4398, loss = 0.69049125\n",
      "Iteration 4399, loss = 0.69049116\n",
      "Iteration 4400, loss = 0.69049012\n",
      "Iteration 4401, loss = 0.69048974\n",
      "Iteration 4402, loss = 0.69048948\n",
      "Iteration 4403, loss = 0.69048892\n",
      "Iteration 4404, loss = 0.69048838\n",
      "Iteration 4405, loss = 0.69048867\n",
      "Iteration 4406, loss = 0.69048782\n",
      "Iteration 4407, loss = 0.69048779\n",
      "Iteration 4408, loss = 0.69048732\n",
      "Iteration 4409, loss = 0.69048679\n",
      "Iteration 4410, loss = 0.69048607\n",
      "Iteration 4411, loss = 0.69048614\n",
      "Iteration 4412, loss = 0.69048643\n",
      "Iteration 4413, loss = 0.69048526\n",
      "Iteration 4414, loss = 0.69048482\n",
      "Iteration 4415, loss = 0.69048425\n",
      "Iteration 4416, loss = 0.69048369\n",
      "Iteration 4417, loss = 0.69048340\n",
      "Iteration 4418, loss = 0.69048296\n",
      "Iteration 4419, loss = 0.69048250\n",
      "Iteration 4420, loss = 0.69048198\n",
      "Iteration 4421, loss = 0.69048159\n",
      "Iteration 4422, loss = 0.69048158\n",
      "Iteration 4423, loss = 0.69048139\n",
      "Iteration 4424, loss = 0.69048056\n",
      "Iteration 4425, loss = 0.69048013\n",
      "Iteration 4426, loss = 0.69047986\n",
      "Iteration 4427, loss = 0.69047938\n",
      "Iteration 4428, loss = 0.69047947\n",
      "Iteration 4429, loss = 0.69047866\n",
      "Iteration 4430, loss = 0.69047807\n",
      "Iteration 4431, loss = 0.69047806\n",
      "Iteration 4432, loss = 0.69047744\n",
      "Iteration 4433, loss = 0.69047699\n",
      "Iteration 4434, loss = 0.69047677\n",
      "Iteration 4435, loss = 0.69047617\n",
      "Iteration 4436, loss = 0.69047568\n",
      "Iteration 4437, loss = 0.69047549\n",
      "Iteration 4438, loss = 0.69047532\n",
      "Iteration 4439, loss = 0.69047500\n",
      "Iteration 4440, loss = 0.69047442\n",
      "Iteration 4441, loss = 0.69047385\n",
      "Iteration 4442, loss = 0.69047334\n",
      "Iteration 4443, loss = 0.69047323\n",
      "Iteration 4444, loss = 0.69047268\n",
      "Iteration 4445, loss = 0.69047234\n",
      "Iteration 4446, loss = 0.69047237\n",
      "Iteration 4447, loss = 0.69047156\n",
      "Iteration 4448, loss = 0.69047106\n",
      "Iteration 4449, loss = 0.69047080\n",
      "Iteration 4450, loss = 0.69047075\n",
      "Iteration 4451, loss = 0.69046988\n",
      "Iteration 4452, loss = 0.69046975\n",
      "Iteration 4453, loss = 0.69046970\n",
      "Iteration 4454, loss = 0.69046865\n",
      "Iteration 4455, loss = 0.69046831\n",
      "Iteration 4456, loss = 0.69046804\n",
      "Iteration 4457, loss = 0.69046746\n",
      "Iteration 4458, loss = 0.69046728\n",
      "Iteration 4459, loss = 0.69046701\n",
      "Iteration 4460, loss = 0.69046649\n",
      "Iteration 4461, loss = 0.69046633\n",
      "Iteration 4462, loss = 0.69046565\n",
      "Iteration 4463, loss = 0.69046525\n",
      "Iteration 4464, loss = 0.69046484\n",
      "Iteration 4465, loss = 0.69046465\n",
      "Iteration 4466, loss = 0.69046409\n",
      "Iteration 4467, loss = 0.69046396\n",
      "Iteration 4468, loss = 0.69046332\n",
      "Iteration 4469, loss = 0.69046402\n",
      "Iteration 4470, loss = 0.69046300\n",
      "Iteration 4471, loss = 0.69046214\n",
      "Iteration 4472, loss = 0.69046185\n",
      "Iteration 4473, loss = 0.69046191\n",
      "Iteration 4474, loss = 0.69046159\n",
      "Iteration 4475, loss = 0.69046174\n",
      "Iteration 4476, loss = 0.69046080\n",
      "Iteration 4477, loss = 0.69046036\n",
      "Iteration 4478, loss = 0.69045942\n",
      "Iteration 4479, loss = 0.69045911\n",
      "Iteration 4480, loss = 0.69045889\n",
      "Iteration 4481, loss = 0.69045835\n",
      "Iteration 4482, loss = 0.69045791\n",
      "Iteration 4483, loss = 0.69045751\n",
      "Iteration 4484, loss = 0.69045738\n",
      "Iteration 4485, loss = 0.69045715\n",
      "Iteration 4486, loss = 0.69045664\n",
      "Iteration 4487, loss = 0.69045591\n",
      "Iteration 4488, loss = 0.69045647\n",
      "Iteration 4489, loss = 0.69045521\n",
      "Iteration 4490, loss = 0.69045503\n",
      "Iteration 4491, loss = 0.69045427\n",
      "Iteration 4492, loss = 0.69045382\n",
      "Iteration 4493, loss = 0.69045372\n",
      "Iteration 4494, loss = 0.69045309\n",
      "Iteration 4495, loss = 0.69045356\n",
      "Iteration 4496, loss = 0.69045266\n",
      "Iteration 4497, loss = 0.69045231\n",
      "Iteration 4498, loss = 0.69045209\n",
      "Iteration 4499, loss = 0.69045193\n",
      "Iteration 4500, loss = 0.69045101\n",
      "Iteration 4501, loss = 0.69045096\n",
      "Iteration 4502, loss = 0.69045050\n",
      "Iteration 4503, loss = 0.69044983\n",
      "Iteration 4504, loss = 0.69044929\n",
      "Iteration 4505, loss = 0.69044873\n",
      "Iteration 4506, loss = 0.69044878\n",
      "Iteration 4507, loss = 0.69044813\n",
      "Iteration 4508, loss = 0.69044820\n",
      "Iteration 4509, loss = 0.69044768\n",
      "Iteration 4510, loss = 0.69044736\n",
      "Iteration 4511, loss = 0.69044726\n",
      "Iteration 4512, loss = 0.69044658\n",
      "Iteration 4513, loss = 0.69044607\n",
      "Iteration 4514, loss = 0.69044552\n",
      "Iteration 4515, loss = 0.69044554\n",
      "Iteration 4516, loss = 0.69044482\n",
      "Iteration 4517, loss = 0.69044452\n",
      "Iteration 4518, loss = 0.69044432\n",
      "Iteration 4519, loss = 0.69044398\n",
      "Iteration 4520, loss = 0.69044406\n",
      "Iteration 4521, loss = 0.69044334\n",
      "Iteration 4522, loss = 0.69044342\n",
      "Iteration 4523, loss = 0.69044288\n",
      "Iteration 4524, loss = 0.69044320\n",
      "Iteration 4525, loss = 0.69044165\n",
      "Iteration 4526, loss = 0.69044151\n",
      "Iteration 4527, loss = 0.69044089\n",
      "Iteration 4528, loss = 0.69044122\n",
      "Iteration 4529, loss = 0.69044066\n",
      "Iteration 4530, loss = 0.69043992\n",
      "Iteration 4531, loss = 0.69043955\n",
      "Iteration 4532, loss = 0.69044128\n",
      "Iteration 4533, loss = 0.69043909\n",
      "Iteration 4534, loss = 0.69043863\n",
      "Iteration 4535, loss = 0.69043808\n",
      "Iteration 4536, loss = 0.69043775\n",
      "Iteration 4537, loss = 0.69043737\n",
      "Iteration 4538, loss = 0.69043718\n",
      "Iteration 4539, loss = 0.69043675\n",
      "Iteration 4540, loss = 0.69043614\n",
      "Iteration 4541, loss = 0.69043645\n",
      "Iteration 4542, loss = 0.69043522\n",
      "Iteration 4543, loss = 0.69043500\n",
      "Iteration 4544, loss = 0.69043444\n",
      "Iteration 4545, loss = 0.69043457\n",
      "Iteration 4546, loss = 0.69043422\n",
      "Iteration 4547, loss = 0.69043379\n",
      "Iteration 4548, loss = 0.69043315\n",
      "Iteration 4549, loss = 0.69043267\n",
      "Iteration 4550, loss = 0.69043222\n",
      "Iteration 4551, loss = 0.69043197\n",
      "Iteration 4552, loss = 0.69043207\n",
      "Iteration 4553, loss = 0.69043129\n",
      "Iteration 4554, loss = 0.69043086\n",
      "Iteration 4555, loss = 0.69043084\n",
      "Iteration 4556, loss = 0.69043010\n",
      "Iteration 4557, loss = 0.69042969\n",
      "Iteration 4558, loss = 0.69042955\n",
      "Iteration 4559, loss = 0.69042915\n",
      "Iteration 4560, loss = 0.69042883\n",
      "Iteration 4561, loss = 0.69042869\n",
      "Iteration 4562, loss = 0.69042783\n",
      "Iteration 4563, loss = 0.69042737\n",
      "Iteration 4564, loss = 0.69042691\n",
      "Iteration 4565, loss = 0.69042648\n",
      "Iteration 4566, loss = 0.69042664\n",
      "Iteration 4567, loss = 0.69042612\n",
      "Iteration 4568, loss = 0.69042565\n",
      "Iteration 4569, loss = 0.69042552\n",
      "Iteration 4570, loss = 0.69042508\n",
      "Iteration 4571, loss = 0.69042440\n",
      "Iteration 4572, loss = 0.69042389\n",
      "Iteration 4573, loss = 0.69042357\n",
      "Iteration 4574, loss = 0.69042332\n",
      "Iteration 4575, loss = 0.69042312\n",
      "Iteration 4576, loss = 0.69042248\n",
      "Iteration 4577, loss = 0.69042218\n",
      "Iteration 4578, loss = 0.69042160\n",
      "Iteration 4579, loss = 0.69042138\n",
      "Iteration 4580, loss = 0.69042112\n",
      "Iteration 4581, loss = 0.69042076\n",
      "Iteration 4582, loss = 0.69042101\n",
      "Iteration 4583, loss = 0.69042010\n",
      "Iteration 4584, loss = 0.69041972\n",
      "Iteration 4585, loss = 0.69041951\n",
      "Iteration 4586, loss = 0.69041901\n",
      "Iteration 4587, loss = 0.69041909\n",
      "Iteration 4588, loss = 0.69041761\n",
      "Iteration 4589, loss = 0.69041808\n",
      "Iteration 4590, loss = 0.69041693\n",
      "Iteration 4591, loss = 0.69041696\n",
      "Iteration 4592, loss = 0.69041726\n",
      "Iteration 4593, loss = 0.69041612\n",
      "Iteration 4594, loss = 0.69041650\n",
      "Iteration 4595, loss = 0.69041543\n",
      "Iteration 4596, loss = 0.69041476\n",
      "Iteration 4597, loss = 0.69041502\n",
      "Iteration 4598, loss = 0.69041443\n",
      "Iteration 4599, loss = 0.69041364\n",
      "Iteration 4600, loss = 0.69041355\n",
      "Iteration 4601, loss = 0.69041325\n",
      "Iteration 4602, loss = 0.69041282\n",
      "Iteration 4603, loss = 0.69041267\n",
      "Iteration 4604, loss = 0.69041190\n",
      "Iteration 4605, loss = 0.69041190\n",
      "Iteration 4606, loss = 0.69041177\n",
      "Iteration 4607, loss = 0.69041067\n",
      "Iteration 4608, loss = 0.69041037\n",
      "Iteration 4609, loss = 0.69041012\n",
      "Iteration 4610, loss = 0.69040974\n",
      "Iteration 4611, loss = 0.69040933\n",
      "Iteration 4612, loss = 0.69040873\n",
      "Iteration 4613, loss = 0.69040854\n",
      "Iteration 4614, loss = 0.69040822\n",
      "Iteration 4615, loss = 0.69040809\n",
      "Iteration 4616, loss = 0.69040729\n",
      "Iteration 4617, loss = 0.69040723\n",
      "Iteration 4618, loss = 0.69040653\n",
      "Iteration 4619, loss = 0.69040653\n",
      "Iteration 4620, loss = 0.69040614\n",
      "Iteration 4621, loss = 0.69040566\n",
      "Iteration 4622, loss = 0.69040584\n",
      "Iteration 4623, loss = 0.69040519\n",
      "Iteration 4624, loss = 0.69040453\n",
      "Iteration 4625, loss = 0.69040433\n",
      "Iteration 4626, loss = 0.69040372\n",
      "Iteration 4627, loss = 0.69040434\n",
      "Iteration 4628, loss = 0.69040319\n",
      "Iteration 4629, loss = 0.69040395\n",
      "Iteration 4630, loss = 0.69040231\n",
      "Iteration 4631, loss = 0.69040227\n",
      "Iteration 4632, loss = 0.69040187\n",
      "Iteration 4633, loss = 0.69040197\n",
      "Iteration 4634, loss = 0.69040080\n",
      "Iteration 4635, loss = 0.69040059\n",
      "Iteration 4636, loss = 0.69040018\n",
      "Iteration 4637, loss = 0.69040000\n",
      "Iteration 4638, loss = 0.69039934\n",
      "Iteration 4639, loss = 0.69039913\n",
      "Iteration 4640, loss = 0.69039850\n",
      "Iteration 4641, loss = 0.69039841\n",
      "Iteration 4642, loss = 0.69039790\n",
      "Iteration 4643, loss = 0.69039735\n",
      "Iteration 4644, loss = 0.69039698\n",
      "Iteration 4645, loss = 0.69039673\n",
      "Iteration 4646, loss = 0.69039692\n",
      "Iteration 4647, loss = 0.69039609\n",
      "Iteration 4648, loss = 0.69039580\n",
      "Iteration 4649, loss = 0.69039545\n",
      "Iteration 4650, loss = 0.69039485\n",
      "Iteration 4651, loss = 0.69039512\n",
      "Iteration 4652, loss = 0.69039447\n",
      "Iteration 4653, loss = 0.69039379\n",
      "Iteration 4654, loss = 0.69039394\n",
      "Iteration 4655, loss = 0.69039313\n",
      "Iteration 4656, loss = 0.69039330\n",
      "Iteration 4657, loss = 0.69039244\n",
      "Iteration 4658, loss = 0.69039280\n",
      "Iteration 4659, loss = 0.69039190\n",
      "Iteration 4660, loss = 0.69039183\n",
      "Iteration 4661, loss = 0.69039140\n",
      "Iteration 4662, loss = 0.69039093\n",
      "Iteration 4663, loss = 0.69039019\n",
      "Iteration 4664, loss = 0.69039047\n",
      "Iteration 4665, loss = 0.69039040\n",
      "Iteration 4666, loss = 0.69038935\n",
      "Iteration 4667, loss = 0.69038890\n",
      "Iteration 4668, loss = 0.69038896\n",
      "Iteration 4669, loss = 0.69038819\n",
      "Iteration 4670, loss = 0.69038785\n",
      "Iteration 4671, loss = 0.69038754\n",
      "Iteration 4672, loss = 0.69038757\n",
      "Iteration 4673, loss = 0.69038691\n",
      "Iteration 4674, loss = 0.69038663\n",
      "Iteration 4675, loss = 0.69038598\n",
      "Iteration 4676, loss = 0.69038563\n",
      "Iteration 4677, loss = 0.69038519\n",
      "Iteration 4678, loss = 0.69038500\n",
      "Iteration 4679, loss = 0.69038494\n",
      "Iteration 4680, loss = 0.69038414\n",
      "Iteration 4681, loss = 0.69038378\n",
      "Iteration 4682, loss = 0.69038344\n",
      "Iteration 4683, loss = 0.69038310\n",
      "Iteration 4684, loss = 0.69038281\n",
      "Iteration 4685, loss = 0.69038251\n",
      "Iteration 4686, loss = 0.69038267\n",
      "Iteration 4687, loss = 0.69038268\n",
      "Iteration 4688, loss = 0.69038170\n",
      "Iteration 4689, loss = 0.69038113\n",
      "Iteration 4690, loss = 0.69038094\n",
      "Iteration 4691, loss = 0.69038058\n",
      "Iteration 4692, loss = 0.69037990\n",
      "Iteration 4693, loss = 0.69037988\n",
      "Iteration 4694, loss = 0.69037924\n",
      "Iteration 4695, loss = 0.69037907\n",
      "Iteration 4696, loss = 0.69037883\n",
      "Iteration 4697, loss = 0.69037866\n",
      "Iteration 4698, loss = 0.69037792\n",
      "Iteration 4699, loss = 0.69037773\n",
      "Iteration 4700, loss = 0.69037757\n",
      "Iteration 4701, loss = 0.69037686\n",
      "Iteration 4702, loss = 0.69037664\n",
      "Iteration 4703, loss = 0.69037600\n",
      "Iteration 4704, loss = 0.69037572\n",
      "Iteration 4705, loss = 0.69037547\n",
      "Iteration 4706, loss = 0.69037502\n",
      "Iteration 4707, loss = 0.69037542\n",
      "Iteration 4708, loss = 0.69037484\n",
      "Iteration 4709, loss = 0.69037451\n",
      "Iteration 4710, loss = 0.69037339\n",
      "Iteration 4711, loss = 0.69037378\n",
      "Iteration 4712, loss = 0.69037321\n",
      "Iteration 4713, loss = 0.69037287\n",
      "Iteration 4714, loss = 0.69037202\n",
      "Iteration 4715, loss = 0.69037225\n",
      "Iteration 4716, loss = 0.69037196\n",
      "Iteration 4717, loss = 0.69037237\n",
      "Iteration 4718, loss = 0.69037104\n",
      "Iteration 4719, loss = 0.69037084\n",
      "Iteration 4720, loss = 0.69037065\n",
      "Iteration 4721, loss = 0.69037048\n",
      "Iteration 4722, loss = 0.69037018\n",
      "Iteration 4723, loss = 0.69036947\n",
      "Iteration 4724, loss = 0.69036892\n",
      "Iteration 4725, loss = 0.69036882\n",
      "Iteration 4726, loss = 0.69036838\n",
      "Iteration 4727, loss = 0.69036799\n",
      "Iteration 4728, loss = 0.69036761\n",
      "Iteration 4729, loss = 0.69036729\n",
      "Iteration 4730, loss = 0.69036750\n",
      "Iteration 4731, loss = 0.69036674\n",
      "Iteration 4732, loss = 0.69036648\n",
      "Iteration 4733, loss = 0.69036622\n",
      "Iteration 4734, loss = 0.69036604\n",
      "Iteration 4735, loss = 0.69036563\n",
      "Iteration 4736, loss = 0.69036528\n",
      "Iteration 4737, loss = 0.69036487\n",
      "Iteration 4738, loss = 0.69036464\n",
      "Iteration 4739, loss = 0.69036419\n",
      "Iteration 4740, loss = 0.69036350\n",
      "Iteration 4741, loss = 0.69036328\n",
      "Iteration 4742, loss = 0.69036315\n",
      "Iteration 4743, loss = 0.69036276\n",
      "Iteration 4744, loss = 0.69036240\n",
      "Iteration 4745, loss = 0.69036198\n",
      "Iteration 4746, loss = 0.69036194\n",
      "Iteration 4747, loss = 0.69036172\n",
      "Iteration 4748, loss = 0.69036101\n",
      "Iteration 4749, loss = 0.69036058\n",
      "Iteration 4750, loss = 0.69036138\n",
      "Iteration 4751, loss = 0.69036020\n",
      "Iteration 4752, loss = 0.69035977\n",
      "Iteration 4753, loss = 0.69035907\n",
      "Iteration 4754, loss = 0.69035917\n",
      "Iteration 4755, loss = 0.69035866\n",
      "Iteration 4756, loss = 0.69035856\n",
      "Iteration 4757, loss = 0.69035804\n",
      "Iteration 4758, loss = 0.69035816\n",
      "Iteration 4759, loss = 0.69035739\n",
      "Iteration 4760, loss = 0.69035670\n",
      "Iteration 4761, loss = 0.69035674\n",
      "Iteration 4762, loss = 0.69035667\n",
      "Iteration 4763, loss = 0.69035606\n",
      "Iteration 4764, loss = 0.69035603\n",
      "Iteration 4765, loss = 0.69035518\n",
      "Iteration 4766, loss = 0.69035487\n",
      "Iteration 4767, loss = 0.69035457\n",
      "Iteration 4768, loss = 0.69035440\n",
      "Iteration 4769, loss = 0.69035433\n",
      "Iteration 4770, loss = 0.69035353\n",
      "Iteration 4771, loss = 0.69035391\n",
      "Iteration 4772, loss = 0.69035350\n",
      "Iteration 4773, loss = 0.69035270\n",
      "Iteration 4774, loss = 0.69035229\n",
      "Iteration 4775, loss = 0.69035221\n",
      "Iteration 4776, loss = 0.69035097\n",
      "Iteration 4777, loss = 0.69035136\n",
      "Iteration 4778, loss = 0.69035199\n",
      "Iteration 4779, loss = 0.69035048\n",
      "Iteration 4780, loss = 0.69035034\n",
      "Iteration 4781, loss = 0.69034975\n",
      "Iteration 4782, loss = 0.69034970\n",
      "Iteration 4783, loss = 0.69034911\n",
      "Iteration 4784, loss = 0.69034884\n",
      "Iteration 4785, loss = 0.69034933\n",
      "Iteration 4786, loss = 0.69034805\n",
      "Iteration 4787, loss = 0.69034795\n",
      "Iteration 4788, loss = 0.69034731\n",
      "Iteration 4789, loss = 0.69034711\n",
      "Iteration 4790, loss = 0.69034663\n",
      "Iteration 4791, loss = 0.69034626\n",
      "Iteration 4792, loss = 0.69034604\n",
      "Iteration 4793, loss = 0.69034560\n",
      "Iteration 4794, loss = 0.69034587\n",
      "Iteration 4795, loss = 0.69034463\n",
      "Iteration 4796, loss = 0.69034443\n",
      "Iteration 4797, loss = 0.69034457\n",
      "Iteration 4798, loss = 0.69034371\n",
      "Iteration 4799, loss = 0.69034341\n",
      "Iteration 4800, loss = 0.69034316\n",
      "Iteration 4801, loss = 0.69034302\n",
      "Iteration 4802, loss = 0.69034233\n",
      "Iteration 4803, loss = 0.69034207\n",
      "Iteration 4804, loss = 0.69034165\n",
      "Iteration 4805, loss = 0.69034146\n",
      "Iteration 4806, loss = 0.69034095\n",
      "Iteration 4807, loss = 0.69034043\n",
      "Iteration 4808, loss = 0.69034012\n",
      "Iteration 4809, loss = 0.69033994\n",
      "Iteration 4810, loss = 0.69033971\n",
      "Iteration 4811, loss = 0.69033979\n",
      "Iteration 4812, loss = 0.69033871\n",
      "Iteration 4813, loss = 0.69033892\n",
      "Iteration 4814, loss = 0.69033872\n",
      "Iteration 4815, loss = 0.69033784\n",
      "Iteration 4816, loss = 0.69033748\n",
      "Iteration 4817, loss = 0.69033718\n",
      "Iteration 4818, loss = 0.69033668\n",
      "Iteration 4819, loss = 0.69033733\n",
      "Iteration 4820, loss = 0.69033732\n",
      "Iteration 4821, loss = 0.69033574\n",
      "Iteration 4822, loss = 0.69033556\n",
      "Iteration 4823, loss = 0.69033504\n",
      "Iteration 4824, loss = 0.69033479\n",
      "Iteration 4825, loss = 0.69033479\n",
      "Iteration 4826, loss = 0.69033386\n",
      "Iteration 4827, loss = 0.69033387\n",
      "Iteration 4828, loss = 0.69033320\n",
      "Iteration 4829, loss = 0.69033304\n",
      "Iteration 4830, loss = 0.69033256\n",
      "Iteration 4831, loss = 0.69033217\n",
      "Iteration 4832, loss = 0.69033215\n",
      "Iteration 4833, loss = 0.69033167\n",
      "Iteration 4834, loss = 0.69033175\n",
      "Iteration 4835, loss = 0.69033085\n",
      "Iteration 4836, loss = 0.69033068\n",
      "Iteration 4837, loss = 0.69033063\n",
      "Iteration 4838, loss = 0.69032985\n",
      "Iteration 4839, loss = 0.69032938\n",
      "Iteration 4840, loss = 0.69032889\n",
      "Iteration 4841, loss = 0.69032850\n",
      "Iteration 4842, loss = 0.69032815\n",
      "Iteration 4843, loss = 0.69032818\n",
      "Iteration 4844, loss = 0.69032789\n",
      "Iteration 4845, loss = 0.69032770\n",
      "Iteration 4846, loss = 0.69032703\n",
      "Iteration 4847, loss = 0.69032679\n",
      "Iteration 4848, loss = 0.69032621\n",
      "Iteration 4849, loss = 0.69032581\n",
      "Iteration 4850, loss = 0.69032565\n",
      "Iteration 4851, loss = 0.69032544\n",
      "Iteration 4852, loss = 0.69032463\n",
      "Iteration 4853, loss = 0.69032488\n",
      "Iteration 4854, loss = 0.69032415\n",
      "Iteration 4855, loss = 0.69032428\n",
      "Iteration 4856, loss = 0.69032350\n",
      "Iteration 4857, loss = 0.69032353\n",
      "Iteration 4858, loss = 0.69032303\n",
      "Iteration 4859, loss = 0.69032245\n",
      "Iteration 4860, loss = 0.69032204\n",
      "Iteration 4861, loss = 0.69032175\n",
      "Iteration 4862, loss = 0.69032187\n",
      "Iteration 4863, loss = 0.69032104\n",
      "Iteration 4864, loss = 0.69032063\n",
      "Iteration 4865, loss = 0.69032050\n",
      "Iteration 4866, loss = 0.69031977\n",
      "Iteration 4867, loss = 0.69031987\n",
      "Iteration 4868, loss = 0.69031912\n",
      "Iteration 4869, loss = 0.69031905\n",
      "Iteration 4870, loss = 0.69031821\n",
      "Iteration 4871, loss = 0.69031846\n",
      "Iteration 4872, loss = 0.69031776\n",
      "Iteration 4873, loss = 0.69031736\n",
      "Iteration 4874, loss = 0.69031740\n",
      "Iteration 4875, loss = 0.69031665\n",
      "Iteration 4876, loss = 0.69031622\n",
      "Iteration 4877, loss = 0.69031612\n",
      "Iteration 4878, loss = 0.69031582\n",
      "Iteration 4879, loss = 0.69031578\n",
      "Iteration 4880, loss = 0.69031487\n",
      "Iteration 4881, loss = 0.69031500\n",
      "Iteration 4882, loss = 0.69031406\n",
      "Iteration 4883, loss = 0.69031410\n",
      "Iteration 4884, loss = 0.69031344\n",
      "Iteration 4885, loss = 0.69031347\n",
      "Iteration 4886, loss = 0.69031275\n",
      "Iteration 4887, loss = 0.69031244\n",
      "Iteration 4888, loss = 0.69031255\n",
      "Iteration 4889, loss = 0.69031162\n",
      "Iteration 4890, loss = 0.69031112\n",
      "Iteration 4891, loss = 0.69031107\n",
      "Iteration 4892, loss = 0.69031063\n",
      "Iteration 4893, loss = 0.69031103\n",
      "Iteration 4894, loss = 0.69031019\n",
      "Iteration 4895, loss = 0.69030953\n",
      "Iteration 4896, loss = 0.69030991\n",
      "Iteration 4897, loss = 0.69030875\n",
      "Iteration 4898, loss = 0.69030894\n",
      "Iteration 4899, loss = 0.69030848\n",
      "Iteration 4900, loss = 0.69030839\n",
      "Iteration 4901, loss = 0.69030740\n",
      "Iteration 4902, loss = 0.69030759\n",
      "Iteration 4903, loss = 0.69030672\n",
      "Iteration 4904, loss = 0.69030688\n",
      "Iteration 4905, loss = 0.69030668\n",
      "Iteration 4906, loss = 0.69030612\n",
      "Iteration 4907, loss = 0.69030620\n",
      "Iteration 4908, loss = 0.69030536\n",
      "Iteration 4909, loss = 0.69030481\n",
      "Iteration 4910, loss = 0.69030423\n",
      "Iteration 4911, loss = 0.69030440\n",
      "Iteration 4912, loss = 0.69030415\n",
      "Iteration 4913, loss = 0.69030364\n",
      "Iteration 4914, loss = 0.69030318\n",
      "Iteration 4915, loss = 0.69030240\n",
      "Iteration 4916, loss = 0.69030247\n",
      "Iteration 4917, loss = 0.69030195\n",
      "Iteration 4918, loss = 0.69030210\n",
      "Iteration 4919, loss = 0.69030141\n",
      "Iteration 4920, loss = 0.69030090\n",
      "Iteration 4921, loss = 0.69030046\n",
      "Iteration 4922, loss = 0.69030044\n",
      "Iteration 4923, loss = 0.69029994\n",
      "Iteration 4924, loss = 0.69029964\n",
      "Iteration 4925, loss = 0.69029945\n",
      "Iteration 4926, loss = 0.69029965\n",
      "Iteration 4927, loss = 0.69029850\n",
      "Iteration 4928, loss = 0.69029871\n",
      "Iteration 4929, loss = 0.69029828\n",
      "Iteration 4930, loss = 0.69029782\n",
      "Iteration 4931, loss = 0.69029749\n",
      "Iteration 4932, loss = 0.69029696\n",
      "Iteration 4933, loss = 0.69029685\n",
      "Iteration 4934, loss = 0.69029647\n",
      "Iteration 4935, loss = 0.69029582\n",
      "Iteration 4936, loss = 0.69029538\n",
      "Iteration 4937, loss = 0.69029528\n",
      "Iteration 4938, loss = 0.69029522\n",
      "Iteration 4939, loss = 0.69029456\n",
      "Iteration 4940, loss = 0.69029427\n",
      "Iteration 4941, loss = 0.69029397\n",
      "Iteration 4942, loss = 0.69029403\n",
      "Iteration 4943, loss = 0.69029390\n",
      "Iteration 4944, loss = 0.69029394\n",
      "Iteration 4945, loss = 0.69029349\n",
      "Iteration 4946, loss = 0.69029237\n",
      "Iteration 4947, loss = 0.69029219\n",
      "Iteration 4948, loss = 0.69029199\n",
      "Iteration 4949, loss = 0.69029109\n",
      "Iteration 4950, loss = 0.69029086\n",
      "Iteration 4951, loss = 0.69029045\n",
      "Iteration 4952, loss = 0.69029073\n",
      "Iteration 4953, loss = 0.69028999\n",
      "Iteration 4954, loss = 0.69028976\n",
      "Iteration 4955, loss = 0.69028960\n",
      "Iteration 4956, loss = 0.69028908\n",
      "Iteration 4957, loss = 0.69028834\n",
      "Iteration 4958, loss = 0.69028838\n",
      "Iteration 4959, loss = 0.69028828\n",
      "Iteration 4960, loss = 0.69028766\n",
      "Iteration 4961, loss = 0.69028737\n",
      "Iteration 4962, loss = 0.69028690\n",
      "Iteration 4963, loss = 0.69028684\n",
      "Iteration 4964, loss = 0.69028625\n",
      "Iteration 4965, loss = 0.69028600\n",
      "Iteration 4966, loss = 0.69028575\n",
      "Iteration 4967, loss = 0.69028551\n",
      "Iteration 4968, loss = 0.69028516\n",
      "Iteration 4969, loss = 0.69028481\n",
      "Iteration 4970, loss = 0.69028468\n",
      "Iteration 4971, loss = 0.69028412\n",
      "Iteration 4972, loss = 0.69028405\n",
      "Iteration 4973, loss = 0.69028330\n",
      "Iteration 4974, loss = 0.69028335\n",
      "Iteration 4975, loss = 0.69028245\n",
      "Iteration 4976, loss = 0.69028363\n",
      "Iteration 4977, loss = 0.69028216\n",
      "Iteration 4978, loss = 0.69028177\n",
      "Iteration 4979, loss = 0.69028167\n",
      "Iteration 4980, loss = 0.69028108\n",
      "Iteration 4981, loss = 0.69028119\n",
      "Iteration 4982, loss = 0.69028127\n",
      "Iteration 4983, loss = 0.69028012\n",
      "Iteration 4984, loss = 0.69028033\n",
      "Iteration 4985, loss = 0.69027959\n",
      "Iteration 4986, loss = 0.69027930\n",
      "Iteration 4987, loss = 0.69027930\n",
      "Iteration 4988, loss = 0.69027856\n",
      "Iteration 4989, loss = 0.69027848\n",
      "Iteration 4990, loss = 0.69027821\n",
      "Iteration 4991, loss = 0.69027761\n",
      "Iteration 4992, loss = 0.69027756\n",
      "Iteration 4993, loss = 0.69027729\n",
      "Iteration 4994, loss = 0.69027658\n",
      "Iteration 4995, loss = 0.69027624\n",
      "Iteration 4996, loss = 0.69027619\n",
      "Iteration 4997, loss = 0.69027557\n",
      "Iteration 4998, loss = 0.69027528\n",
      "Iteration 4999, loss = 0.69027525\n",
      "Iteration 5000, loss = 0.69027520\n",
      "Iteration 5001, loss = 0.69027477\n",
      "Iteration 5002, loss = 0.69027442\n",
      "Iteration 5003, loss = 0.69027388\n",
      "Iteration 5004, loss = 0.69027349\n",
      "Iteration 5005, loss = 0.69027315\n",
      "Iteration 5006, loss = 0.69027293\n",
      "Iteration 5007, loss = 0.69027250\n",
      "Iteration 5008, loss = 0.69027234\n",
      "Iteration 5009, loss = 0.69027217\n",
      "Iteration 5010, loss = 0.69027145\n",
      "Iteration 5011, loss = 0.69027117\n",
      "Iteration 5012, loss = 0.69027067\n",
      "Iteration 5013, loss = 0.69027104\n",
      "Iteration 5014, loss = 0.69027047\n",
      "Iteration 5015, loss = 0.69027040\n",
      "Iteration 5016, loss = 0.69027032\n",
      "Iteration 5017, loss = 0.69026992\n",
      "Iteration 5018, loss = 0.69026972\n",
      "Iteration 5019, loss = 0.69026953\n",
      "Iteration 5020, loss = 0.69026898\n",
      "Iteration 5021, loss = 0.69026835\n",
      "Iteration 5022, loss = 0.69026800\n",
      "Iteration 5023, loss = 0.69026806\n",
      "Iteration 5024, loss = 0.69026775\n",
      "Iteration 5025, loss = 0.69026724\n",
      "Iteration 5026, loss = 0.69026683\n",
      "Iteration 5027, loss = 0.69026664\n",
      "Iteration 5028, loss = 0.69026608\n",
      "Iteration 5029, loss = 0.69026583\n",
      "Iteration 5030, loss = 0.69026577\n",
      "Iteration 5031, loss = 0.69026563\n",
      "Iteration 5032, loss = 0.69026500\n",
      "Iteration 5033, loss = 0.69026533\n",
      "Iteration 5034, loss = 0.69026433\n",
      "Iteration 5035, loss = 0.69026384\n",
      "Iteration 5036, loss = 0.69026390\n",
      "Iteration 5037, loss = 0.69026329\n",
      "Iteration 5038, loss = 0.69026279\n",
      "Iteration 5039, loss = 0.69026283\n",
      "Iteration 5040, loss = 0.69026236\n",
      "Iteration 5041, loss = 0.69026193\n",
      "Iteration 5042, loss = 0.69026159\n",
      "Iteration 5043, loss = 0.69026168\n",
      "Iteration 5044, loss = 0.69026144\n",
      "Iteration 5045, loss = 0.69026086\n",
      "Iteration 5046, loss = 0.69026047\n",
      "Iteration 5047, loss = 0.69026006\n",
      "Iteration 5048, loss = 0.69026039\n",
      "Iteration 5049, loss = 0.69025944\n",
      "Iteration 5050, loss = 0.69025913\n",
      "Iteration 5051, loss = 0.69025873\n",
      "Iteration 5052, loss = 0.69025854\n",
      "Iteration 5053, loss = 0.69025820\n",
      "Iteration 5054, loss = 0.69025773\n",
      "Iteration 5055, loss = 0.69025760\n",
      "Iteration 5056, loss = 0.69025814\n",
      "Iteration 5057, loss = 0.69025690\n",
      "Iteration 5058, loss = 0.69025752\n",
      "Iteration 5059, loss = 0.69025630\n",
      "Iteration 5060, loss = 0.69025574\n",
      "Iteration 5061, loss = 0.69025560\n",
      "Iteration 5062, loss = 0.69025543\n",
      "Iteration 5063, loss = 0.69025506\n",
      "Iteration 5064, loss = 0.69025490\n",
      "Iteration 5065, loss = 0.69025406\n",
      "Iteration 5066, loss = 0.69025410\n",
      "Iteration 5067, loss = 0.69025353\n",
      "Iteration 5068, loss = 0.69025321\n",
      "Iteration 5069, loss = 0.69025328\n",
      "Iteration 5070, loss = 0.69025283\n",
      "Iteration 5071, loss = 0.69025214\n",
      "Iteration 5072, loss = 0.69025206\n",
      "Iteration 5073, loss = 0.69025164\n",
      "Iteration 5074, loss = 0.69025131\n",
      "Iteration 5075, loss = 0.69025106\n",
      "Iteration 5076, loss = 0.69025074\n",
      "Iteration 5077, loss = 0.69025004\n",
      "Iteration 5078, loss = 0.69024996\n",
      "Iteration 5079, loss = 0.69024968\n",
      "Iteration 5080, loss = 0.69024961\n",
      "Iteration 5081, loss = 0.69024893\n",
      "Iteration 5082, loss = 0.69024856\n",
      "Iteration 5083, loss = 0.69024941\n",
      "Iteration 5084, loss = 0.69024808\n",
      "Iteration 5085, loss = 0.69024824\n",
      "Iteration 5086, loss = 0.69024758\n",
      "Iteration 5087, loss = 0.69024714\n",
      "Iteration 5088, loss = 0.69024674\n",
      "Iteration 5089, loss = 0.69024626\n",
      "Iteration 5090, loss = 0.69024650\n",
      "Iteration 5091, loss = 0.69024610\n",
      "Iteration 5092, loss = 0.69024548\n",
      "Iteration 5093, loss = 0.69024509\n",
      "Iteration 5094, loss = 0.69024515\n",
      "Iteration 5095, loss = 0.69024447\n",
      "Iteration 5096, loss = 0.69024410\n",
      "Iteration 5097, loss = 0.69024409\n",
      "Iteration 5098, loss = 0.69024391\n",
      "Iteration 5099, loss = 0.69024359\n",
      "Iteration 5100, loss = 0.69024289\n",
      "Iteration 5101, loss = 0.69024331\n",
      "Iteration 5102, loss = 0.69024206\n",
      "Iteration 5103, loss = 0.69024189\n",
      "Iteration 5104, loss = 0.69024165\n",
      "Iteration 5105, loss = 0.69024136\n",
      "Iteration 5106, loss = 0.69024125\n",
      "Iteration 5107, loss = 0.69024067\n",
      "Iteration 5108, loss = 0.69024032\n",
      "Iteration 5109, loss = 0.69024000\n",
      "Iteration 5110, loss = 0.69023973\n",
      "Iteration 5111, loss = 0.69023932\n",
      "Iteration 5112, loss = 0.69023908\n",
      "Iteration 5113, loss = 0.69023929\n",
      "Iteration 5114, loss = 0.69023831\n",
      "Iteration 5115, loss = 0.69023813\n",
      "Iteration 5116, loss = 0.69023765\n",
      "Iteration 5117, loss = 0.69023762\n",
      "Iteration 5118, loss = 0.69023729\n",
      "Iteration 5119, loss = 0.69023791\n",
      "Iteration 5120, loss = 0.69023669\n",
      "Iteration 5121, loss = 0.69023604\n",
      "Iteration 5122, loss = 0.69023572\n",
      "Iteration 5123, loss = 0.69023527\n",
      "Iteration 5124, loss = 0.69023486\n",
      "Iteration 5125, loss = 0.69023512\n",
      "Iteration 5126, loss = 0.69023447\n",
      "Iteration 5127, loss = 0.69023417\n",
      "Iteration 5128, loss = 0.69023427\n",
      "Iteration 5129, loss = 0.69023352\n",
      "Iteration 5130, loss = 0.69023336\n",
      "Iteration 5131, loss = 0.69023298\n",
      "Iteration 5132, loss = 0.69023276\n",
      "Iteration 5133, loss = 0.69023250\n",
      "Iteration 5134, loss = 0.69023215\n",
      "Iteration 5135, loss = 0.69023202\n",
      "Iteration 5136, loss = 0.69023142\n",
      "Iteration 5137, loss = 0.69023101\n",
      "Iteration 5138, loss = 0.69023065\n",
      "Iteration 5139, loss = 0.69023043\n",
      "Iteration 5140, loss = 0.69023069\n",
      "Iteration 5141, loss = 0.69022993\n",
      "Iteration 5142, loss = 0.69022972\n",
      "Iteration 5143, loss = 0.69022920\n",
      "Iteration 5144, loss = 0.69022877\n",
      "Iteration 5145, loss = 0.69022863\n",
      "Iteration 5146, loss = 0.69022826\n",
      "Iteration 5147, loss = 0.69022787\n",
      "Iteration 5148, loss = 0.69022752\n",
      "Iteration 5149, loss = 0.69022824\n",
      "Iteration 5150, loss = 0.69022787\n",
      "Iteration 5151, loss = 0.69022643\n",
      "Iteration 5152, loss = 0.69022637\n",
      "Iteration 5153, loss = 0.69022620\n",
      "Iteration 5154, loss = 0.69022585\n",
      "Iteration 5155, loss = 0.69022540\n",
      "Iteration 5156, loss = 0.69022505\n",
      "Iteration 5157, loss = 0.69022487\n",
      "Iteration 5158, loss = 0.69022434\n",
      "Iteration 5159, loss = 0.69022468\n",
      "Iteration 5160, loss = 0.69022365\n",
      "Iteration 5161, loss = 0.69022366\n",
      "Iteration 5162, loss = 0.69022333\n",
      "Iteration 5163, loss = 0.69022287\n",
      "Iteration 5164, loss = 0.69022244\n",
      "Iteration 5165, loss = 0.69022229\n",
      "Iteration 5166, loss = 0.69022195\n",
      "Iteration 5167, loss = 0.69022141\n",
      "Iteration 5168, loss = 0.69022126\n",
      "Iteration 5169, loss = 0.69022111\n",
      "Iteration 5170, loss = 0.69022061\n",
      "Iteration 5171, loss = 0.69022028\n",
      "Iteration 5172, loss = 0.69022024\n",
      "Iteration 5173, loss = 0.69021942\n",
      "Iteration 5174, loss = 0.69021939\n",
      "Iteration 5175, loss = 0.69021870\n",
      "Iteration 5176, loss = 0.69021882\n",
      "Iteration 5177, loss = 0.69021831\n",
      "Iteration 5178, loss = 0.69021782\n",
      "Iteration 5179, loss = 0.69021762\n",
      "Iteration 5180, loss = 0.69021756\n",
      "Iteration 5181, loss = 0.69021716\n",
      "Iteration 5182, loss = 0.69021662\n",
      "Iteration 5183, loss = 0.69021668\n",
      "Iteration 5184, loss = 0.69021633\n",
      "Iteration 5185, loss = 0.69021592\n",
      "Iteration 5186, loss = 0.69021541\n",
      "Iteration 5187, loss = 0.69021574\n",
      "Iteration 5188, loss = 0.69021482\n",
      "Iteration 5189, loss = 0.69021484\n",
      "Iteration 5190, loss = 0.69021430\n",
      "Iteration 5191, loss = 0.69021514\n",
      "Iteration 5192, loss = 0.69021361\n",
      "Iteration 5193, loss = 0.69021389\n",
      "Iteration 5194, loss = 0.69021324\n",
      "Iteration 5195, loss = 0.69021308\n",
      "Iteration 5196, loss = 0.69021238\n",
      "Iteration 5197, loss = 0.69021218\n",
      "Iteration 5198, loss = 0.69021189\n",
      "Iteration 5199, loss = 0.69021133\n",
      "Iteration 5200, loss = 0.69021124\n",
      "Iteration 5201, loss = 0.69021084\n",
      "Iteration 5202, loss = 0.69021036\n",
      "Iteration 5203, loss = 0.69021056\n",
      "Iteration 5204, loss = 0.69021000\n",
      "Iteration 5205, loss = 0.69020952\n",
      "Iteration 5206, loss = 0.69020977\n",
      "Iteration 5207, loss = 0.69020873\n",
      "Iteration 5208, loss = 0.69020840\n",
      "Iteration 5209, loss = 0.69020845\n",
      "Iteration 5210, loss = 0.69020785\n",
      "Iteration 5211, loss = 0.69020825\n",
      "Iteration 5212, loss = 0.69020798\n",
      "Iteration 5213, loss = 0.69020722\n",
      "Iteration 5214, loss = 0.69020689\n",
      "Iteration 5215, loss = 0.69020638\n",
      "Iteration 5216, loss = 0.69020593\n",
      "Iteration 5217, loss = 0.69020573\n",
      "Iteration 5218, loss = 0.69020577\n",
      "Iteration 5219, loss = 0.69020533\n",
      "Iteration 5220, loss = 0.69020530\n",
      "Iteration 5221, loss = 0.69020462\n",
      "Iteration 5222, loss = 0.69020443\n",
      "Iteration 5223, loss = 0.69020483\n",
      "Iteration 5224, loss = 0.69020380\n",
      "Iteration 5225, loss = 0.69020341\n",
      "Iteration 5226, loss = 0.69020344\n",
      "Iteration 5227, loss = 0.69020328\n",
      "Iteration 5228, loss = 0.69020262\n",
      "Iteration 5229, loss = 0.69020297\n",
      "Iteration 5230, loss = 0.69020195\n",
      "Iteration 5231, loss = 0.69020200\n",
      "Iteration 5232, loss = 0.69020132\n",
      "Iteration 5233, loss = 0.69020121\n",
      "Iteration 5234, loss = 0.69020080\n",
      "Iteration 5235, loss = 0.69020044\n",
      "Iteration 5236, loss = 0.69020028\n",
      "Iteration 5237, loss = 0.69020008\n",
      "Iteration 5238, loss = 0.69019955\n",
      "Iteration 5239, loss = 0.69019917\n",
      "Iteration 5240, loss = 0.69019908\n",
      "Iteration 5241, loss = 0.69019862\n",
      "Iteration 5242, loss = 0.69019853\n",
      "Iteration 5243, loss = 0.69019828\n",
      "Iteration 5244, loss = 0.69019800\n",
      "Iteration 5245, loss = 0.69019808\n",
      "Iteration 5246, loss = 0.69019709\n",
      "Iteration 5247, loss = 0.69019711\n",
      "Iteration 5248, loss = 0.69019676\n",
      "Iteration 5249, loss = 0.69019653\n",
      "Iteration 5250, loss = 0.69019609\n",
      "Iteration 5251, loss = 0.69019621\n",
      "Iteration 5252, loss = 0.69019537\n",
      "Iteration 5253, loss = 0.69019527\n",
      "Iteration 5254, loss = 0.69019496\n",
      "Iteration 5255, loss = 0.69019490\n",
      "Iteration 5256, loss = 0.69019417\n",
      "Iteration 5257, loss = 0.69019399\n",
      "Iteration 5258, loss = 0.69019355\n",
      "Iteration 5259, loss = 0.69019335\n",
      "Iteration 5260, loss = 0.69019305\n",
      "Iteration 5261, loss = 0.69019248\n",
      "Iteration 5262, loss = 0.69019226\n",
      "Iteration 5263, loss = 0.69019221\n",
      "Iteration 5264, loss = 0.69019257\n",
      "Iteration 5265, loss = 0.69019145\n",
      "Iteration 5266, loss = 0.69019115\n",
      "Iteration 5267, loss = 0.69019097\n",
      "Iteration 5268, loss = 0.69019064\n",
      "Iteration 5269, loss = 0.69019025\n",
      "Iteration 5270, loss = 0.69019014\n",
      "Iteration 5271, loss = 0.69018974\n",
      "Iteration 5272, loss = 0.69018931\n",
      "Iteration 5273, loss = 0.69018913\n",
      "Iteration 5274, loss = 0.69018923\n",
      "Iteration 5275, loss = 0.69018831\n",
      "Iteration 5276, loss = 0.69018855\n",
      "Iteration 5277, loss = 0.69018806\n",
      "Iteration 5278, loss = 0.69018829\n",
      "Iteration 5279, loss = 0.69018733\n",
      "Iteration 5280, loss = 0.69018690\n",
      "Iteration 5281, loss = 0.69018657\n",
      "Iteration 5282, loss = 0.69018650\n",
      "Iteration 5283, loss = 0.69018606\n",
      "Iteration 5284, loss = 0.69018626\n",
      "Iteration 5285, loss = 0.69018567\n",
      "Iteration 5286, loss = 0.69018543\n",
      "Iteration 5287, loss = 0.69018535\n",
      "Iteration 5288, loss = 0.69018519\n",
      "Iteration 5289, loss = 0.69018449\n",
      "Iteration 5290, loss = 0.69018437\n",
      "Iteration 5291, loss = 0.69018345\n",
      "Iteration 5292, loss = 0.69018334\n",
      "Iteration 5293, loss = 0.69018305\n",
      "Iteration 5294, loss = 0.69018259\n",
      "Iteration 5295, loss = 0.69018222\n",
      "Iteration 5296, loss = 0.69018195\n",
      "Iteration 5297, loss = 0.69018207\n",
      "Iteration 5298, loss = 0.69018155\n",
      "Iteration 5299, loss = 0.69018086\n",
      "Iteration 5300, loss = 0.69018073\n",
      "Iteration 5301, loss = 0.69018059\n",
      "Iteration 5302, loss = 0.69018011\n",
      "Iteration 5303, loss = 0.69018017\n",
      "Iteration 5304, loss = 0.69017949\n",
      "Iteration 5305, loss = 0.69017928\n",
      "Iteration 5306, loss = 0.69017901\n",
      "Iteration 5307, loss = 0.69017876\n",
      "Iteration 5308, loss = 0.69017837\n",
      "Iteration 5309, loss = 0.69017870\n",
      "Iteration 5310, loss = 0.69017830\n",
      "Iteration 5311, loss = 0.69017764\n",
      "Iteration 5312, loss = 0.69017805\n",
      "Iteration 5313, loss = 0.69017747\n",
      "Iteration 5314, loss = 0.69017696\n",
      "Iteration 5315, loss = 0.69017627\n",
      "Iteration 5316, loss = 0.69017647\n",
      "Iteration 5317, loss = 0.69017591\n",
      "Iteration 5318, loss = 0.69017555\n",
      "Iteration 5319, loss = 0.69017523\n",
      "Iteration 5320, loss = 0.69017486\n",
      "Iteration 5321, loss = 0.69017467\n",
      "Iteration 5322, loss = 0.69017443\n",
      "Iteration 5323, loss = 0.69017417\n",
      "Iteration 5324, loss = 0.69017375\n",
      "Iteration 5325, loss = 0.69017360\n",
      "Iteration 5326, loss = 0.69017312\n",
      "Iteration 5327, loss = 0.69017287\n",
      "Iteration 5328, loss = 0.69017348\n",
      "Iteration 5329, loss = 0.69017270\n",
      "Iteration 5330, loss = 0.69017251\n",
      "Iteration 5331, loss = 0.69017164\n",
      "Iteration 5332, loss = 0.69017177\n",
      "Iteration 5333, loss = 0.69017089\n",
      "Iteration 5334, loss = 0.69017056\n",
      "Iteration 5335, loss = 0.69017023\n",
      "Iteration 5336, loss = 0.69017039\n",
      "Iteration 5337, loss = 0.69016990\n",
      "Iteration 5338, loss = 0.69016935\n",
      "Iteration 5339, loss = 0.69016914\n",
      "Iteration 5340, loss = 0.69016883\n",
      "Iteration 5341, loss = 0.69016891\n",
      "Iteration 5342, loss = 0.69016844\n",
      "Iteration 5343, loss = 0.69016799\n",
      "Iteration 5344, loss = 0.69016795\n",
      "Iteration 5345, loss = 0.69016783\n",
      "Iteration 5346, loss = 0.69016743\n",
      "Iteration 5347, loss = 0.69016746\n",
      "Iteration 5348, loss = 0.69016669\n",
      "Iteration 5349, loss = 0.69016635\n",
      "Iteration 5350, loss = 0.69016595\n",
      "Iteration 5351, loss = 0.69016621\n",
      "Iteration 5352, loss = 0.69016527\n",
      "Iteration 5353, loss = 0.69016530\n",
      "Iteration 5354, loss = 0.69016506\n",
      "Iteration 5355, loss = 0.69016485\n",
      "Iteration 5356, loss = 0.69016452\n",
      "Iteration 5357, loss = 0.69016392\n",
      "Iteration 5358, loss = 0.69016364\n",
      "Iteration 5359, loss = 0.69016371\n",
      "Iteration 5360, loss = 0.69016290\n",
      "Iteration 5361, loss = 0.69016272\n",
      "Iteration 5362, loss = 0.69016269\n",
      "Iteration 5363, loss = 0.69016214\n",
      "Iteration 5364, loss = 0.69016222\n",
      "Iteration 5365, loss = 0.69016173\n",
      "Iteration 5366, loss = 0.69016133\n",
      "Iteration 5367, loss = 0.69016147\n",
      "Iteration 5368, loss = 0.69016143\n",
      "Iteration 5369, loss = 0.69016047\n",
      "Iteration 5370, loss = 0.69016017\n",
      "Iteration 5371, loss = 0.69016013\n",
      "Iteration 5372, loss = 0.69015966\n",
      "Iteration 5373, loss = 0.69015909\n",
      "Iteration 5374, loss = 0.69015935\n",
      "Iteration 5375, loss = 0.69015934\n",
      "Iteration 5376, loss = 0.69015847\n",
      "Iteration 5377, loss = 0.69015797\n",
      "Iteration 5378, loss = 0.69015789\n",
      "Iteration 5379, loss = 0.69015743\n",
      "Iteration 5380, loss = 0.69015708\n",
      "Iteration 5381, loss = 0.69015690\n",
      "Iteration 5382, loss = 0.69015660\n",
      "Iteration 5383, loss = 0.69015619\n",
      "Iteration 5384, loss = 0.69015588\n",
      "Iteration 5385, loss = 0.69015544\n",
      "Iteration 5386, loss = 0.69015580\n",
      "Iteration 5387, loss = 0.69015515\n",
      "Iteration 5388, loss = 0.69015591\n",
      "Iteration 5389, loss = 0.69015462\n",
      "Iteration 5390, loss = 0.69015450\n",
      "Iteration 5391, loss = 0.69015414\n",
      "Iteration 5392, loss = 0.69015404\n",
      "Iteration 5393, loss = 0.69015323\n",
      "Iteration 5394, loss = 0.69015325\n",
      "Iteration 5395, loss = 0.69015265\n",
      "Iteration 5396, loss = 0.69015231\n",
      "Iteration 5397, loss = 0.69015202\n",
      "Iteration 5398, loss = 0.69015183\n",
      "Iteration 5399, loss = 0.69015182\n",
      "Iteration 5400, loss = 0.69015200\n",
      "Iteration 5401, loss = 0.69015067\n",
      "Iteration 5402, loss = 0.69015054\n",
      "Iteration 5403, loss = 0.69015061\n",
      "Iteration 5404, loss = 0.69015037\n",
      "Iteration 5405, loss = 0.69014982\n",
      "Iteration 5406, loss = 0.69014964\n",
      "Iteration 5407, loss = 0.69014925\n",
      "Iteration 5408, loss = 0.69014880\n",
      "Iteration 5409, loss = 0.69014896\n",
      "Iteration 5410, loss = 0.69014842\n",
      "Iteration 5411, loss = 0.69014804\n",
      "Iteration 5412, loss = 0.69014807\n",
      "Iteration 5413, loss = 0.69014789\n",
      "Iteration 5414, loss = 0.69014751\n",
      "Iteration 5415, loss = 0.69014686\n",
      "Iteration 5416, loss = 0.69014682\n",
      "Iteration 5417, loss = 0.69014619\n",
      "Iteration 5418, loss = 0.69014581\n",
      "Iteration 5419, loss = 0.69014558\n",
      "Iteration 5420, loss = 0.69014580\n",
      "Iteration 5421, loss = 0.69014530\n",
      "Iteration 5422, loss = 0.69014472\n",
      "Iteration 5423, loss = 0.69014454\n",
      "Iteration 5424, loss = 0.69014419\n",
      "Iteration 5425, loss = 0.69014392\n",
      "Iteration 5426, loss = 0.69014352\n",
      "Iteration 5427, loss = 0.69014315\n",
      "Iteration 5428, loss = 0.69014281\n",
      "Iteration 5429, loss = 0.69014264\n",
      "Iteration 5430, loss = 0.69014217\n",
      "Iteration 5431, loss = 0.69014236\n",
      "Iteration 5432, loss = 0.69014166\n",
      "Iteration 5433, loss = 0.69014173\n",
      "Iteration 5434, loss = 0.69014114\n",
      "Iteration 5435, loss = 0.69014077\n",
      "Iteration 5436, loss = 0.69014045\n",
      "Iteration 5437, loss = 0.69014032\n",
      "Iteration 5438, loss = 0.69014016\n",
      "Iteration 5439, loss = 0.69014032\n",
      "Iteration 5440, loss = 0.69013937\n",
      "Iteration 5441, loss = 0.69013913\n",
      "Iteration 5442, loss = 0.69013903\n",
      "Iteration 5443, loss = 0.69013885\n",
      "Iteration 5444, loss = 0.69013864\n",
      "Iteration 5445, loss = 0.69013799\n",
      "Iteration 5446, loss = 0.69013767\n",
      "Iteration 5447, loss = 0.69013792\n",
      "Iteration 5448, loss = 0.69013737\n",
      "Iteration 5449, loss = 0.69013752\n",
      "Iteration 5450, loss = 0.69013658\n",
      "Iteration 5451, loss = 0.69013650\n",
      "Iteration 5452, loss = 0.69013579\n",
      "Iteration 5453, loss = 0.69013646\n",
      "Iteration 5454, loss = 0.69013536\n",
      "Iteration 5455, loss = 0.69013592\n",
      "Iteration 5456, loss = 0.69013465\n",
      "Iteration 5457, loss = 0.69013502\n",
      "Iteration 5458, loss = 0.69013427\n",
      "Iteration 5459, loss = 0.69013410\n",
      "Iteration 5460, loss = 0.69013368\n",
      "Iteration 5461, loss = 0.69013374\n",
      "Iteration 5462, loss = 0.69013324\n",
      "Iteration 5463, loss = 0.69013319\n",
      "Iteration 5464, loss = 0.69013254\n",
      "Iteration 5465, loss = 0.69013221\n",
      "Iteration 5466, loss = 0.69013163\n",
      "Iteration 5467, loss = 0.69013124\n",
      "Iteration 5468, loss = 0.69013107\n",
      "Iteration 5469, loss = 0.69013117\n",
      "Iteration 5470, loss = 0.69013056\n",
      "Iteration 5471, loss = 0.69013012\n",
      "Iteration 5472, loss = 0.69012974\n",
      "Iteration 5473, loss = 0.69012993\n",
      "Iteration 5474, loss = 0.69012919\n",
      "Iteration 5475, loss = 0.69012889\n",
      "Iteration 5476, loss = 0.69012864\n",
      "Iteration 5477, loss = 0.69012839\n",
      "Iteration 5478, loss = 0.69012837\n",
      "Iteration 5479, loss = 0.69012780\n",
      "Iteration 5480, loss = 0.69012734\n",
      "Iteration 5481, loss = 0.69012707\n",
      "Iteration 5482, loss = 0.69012650\n",
      "Iteration 5483, loss = 0.69012617\n",
      "Iteration 5484, loss = 0.69012583\n",
      "Iteration 5485, loss = 0.69012576\n",
      "Iteration 5486, loss = 0.69012558\n",
      "Iteration 5487, loss = 0.69012528\n",
      "Iteration 5488, loss = 0.69012454\n",
      "Iteration 5489, loss = 0.69012463\n",
      "Iteration 5490, loss = 0.69012440\n",
      "Iteration 5491, loss = 0.69012387\n",
      "Iteration 5492, loss = 0.69012384\n",
      "Iteration 5493, loss = 0.69012329\n",
      "Iteration 5494, loss = 0.69012310\n",
      "Iteration 5495, loss = 0.69012299\n",
      "Iteration 5496, loss = 0.69012224\n",
      "Iteration 5497, loss = 0.69012267\n",
      "Iteration 5498, loss = 0.69012147\n",
      "Iteration 5499, loss = 0.69012189\n",
      "Iteration 5500, loss = 0.69012107\n",
      "Iteration 5501, loss = 0.69012085\n",
      "Iteration 5502, loss = 0.69012065\n",
      "Iteration 5503, loss = 0.69012038\n",
      "Iteration 5504, loss = 0.69012011\n",
      "Iteration 5505, loss = 0.69011961\n",
      "Iteration 5506, loss = 0.69011943\n",
      "Iteration 5507, loss = 0.69011888\n",
      "Iteration 5508, loss = 0.69011903\n",
      "Iteration 5509, loss = 0.69011891\n",
      "Iteration 5510, loss = 0.69011821\n",
      "Iteration 5511, loss = 0.69011784\n",
      "Iteration 5512, loss = 0.69011761\n",
      "Iteration 5513, loss = 0.69011751\n",
      "Iteration 5514, loss = 0.69011757\n",
      "Iteration 5515, loss = 0.69011723\n",
      "Iteration 5516, loss = 0.69011670\n",
      "Iteration 5517, loss = 0.69011606\n",
      "Iteration 5518, loss = 0.69011634\n",
      "Iteration 5519, loss = 0.69011623\n",
      "Iteration 5520, loss = 0.69011530\n",
      "Iteration 5521, loss = 0.69011508\n",
      "Iteration 5522, loss = 0.69011495\n",
      "Iteration 5523, loss = 0.69011438\n",
      "Iteration 5524, loss = 0.69011424\n",
      "Iteration 5525, loss = 0.69011399\n",
      "Iteration 5526, loss = 0.69011354\n",
      "Iteration 5527, loss = 0.69011312\n",
      "Iteration 5528, loss = 0.69011302\n",
      "Iteration 5529, loss = 0.69011250\n",
      "Iteration 5530, loss = 0.69011228\n",
      "Iteration 5531, loss = 0.69011244\n",
      "Iteration 5532, loss = 0.69011187\n",
      "Iteration 5533, loss = 0.69011125\n",
      "Iteration 5534, loss = 0.69011099\n",
      "Iteration 5535, loss = 0.69011037\n",
      "Iteration 5536, loss = 0.69011037\n",
      "Iteration 5537, loss = 0.69011025\n",
      "Iteration 5538, loss = 0.69010968\n",
      "Iteration 5539, loss = 0.69010968\n",
      "Iteration 5540, loss = 0.69010956\n",
      "Iteration 5541, loss = 0.69010884\n",
      "Iteration 5542, loss = 0.69010930\n",
      "Iteration 5543, loss = 0.69010835\n",
      "Iteration 5544, loss = 0.69010765\n",
      "Iteration 5545, loss = 0.69010777\n",
      "Iteration 5546, loss = 0.69010706\n",
      "Iteration 5547, loss = 0.69010742\n",
      "Iteration 5548, loss = 0.69010689\n",
      "Iteration 5549, loss = 0.69010631\n",
      "Iteration 5550, loss = 0.69010657\n",
      "Iteration 5551, loss = 0.69010580\n",
      "Iteration 5552, loss = 0.69010596\n",
      "Iteration 5553, loss = 0.69010603\n",
      "Iteration 5554, loss = 0.69010542\n",
      "Iteration 5555, loss = 0.69010455\n",
      "Iteration 5556, loss = 0.69010504\n",
      "Iteration 5557, loss = 0.69010392\n",
      "Iteration 5558, loss = 0.69010351\n",
      "Iteration 5559, loss = 0.69010372\n",
      "Iteration 5560, loss = 0.69010310\n",
      "Iteration 5561, loss = 0.69010356\n",
      "Iteration 5562, loss = 0.69010267\n",
      "Iteration 5563, loss = 0.69010210\n",
      "Iteration 5564, loss = 0.69010175\n",
      "Iteration 5565, loss = 0.69010172\n",
      "Iteration 5566, loss = 0.69010130\n",
      "Iteration 5567, loss = 0.69010108\n",
      "Iteration 5568, loss = 0.69010073\n",
      "Iteration 5569, loss = 0.69010079\n",
      "Iteration 5570, loss = 0.69010024\n",
      "Iteration 5571, loss = 0.69010006\n",
      "Iteration 5572, loss = 0.69010021\n",
      "Iteration 5573, loss = 0.69009943\n",
      "Iteration 5574, loss = 0.69009931\n",
      "Iteration 5575, loss = 0.69009905\n",
      "Iteration 5576, loss = 0.69009838\n",
      "Iteration 5577, loss = 0.69009807\n",
      "Iteration 5578, loss = 0.69009795\n",
      "Iteration 5579, loss = 0.69009767\n",
      "Iteration 5580, loss = 0.69009744\n",
      "Iteration 5581, loss = 0.69009705\n",
      "Iteration 5582, loss = 0.69009688\n",
      "Iteration 5583, loss = 0.69009651\n",
      "Iteration 5584, loss = 0.69009602\n",
      "Iteration 5585, loss = 0.69009578\n",
      "Iteration 5586, loss = 0.69009573\n",
      "Iteration 5587, loss = 0.69009521\n",
      "Iteration 5588, loss = 0.69009481\n",
      "Iteration 5589, loss = 0.69009489\n",
      "Iteration 5590, loss = 0.69009433\n",
      "Iteration 5591, loss = 0.69009437\n",
      "Iteration 5592, loss = 0.69009398\n",
      "Iteration 5593, loss = 0.69009353\n",
      "Iteration 5594, loss = 0.69009285\n",
      "Iteration 5595, loss = 0.69009284\n",
      "Iteration 5596, loss = 0.69009331\n",
      "Iteration 5597, loss = 0.69009284\n",
      "Iteration 5598, loss = 0.69009232\n",
      "Iteration 5599, loss = 0.69009186\n",
      "Iteration 5600, loss = 0.69009140\n",
      "Iteration 5601, loss = 0.69009123\n",
      "Iteration 5602, loss = 0.69009110\n",
      "Iteration 5603, loss = 0.69009068\n",
      "Iteration 5604, loss = 0.69009094\n",
      "Iteration 5605, loss = 0.69009013\n",
      "Iteration 5606, loss = 0.69008956\n",
      "Iteration 5607, loss = 0.69008935\n",
      "Iteration 5608, loss = 0.69008919\n",
      "Iteration 5609, loss = 0.69008891\n",
      "Iteration 5610, loss = 0.69008842\n",
      "Iteration 5611, loss = 0.69008841\n",
      "Iteration 5612, loss = 0.69008797\n",
      "Iteration 5613, loss = 0.69008806\n",
      "Iteration 5614, loss = 0.69008768\n",
      "Iteration 5615, loss = 0.69008775\n",
      "Iteration 5616, loss = 0.69008677\n",
      "Iteration 5617, loss = 0.69008649\n",
      "Iteration 5618, loss = 0.69008612\n",
      "Iteration 5619, loss = 0.69008647\n",
      "Iteration 5620, loss = 0.69008585\n",
      "Iteration 5621, loss = 0.69008544\n",
      "Iteration 5622, loss = 0.69008511\n",
      "Iteration 5623, loss = 0.69008469\n",
      "Iteration 5624, loss = 0.69008455\n",
      "Iteration 5625, loss = 0.69008447\n",
      "Iteration 5626, loss = 0.69008414\n",
      "Iteration 5627, loss = 0.69008393\n",
      "Iteration 5628, loss = 0.69008320\n",
      "Iteration 5629, loss = 0.69008312\n",
      "Iteration 5630, loss = 0.69008288\n",
      "Iteration 5631, loss = 0.69008258\n",
      "Iteration 5632, loss = 0.69008305\n",
      "Iteration 5633, loss = 0.69008206\n",
      "Iteration 5634, loss = 0.69008214\n",
      "Iteration 5635, loss = 0.69008098\n",
      "Iteration 5636, loss = 0.69008118\n",
      "Iteration 5637, loss = 0.69008120\n",
      "Iteration 5638, loss = 0.69008089\n",
      "Iteration 5639, loss = 0.69008040\n",
      "Iteration 5640, loss = 0.69008009\n",
      "Iteration 5641, loss = 0.69008008\n",
      "Iteration 5642, loss = 0.69007973\n",
      "Iteration 5643, loss = 0.69007987\n",
      "Iteration 5644, loss = 0.69007901\n",
      "Iteration 5645, loss = 0.69007871\n",
      "Iteration 5646, loss = 0.69007914\n",
      "Iteration 5647, loss = 0.69007792\n",
      "Iteration 5648, loss = 0.69007778\n",
      "Iteration 5649, loss = 0.69007797\n",
      "Iteration 5650, loss = 0.69007744\n",
      "Iteration 5651, loss = 0.69007679\n",
      "Iteration 5652, loss = 0.69007672\n",
      "Iteration 5653, loss = 0.69007713\n",
      "Iteration 5654, loss = 0.69007604\n",
      "Iteration 5655, loss = 0.69007579\n",
      "Iteration 5656, loss = 0.69007541\n",
      "Iteration 5657, loss = 0.69007576\n",
      "Iteration 5658, loss = 0.69007521\n",
      "Iteration 5659, loss = 0.69007457\n",
      "Iteration 5660, loss = 0.69007481\n",
      "Iteration 5661, loss = 0.69007444\n",
      "Iteration 5662, loss = 0.69007396\n",
      "Iteration 5663, loss = 0.69007386\n",
      "Iteration 5664, loss = 0.69007341\n",
      "Iteration 5665, loss = 0.69007295\n",
      "Iteration 5666, loss = 0.69007291\n",
      "Iteration 5667, loss = 0.69007281\n",
      "Iteration 5668, loss = 0.69007199\n",
      "Iteration 5669, loss = 0.69007239\n",
      "Iteration 5670, loss = 0.69007189\n",
      "Iteration 5671, loss = 0.69007171\n",
      "Iteration 5672, loss = 0.69007151\n",
      "Iteration 5673, loss = 0.69007100\n",
      "Iteration 5674, loss = 0.69007064\n",
      "Iteration 5675, loss = 0.69007086\n",
      "Iteration 5676, loss = 0.69007004\n",
      "Iteration 5677, loss = 0.69006999\n",
      "Iteration 5678, loss = 0.69006965\n",
      "Iteration 5679, loss = 0.69006949\n",
      "Iteration 5680, loss = 0.69006922\n",
      "Iteration 5681, loss = 0.69006918\n",
      "Iteration 5682, loss = 0.69006919\n",
      "Iteration 5683, loss = 0.69006812\n",
      "Iteration 5684, loss = 0.69006767\n",
      "Iteration 5685, loss = 0.69006757\n",
      "Iteration 5686, loss = 0.69006731\n",
      "Iteration 5687, loss = 0.69006692\n",
      "Iteration 5688, loss = 0.69006678\n",
      "Iteration 5689, loss = 0.69006674\n",
      "Iteration 5690, loss = 0.69006678\n",
      "Iteration 5691, loss = 0.69006574\n",
      "Iteration 5692, loss = 0.69006573\n",
      "Iteration 5693, loss = 0.69006540\n",
      "Iteration 5694, loss = 0.69006541\n",
      "Iteration 5695, loss = 0.69006475\n",
      "Iteration 5696, loss = 0.69006483\n",
      "Iteration 5697, loss = 0.69006431\n",
      "Iteration 5698, loss = 0.69006364\n",
      "Iteration 5699, loss = 0.69006350\n",
      "Iteration 5700, loss = 0.69006318\n",
      "Iteration 5701, loss = 0.69006273\n",
      "Iteration 5702, loss = 0.69006254\n",
      "Iteration 5703, loss = 0.69006237\n",
      "Iteration 5704, loss = 0.69006233\n",
      "Iteration 5705, loss = 0.69006249\n",
      "Iteration 5706, loss = 0.69006169\n",
      "Iteration 5707, loss = 0.69006130\n",
      "Iteration 5708, loss = 0.69006077\n",
      "Iteration 5709, loss = 0.69006058\n",
      "Iteration 5710, loss = 0.69006041\n",
      "Iteration 5711, loss = 0.69006035\n",
      "Iteration 5712, loss = 0.69005948\n",
      "Iteration 5713, loss = 0.69005946\n",
      "Iteration 5714, loss = 0.69005920\n",
      "Iteration 5715, loss = 0.69005927\n",
      "Iteration 5716, loss = 0.69005857\n",
      "Iteration 5717, loss = 0.69005870\n",
      "Iteration 5718, loss = 0.69005841\n",
      "Iteration 5719, loss = 0.69005750\n",
      "Iteration 5720, loss = 0.69005759\n",
      "Iteration 5721, loss = 0.69005707\n",
      "Iteration 5722, loss = 0.69005702\n",
      "Iteration 5723, loss = 0.69005661\n",
      "Iteration 5724, loss = 0.69005627\n",
      "Iteration 5725, loss = 0.69005645\n",
      "Iteration 5726, loss = 0.69005584\n",
      "Iteration 5727, loss = 0.69005551\n",
      "Iteration 5728, loss = 0.69005539\n",
      "Iteration 5729, loss = 0.69005538\n",
      "Iteration 5730, loss = 0.69005487\n",
      "Iteration 5731, loss = 0.69005476\n",
      "Iteration 5732, loss = 0.69005431\n",
      "Iteration 5733, loss = 0.69005482\n",
      "Iteration 5734, loss = 0.69005354\n",
      "Iteration 5735, loss = 0.69005352\n",
      "Iteration 5736, loss = 0.69005338\n",
      "Iteration 5737, loss = 0.69005297\n",
      "Iteration 5738, loss = 0.69005241\n",
      "Iteration 5739, loss = 0.69005229\n",
      "Iteration 5740, loss = 0.69005213\n",
      "Iteration 5741, loss = 0.69005155\n",
      "Iteration 5742, loss = 0.69005153\n",
      "Iteration 5743, loss = 0.69005171\n",
      "Iteration 5744, loss = 0.69005132\n",
      "Iteration 5745, loss = 0.69005053\n",
      "Iteration 5746, loss = 0.69005038\n",
      "Iteration 5747, loss = 0.69005070\n",
      "Iteration 5748, loss = 0.69004989\n",
      "Iteration 5749, loss = 0.69004972\n",
      "Iteration 5750, loss = 0.69004921\n",
      "Iteration 5751, loss = 0.69004944\n",
      "Iteration 5752, loss = 0.69004962\n",
      "Iteration 5753, loss = 0.69004848\n",
      "Iteration 5754, loss = 0.69004828\n",
      "Iteration 5755, loss = 0.69004808\n",
      "Iteration 5756, loss = 0.69004761\n",
      "Iteration 5757, loss = 0.69004724\n",
      "Iteration 5758, loss = 0.69004699\n",
      "Iteration 5759, loss = 0.69004681\n",
      "Iteration 5760, loss = 0.69004654\n",
      "Iteration 5761, loss = 0.69004654\n",
      "Iteration 5762, loss = 0.69004608\n",
      "Iteration 5763, loss = 0.69004630\n",
      "Iteration 5764, loss = 0.69004584\n",
      "Iteration 5765, loss = 0.69004549\n",
      "Iteration 5766, loss = 0.69004501\n",
      "Iteration 5767, loss = 0.69004454\n",
      "Iteration 5768, loss = 0.69004429\n",
      "Iteration 5769, loss = 0.69004424\n",
      "Iteration 5770, loss = 0.69004412\n",
      "Iteration 5771, loss = 0.69004374\n",
      "Iteration 5772, loss = 0.69004328\n",
      "Iteration 5773, loss = 0.69004292\n",
      "Iteration 5774, loss = 0.69004269\n",
      "Iteration 5775, loss = 0.69004258\n",
      "Iteration 5776, loss = 0.69004200\n",
      "Iteration 5777, loss = 0.69004170\n",
      "Iteration 5778, loss = 0.69004181\n",
      "Iteration 5779, loss = 0.69004154\n",
      "Iteration 5780, loss = 0.69004130\n",
      "Iteration 5781, loss = 0.69004082\n",
      "Iteration 5782, loss = 0.69004078\n",
      "Iteration 5783, loss = 0.69004023\n",
      "Iteration 5784, loss = 0.69003976\n",
      "Iteration 5785, loss = 0.69003994\n",
      "Iteration 5786, loss = 0.69003930\n",
      "Iteration 5787, loss = 0.69003933\n",
      "Iteration 5788, loss = 0.69003927\n",
      "Iteration 5789, loss = 0.69003852\n",
      "Iteration 5790, loss = 0.69003889\n",
      "Iteration 5791, loss = 0.69003799\n",
      "Iteration 5792, loss = 0.69003780\n",
      "Iteration 5793, loss = 0.69003769\n",
      "Iteration 5794, loss = 0.69003819\n",
      "Iteration 5795, loss = 0.69003701\n",
      "Iteration 5796, loss = 0.69003653\n",
      "Iteration 5797, loss = 0.69003669\n",
      "Iteration 5798, loss = 0.69003701\n",
      "Iteration 5799, loss = 0.69003627\n",
      "Iteration 5800, loss = 0.69003585\n",
      "Iteration 5801, loss = 0.69003776\n",
      "Iteration 5802, loss = 0.69003567\n",
      "Iteration 5803, loss = 0.69003487\n",
      "Iteration 5804, loss = 0.69003482\n",
      "Iteration 5805, loss = 0.69003451\n",
      "Iteration 5806, loss = 0.69003445\n",
      "Iteration 5807, loss = 0.69003477\n",
      "Iteration 5808, loss = 0.69003371\n",
      "Iteration 5809, loss = 0.69003365\n",
      "Iteration 5810, loss = 0.69003301\n",
      "Iteration 5811, loss = 0.69003273\n",
      "Iteration 5812, loss = 0.69003292\n",
      "Iteration 5813, loss = 0.69003235\n",
      "Iteration 5814, loss = 0.69003205\n",
      "Iteration 5815, loss = 0.69003164\n",
      "Iteration 5816, loss = 0.69003185\n",
      "Iteration 5817, loss = 0.69003126\n",
      "Iteration 5818, loss = 0.69003081\n",
      "Iteration 5819, loss = 0.69003069\n",
      "Iteration 5820, loss = 0.69003075\n",
      "Iteration 5821, loss = 0.69003028\n",
      "Iteration 5822, loss = 0.69002993\n",
      "Iteration 5823, loss = 0.69002962\n",
      "Iteration 5824, loss = 0.69002958\n",
      "Iteration 5825, loss = 0.69002881\n",
      "Iteration 5826, loss = 0.69002873\n",
      "Iteration 5827, loss = 0.69002848\n",
      "Iteration 5828, loss = 0.69002822\n",
      "Iteration 5829, loss = 0.69002804\n",
      "Iteration 5830, loss = 0.69002785\n",
      "Iteration 5831, loss = 0.69002770\n",
      "Iteration 5832, loss = 0.69002726\n",
      "Iteration 5833, loss = 0.69002692\n",
      "Iteration 5834, loss = 0.69002642\n",
      "Iteration 5835, loss = 0.69002676\n",
      "Iteration 5836, loss = 0.69002633\n",
      "Iteration 5837, loss = 0.69002559\n",
      "Iteration 5838, loss = 0.69002548\n",
      "Iteration 5839, loss = 0.69002535\n",
      "Iteration 5840, loss = 0.69002527\n",
      "Iteration 5841, loss = 0.69002529\n",
      "Iteration 5842, loss = 0.69002472\n",
      "Iteration 5843, loss = 0.69002421\n",
      "Iteration 5844, loss = 0.69002396\n",
      "Iteration 5845, loss = 0.69002399\n",
      "Iteration 5846, loss = 0.69002346\n",
      "Iteration 5847, loss = 0.69002368\n",
      "Iteration 5848, loss = 0.69002287\n",
      "Iteration 5849, loss = 0.69002344\n",
      "Iteration 5850, loss = 0.69002282\n",
      "Iteration 5851, loss = 0.69002280\n",
      "Iteration 5852, loss = 0.69002179\n",
      "Iteration 5853, loss = 0.69002153\n",
      "Iteration 5854, loss = 0.69002162\n",
      "Iteration 5855, loss = 0.69002099\n",
      "Iteration 5856, loss = 0.69002120\n",
      "Iteration 5857, loss = 0.69002064\n",
      "Iteration 5858, loss = 0.69002011\n",
      "Iteration 5859, loss = 0.69002002\n",
      "Iteration 5860, loss = 0.69001990\n",
      "Iteration 5861, loss = 0.69002028\n",
      "Iteration 5862, loss = 0.69001911\n",
      "Iteration 5863, loss = 0.69001890\n",
      "Iteration 5864, loss = 0.69001899\n",
      "Iteration 5865, loss = 0.69001947\n",
      "Iteration 5866, loss = 0.69001847\n",
      "Iteration 5867, loss = 0.69001802\n",
      "Iteration 5868, loss = 0.69001778\n",
      "Iteration 5869, loss = 0.69001746\n",
      "Iteration 5870, loss = 0.69001724\n",
      "Iteration 5871, loss = 0.69001684\n",
      "Iteration 5872, loss = 0.69001678\n",
      "Iteration 5873, loss = 0.69001630\n",
      "Iteration 5874, loss = 0.69001673\n",
      "Iteration 5875, loss = 0.69001572\n",
      "Iteration 5876, loss = 0.69001554\n",
      "Iteration 5877, loss = 0.69001556\n",
      "Iteration 5878, loss = 0.69001500\n",
      "Iteration 5879, loss = 0.69001463\n",
      "Iteration 5880, loss = 0.69001430\n",
      "Iteration 5881, loss = 0.69001430\n",
      "Iteration 5882, loss = 0.69001403\n",
      "Iteration 5883, loss = 0.69001370\n",
      "Iteration 5884, loss = 0.69001355\n",
      "Iteration 5885, loss = 0.69001369\n",
      "Iteration 5886, loss = 0.69001267\n",
      "Iteration 5887, loss = 0.69001243\n",
      "Iteration 5888, loss = 0.69001234\n",
      "Iteration 5889, loss = 0.69001242\n",
      "Iteration 5890, loss = 0.69001177\n",
      "Iteration 5891, loss = 0.69001150\n",
      "Iteration 5892, loss = 0.69001138\n",
      "Iteration 5893, loss = 0.69001102\n",
      "Iteration 5894, loss = 0.69001085\n",
      "Iteration 5895, loss = 0.69001140\n",
      "Iteration 5896, loss = 0.69000996\n",
      "Iteration 5897, loss = 0.69001122\n",
      "Iteration 5898, loss = 0.69000993\n",
      "Iteration 5899, loss = 0.69000935\n",
      "Iteration 5900, loss = 0.69000891\n",
      "Iteration 5901, loss = 0.69000848\n",
      "Iteration 5902, loss = 0.69000821\n",
      "Iteration 5903, loss = 0.69000871\n",
      "Iteration 5904, loss = 0.69000795\n",
      "Iteration 5905, loss = 0.69000753\n",
      "Iteration 5906, loss = 0.69000780\n",
      "Iteration 5907, loss = 0.69000702\n",
      "Iteration 5908, loss = 0.69000732\n",
      "Iteration 5909, loss = 0.69000658\n",
      "Iteration 5910, loss = 0.69000651\n",
      "Iteration 5911, loss = 0.69000581\n",
      "Iteration 5912, loss = 0.69000541\n",
      "Iteration 5913, loss = 0.69000538\n",
      "Iteration 5914, loss = 0.69000502\n",
      "Iteration 5915, loss = 0.69000508\n",
      "Iteration 5916, loss = 0.69000455\n",
      "Iteration 5917, loss = 0.69000444\n",
      "Iteration 5918, loss = 0.69000400\n",
      "Iteration 5919, loss = 0.69000397\n",
      "Iteration 5920, loss = 0.69000349\n",
      "Iteration 5921, loss = 0.69000328\n",
      "Iteration 5922, loss = 0.69000303\n",
      "Iteration 5923, loss = 0.69000264\n",
      "Iteration 5924, loss = 0.69000241\n",
      "Iteration 5925, loss = 0.69000234\n",
      "Iteration 5926, loss = 0.69000209\n",
      "Iteration 5927, loss = 0.69000174\n",
      "Iteration 5928, loss = 0.69000137\n",
      "Iteration 5929, loss = 0.69000200\n",
      "Iteration 5930, loss = 0.69000095\n",
      "Iteration 5931, loss = 0.69000068\n",
      "Iteration 5932, loss = 0.69000024\n",
      "Iteration 5933, loss = 0.69000085\n",
      "Iteration 5934, loss = 0.68999988\n",
      "Iteration 5935, loss = 0.69000021\n",
      "Iteration 5936, loss = 0.68999938\n",
      "Iteration 5937, loss = 0.68999893\n",
      "Iteration 5938, loss = 0.68999930\n",
      "Iteration 5939, loss = 0.68999853\n",
      "Iteration 5940, loss = 0.68999831\n",
      "Iteration 5941, loss = 0.68999812\n",
      "Iteration 5942, loss = 0.68999769\n",
      "Iteration 5943, loss = 0.68999730\n",
      "Iteration 5944, loss = 0.68999726\n",
      "Iteration 5945, loss = 0.68999746\n",
      "Iteration 5946, loss = 0.68999679\n",
      "Iteration 5947, loss = 0.68999650\n",
      "Iteration 5948, loss = 0.68999610\n",
      "Iteration 5949, loss = 0.68999629\n",
      "Iteration 5950, loss = 0.68999583\n",
      "Iteration 5951, loss = 0.68999590\n",
      "Iteration 5952, loss = 0.68999544\n",
      "Iteration 5953, loss = 0.68999490\n",
      "Iteration 5954, loss = 0.68999461\n",
      "Iteration 5955, loss = 0.68999465\n",
      "Iteration 5956, loss = 0.68999416\n",
      "Iteration 5957, loss = 0.68999427\n",
      "Iteration 5958, loss = 0.68999375\n",
      "Iteration 5959, loss = 0.68999337\n",
      "Iteration 5960, loss = 0.68999318\n",
      "Iteration 5961, loss = 0.68999328\n",
      "Iteration 5962, loss = 0.68999305\n",
      "Iteration 5963, loss = 0.68999241\n",
      "Iteration 5964, loss = 0.68999246\n",
      "Iteration 5965, loss = 0.68999200\n",
      "Iteration 5966, loss = 0.68999194\n",
      "Iteration 5967, loss = 0.68999239\n",
      "Iteration 5968, loss = 0.68999152\n",
      "Iteration 5969, loss = 0.68999091\n",
      "Iteration 5970, loss = 0.68999061\n",
      "Iteration 5971, loss = 0.68999005\n",
      "Iteration 5972, loss = 0.68999027\n",
      "Iteration 5973, loss = 0.68999001\n",
      "Iteration 5974, loss = 0.68998952\n",
      "Iteration 5975, loss = 0.68998908\n",
      "Iteration 5976, loss = 0.68998920\n",
      "Iteration 5977, loss = 0.68998863\n",
      "Iteration 5978, loss = 0.68998894\n",
      "Iteration 5979, loss = 0.68998818\n",
      "Iteration 5980, loss = 0.68998828\n",
      "Iteration 5981, loss = 0.68998801\n",
      "Iteration 5982, loss = 0.68998765\n",
      "Iteration 5983, loss = 0.68998782\n",
      "Iteration 5984, loss = 0.68998717\n",
      "Iteration 5985, loss = 0.68998671\n",
      "Iteration 5986, loss = 0.68998693\n",
      "Iteration 5987, loss = 0.68998669\n",
      "Iteration 5988, loss = 0.68998610\n",
      "Iteration 5989, loss = 0.68998566\n",
      "Iteration 5990, loss = 0.68998600\n",
      "Iteration 5991, loss = 0.68998545\n",
      "Iteration 5992, loss = 0.68998501\n",
      "Iteration 5993, loss = 0.68998540\n",
      "Iteration 5994, loss = 0.68998518\n",
      "Iteration 5995, loss = 0.68998473\n",
      "Iteration 5996, loss = 0.68998372\n",
      "Iteration 5997, loss = 0.68998380\n",
      "Iteration 5998, loss = 0.68998369\n",
      "Iteration 5999, loss = 0.68998338\n",
      "Iteration 6000, loss = 0.68998322\n",
      "Iteration 6001, loss = 0.68998287\n",
      "Iteration 6002, loss = 0.68998229\n",
      "Iteration 6003, loss = 0.68998234\n",
      "Iteration 6004, loss = 0.68998221\n",
      "Iteration 6005, loss = 0.68998171\n",
      "Iteration 6006, loss = 0.68998158\n",
      "Iteration 6007, loss = 0.68998156\n",
      "Iteration 6008, loss = 0.68998103\n",
      "Iteration 6009, loss = 0.68998096\n",
      "Iteration 6010, loss = 0.68998079\n",
      "Iteration 6011, loss = 0.68998027\n",
      "Iteration 6012, loss = 0.68997981\n",
      "Iteration 6013, loss = 0.68997981\n",
      "Iteration 6014, loss = 0.68997941\n",
      "Iteration 6015, loss = 0.68997915\n",
      "Iteration 6016, loss = 0.68997882\n",
      "Iteration 6017, loss = 0.68997891\n",
      "Iteration 6018, loss = 0.68997814\n",
      "Iteration 6019, loss = 0.68997800\n",
      "Iteration 6020, loss = 0.68997808\n",
      "Iteration 6021, loss = 0.68997762\n",
      "Iteration 6022, loss = 0.68997736\n",
      "Iteration 6023, loss = 0.68997769\n",
      "Iteration 6024, loss = 0.68997722\n",
      "Iteration 6025, loss = 0.68997653\n",
      "Iteration 6026, loss = 0.68997632\n",
      "Iteration 6027, loss = 0.68997634\n",
      "Iteration 6028, loss = 0.68997591\n",
      "Iteration 6029, loss = 0.68997616\n",
      "Iteration 6030, loss = 0.68997584\n",
      "Iteration 6031, loss = 0.68997547\n",
      "Iteration 6032, loss = 0.68997483\n",
      "Iteration 6033, loss = 0.68997457\n",
      "Iteration 6034, loss = 0.68997432\n",
      "Iteration 6035, loss = 0.68997424\n",
      "Iteration 6036, loss = 0.68997424\n",
      "Iteration 6037, loss = 0.68997364\n",
      "Iteration 6038, loss = 0.68997317\n",
      "Iteration 6039, loss = 0.68997358\n",
      "Iteration 6040, loss = 0.68997270\n",
      "Iteration 6041, loss = 0.68997260\n",
      "Iteration 6042, loss = 0.68997282\n",
      "Iteration 6043, loss = 0.68997218\n",
      "Iteration 6044, loss = 0.68997182\n",
      "Iteration 6045, loss = 0.68997208\n",
      "Iteration 6046, loss = 0.68997187\n",
      "Iteration 6047, loss = 0.68997086\n",
      "Iteration 6048, loss = 0.68997093\n",
      "Iteration 6049, loss = 0.68997088\n",
      "Iteration 6050, loss = 0.68997038\n",
      "Iteration 6051, loss = 0.68997019\n",
      "Iteration 6052, loss = 0.68996984\n",
      "Iteration 6053, loss = 0.68996996\n",
      "Iteration 6054, loss = 0.68996966\n",
      "Iteration 6055, loss = 0.68996963\n",
      "Iteration 6056, loss = 0.68996911\n",
      "Iteration 6057, loss = 0.68996891\n",
      "Iteration 6058, loss = 0.68996869\n",
      "Iteration 6059, loss = 0.68996809\n",
      "Iteration 6060, loss = 0.68996791\n",
      "Iteration 6061, loss = 0.68996806\n",
      "Iteration 6062, loss = 0.68996764\n",
      "Iteration 6063, loss = 0.68996735\n",
      "Iteration 6064, loss = 0.68996691\n",
      "Iteration 6065, loss = 0.68996681\n",
      "Iteration 6066, loss = 0.68996674\n",
      "Iteration 6067, loss = 0.68996660\n",
      "Iteration 6068, loss = 0.68996584\n",
      "Iteration 6069, loss = 0.68996597\n",
      "Iteration 6070, loss = 0.68996602\n",
      "Iteration 6071, loss = 0.68996558\n",
      "Iteration 6072, loss = 0.68996487\n",
      "Iteration 6073, loss = 0.68996474\n",
      "Iteration 6074, loss = 0.68996458\n",
      "Iteration 6075, loss = 0.68996452\n",
      "Iteration 6076, loss = 0.68996410\n",
      "Iteration 6077, loss = 0.68996394\n",
      "Iteration 6078, loss = 0.68996367\n",
      "Iteration 6079, loss = 0.68996342\n",
      "Iteration 6080, loss = 0.68996315\n",
      "Iteration 6081, loss = 0.68996327\n",
      "Iteration 6082, loss = 0.68996285\n",
      "Iteration 6083, loss = 0.68996287\n",
      "Iteration 6084, loss = 0.68996200\n",
      "Iteration 6085, loss = 0.68996262\n",
      "Iteration 6086, loss = 0.68996186\n",
      "Iteration 6087, loss = 0.68996205\n",
      "Iteration 6088, loss = 0.68996135\n",
      "Iteration 6089, loss = 0.68996081\n",
      "Iteration 6090, loss = 0.68996048\n",
      "Iteration 6091, loss = 0.68996033\n",
      "Iteration 6092, loss = 0.68996055\n",
      "Iteration 6093, loss = 0.68995983\n",
      "Iteration 6094, loss = 0.68995983\n",
      "Iteration 6095, loss = 0.68995947\n",
      "Iteration 6096, loss = 0.68995969\n",
      "Iteration 6097, loss = 0.68995877\n",
      "Iteration 6098, loss = 0.68995873\n",
      "Iteration 6099, loss = 0.68995835\n",
      "Iteration 6100, loss = 0.68995799\n",
      "Iteration 6101, loss = 0.68995769\n",
      "Iteration 6102, loss = 0.68995742\n",
      "Iteration 6103, loss = 0.68995767\n",
      "Iteration 6104, loss = 0.68995725\n",
      "Iteration 6105, loss = 0.68995692\n",
      "Iteration 6106, loss = 0.68995679\n",
      "Iteration 6107, loss = 0.68995630\n",
      "Iteration 6108, loss = 0.68995619\n",
      "Iteration 6109, loss = 0.68995594\n",
      "Iteration 6110, loss = 0.68995554\n",
      "Iteration 6111, loss = 0.68995535\n",
      "Iteration 6112, loss = 0.68995566\n",
      "Iteration 6113, loss = 0.68995487\n",
      "Iteration 6114, loss = 0.68995483\n",
      "Iteration 6115, loss = 0.68995426\n",
      "Iteration 6116, loss = 0.68995448\n",
      "Iteration 6117, loss = 0.68995402\n",
      "Iteration 6118, loss = 0.68995369\n",
      "Iteration 6119, loss = 0.68995345\n",
      "Iteration 6120, loss = 0.68995315\n",
      "Iteration 6121, loss = 0.68995277\n",
      "Iteration 6122, loss = 0.68995307\n",
      "Iteration 6123, loss = 0.68995240\n",
      "Iteration 6124, loss = 0.68995300\n",
      "Iteration 6125, loss = 0.68995275\n",
      "Iteration 6126, loss = 0.68995148\n",
      "Iteration 6127, loss = 0.68995145\n",
      "Iteration 6128, loss = 0.68995126\n",
      "Iteration 6129, loss = 0.68995081\n",
      "Iteration 6130, loss = 0.68995061\n",
      "Iteration 6131, loss = 0.68995041\n",
      "Iteration 6132, loss = 0.68995001\n",
      "Iteration 6133, loss = 0.68995004\n",
      "Iteration 6134, loss = 0.68995007\n",
      "Iteration 6135, loss = 0.68994979\n",
      "Iteration 6136, loss = 0.68994941\n",
      "Iteration 6137, loss = 0.68994876\n",
      "Iteration 6138, loss = 0.68994886\n",
      "Iteration 6139, loss = 0.68994864\n",
      "Iteration 6140, loss = 0.68994836\n",
      "Iteration 6141, loss = 0.68994771\n",
      "Iteration 6142, loss = 0.68994805\n",
      "Iteration 6143, loss = 0.68994737\n",
      "Iteration 6144, loss = 0.68994740\n",
      "Iteration 6145, loss = 0.68994734\n",
      "Iteration 6146, loss = 0.68994677\n",
      "Iteration 6147, loss = 0.68994638\n",
      "Iteration 6148, loss = 0.68994612\n",
      "Iteration 6149, loss = 0.68994610\n",
      "Iteration 6150, loss = 0.68994607\n",
      "Iteration 6151, loss = 0.68994564\n",
      "Iteration 6152, loss = 0.68994529\n",
      "Iteration 6153, loss = 0.68994499\n",
      "Iteration 6154, loss = 0.68994480\n",
      "Iteration 6155, loss = 0.68994464\n",
      "Iteration 6156, loss = 0.68994478\n",
      "Iteration 6157, loss = 0.68994417\n",
      "Iteration 6158, loss = 0.68994389\n",
      "Iteration 6159, loss = 0.68994344\n",
      "Iteration 6160, loss = 0.68994320\n",
      "Iteration 6161, loss = 0.68994320\n",
      "Iteration 6162, loss = 0.68994285\n",
      "Iteration 6163, loss = 0.68994294\n",
      "Iteration 6164, loss = 0.68994247\n",
      "Iteration 6165, loss = 0.68994198\n",
      "Iteration 6166, loss = 0.68994184\n",
      "Iteration 6167, loss = 0.68994167\n",
      "Iteration 6168, loss = 0.68994113\n",
      "Iteration 6169, loss = 0.68994152\n",
      "Iteration 6170, loss = 0.68994086\n",
      "Iteration 6171, loss = 0.68994066\n",
      "Iteration 6172, loss = 0.68994054\n",
      "Iteration 6173, loss = 0.68994084\n",
      "Iteration 6174, loss = 0.68994017\n",
      "Iteration 6175, loss = 0.68993940\n",
      "Iteration 6176, loss = 0.68993971\n",
      "Iteration 6177, loss = 0.68993902\n",
      "Iteration 6178, loss = 0.68993868\n",
      "Iteration 6179, loss = 0.68993903\n",
      "Iteration 6180, loss = 0.68993836\n",
      "Iteration 6181, loss = 0.68993781\n",
      "Iteration 6182, loss = 0.68993800\n",
      "Iteration 6183, loss = 0.68993756\n",
      "Iteration 6184, loss = 0.68993748\n",
      "Iteration 6185, loss = 0.68993758\n",
      "Iteration 6186, loss = 0.68993707\n",
      "Iteration 6187, loss = 0.68993677\n",
      "Iteration 6188, loss = 0.68993645\n",
      "Iteration 6189, loss = 0.68993619\n",
      "Iteration 6190, loss = 0.68993570\n",
      "Iteration 6191, loss = 0.68993548\n",
      "Iteration 6192, loss = 0.68993549\n",
      "Iteration 6193, loss = 0.68993518\n",
      "Iteration 6194, loss = 0.68993489\n",
      "Iteration 6195, loss = 0.68993483\n",
      "Iteration 6196, loss = 0.68993492\n",
      "Iteration 6197, loss = 0.68993428\n",
      "Iteration 6198, loss = 0.68993426\n",
      "Iteration 6199, loss = 0.68993388\n",
      "Iteration 6200, loss = 0.68993362\n",
      "Iteration 6201, loss = 0.68993304\n",
      "Iteration 6202, loss = 0.68993304\n",
      "Iteration 6203, loss = 0.68993261\n",
      "Iteration 6204, loss = 0.68993234\n",
      "Iteration 6205, loss = 0.68993222\n",
      "Iteration 6206, loss = 0.68993211\n",
      "Iteration 6207, loss = 0.68993153\n",
      "Iteration 6208, loss = 0.68993175\n",
      "Iteration 6209, loss = 0.68993174\n",
      "Iteration 6210, loss = 0.68993147\n",
      "Iteration 6211, loss = 0.68993077\n",
      "Iteration 6212, loss = 0.68993072\n",
      "Iteration 6213, loss = 0.68993073\n",
      "Iteration 6214, loss = 0.68993020\n",
      "Iteration 6215, loss = 0.68992979\n",
      "Iteration 6216, loss = 0.68992963\n",
      "Iteration 6217, loss = 0.68992956\n",
      "Iteration 6218, loss = 0.68992900\n",
      "Iteration 6219, loss = 0.68992904\n",
      "Iteration 6220, loss = 0.68992885\n",
      "Iteration 6221, loss = 0.68992899\n",
      "Iteration 6222, loss = 0.68992786\n",
      "Iteration 6223, loss = 0.68992817\n",
      "Iteration 6224, loss = 0.68992761\n",
      "Iteration 6225, loss = 0.68992718\n",
      "Iteration 6226, loss = 0.68992718\n",
      "Iteration 6227, loss = 0.68992699\n",
      "Iteration 6228, loss = 0.68992674\n",
      "Iteration 6229, loss = 0.68992650\n",
      "Iteration 6230, loss = 0.68992656\n",
      "Iteration 6231, loss = 0.68992578\n",
      "Iteration 6232, loss = 0.68992549\n",
      "Iteration 6233, loss = 0.68992537\n",
      "Iteration 6234, loss = 0.68992579\n",
      "Iteration 6235, loss = 0.68992498\n",
      "Iteration 6236, loss = 0.68992489\n",
      "Iteration 6237, loss = 0.68992453\n",
      "Iteration 6238, loss = 0.68992425\n",
      "Iteration 6239, loss = 0.68992387\n",
      "Iteration 6240, loss = 0.68992372\n",
      "Iteration 6241, loss = 0.68992397\n",
      "Iteration 6242, loss = 0.68992312\n",
      "Iteration 6243, loss = 0.68992299\n",
      "Iteration 6244, loss = 0.68992282\n",
      "Iteration 6245, loss = 0.68992247\n",
      "Iteration 6246, loss = 0.68992225\n",
      "Iteration 6247, loss = 0.68992200\n",
      "Iteration 6248, loss = 0.68992175\n",
      "Iteration 6249, loss = 0.68992187\n",
      "Iteration 6250, loss = 0.68992115\n",
      "Iteration 6251, loss = 0.68992173\n",
      "Iteration 6252, loss = 0.68992127\n",
      "Iteration 6253, loss = 0.68992086\n",
      "Iteration 6254, loss = 0.68992076\n",
      "Iteration 6255, loss = 0.68992081\n",
      "Iteration 6256, loss = 0.68991967\n",
      "Iteration 6257, loss = 0.68991959\n",
      "Iteration 6258, loss = 0.68991964\n",
      "Iteration 6259, loss = 0.68991946\n",
      "Iteration 6260, loss = 0.68991896\n",
      "Iteration 6261, loss = 0.68991874\n",
      "Iteration 6262, loss = 0.68991851\n",
      "Iteration 6263, loss = 0.68991844\n",
      "Iteration 6264, loss = 0.68991801\n",
      "Iteration 6265, loss = 0.68991828\n",
      "Iteration 6266, loss = 0.68991737\n",
      "Iteration 6267, loss = 0.68991733\n",
      "Iteration 6268, loss = 0.68991773\n",
      "Iteration 6269, loss = 0.68991648\n",
      "Iteration 6270, loss = 0.68991660\n",
      "Iteration 6271, loss = 0.68991623\n",
      "Iteration 6272, loss = 0.68991604\n",
      "Iteration 6273, loss = 0.68991603\n",
      "Iteration 6274, loss = 0.68991588\n",
      "Iteration 6275, loss = 0.68991545\n",
      "Iteration 6276, loss = 0.68991505\n",
      "Iteration 6277, loss = 0.68991491\n",
      "Iteration 6278, loss = 0.68991447\n",
      "Iteration 6279, loss = 0.68991463\n",
      "Iteration 6280, loss = 0.68991400\n",
      "Iteration 6281, loss = 0.68991383\n",
      "Iteration 6282, loss = 0.68991372\n",
      "Iteration 6283, loss = 0.68991350\n",
      "Iteration 6284, loss = 0.68991378\n",
      "Iteration 6285, loss = 0.68991291\n",
      "Iteration 6286, loss = 0.68991327\n",
      "Iteration 6287, loss = 0.68991290\n",
      "Iteration 6288, loss = 0.68991235\n",
      "Iteration 6289, loss = 0.68991180\n",
      "Iteration 6290, loss = 0.68991163\n",
      "Iteration 6291, loss = 0.68991215\n",
      "Iteration 6292, loss = 0.68991228\n",
      "Iteration 6293, loss = 0.68991178\n",
      "Iteration 6294, loss = 0.68991109\n",
      "Iteration 6295, loss = 0.68991112\n",
      "Iteration 6296, loss = 0.68991110\n",
      "Iteration 6297, loss = 0.68991021\n",
      "Iteration 6298, loss = 0.68991007\n",
      "Iteration 6299, loss = 0.68991047\n",
      "Iteration 6300, loss = 0.68990959\n",
      "Iteration 6301, loss = 0.68990951\n",
      "Iteration 6302, loss = 0.68990910\n",
      "Iteration 6303, loss = 0.68990889\n",
      "Iteration 6304, loss = 0.68990872\n",
      "Iteration 6305, loss = 0.68990870\n",
      "Iteration 6306, loss = 0.68990892\n",
      "Iteration 6307, loss = 0.68990848\n",
      "Iteration 6308, loss = 0.68990811\n",
      "Iteration 6309, loss = 0.68990747\n",
      "Iteration 6310, loss = 0.68990753\n",
      "Iteration 6311, loss = 0.68990751\n",
      "Iteration 6312, loss = 0.68990652\n",
      "Iteration 6313, loss = 0.68990644\n",
      "Iteration 6314, loss = 0.68990644\n",
      "Iteration 6315, loss = 0.68990650\n",
      "Iteration 6316, loss = 0.68990586\n",
      "Iteration 6317, loss = 0.68990562\n",
      "Iteration 6318, loss = 0.68990585\n",
      "Iteration 6319, loss = 0.68990600\n",
      "Iteration 6320, loss = 0.68990493\n",
      "Iteration 6321, loss = 0.68990475\n",
      "Iteration 6322, loss = 0.68990498\n",
      "Iteration 6323, loss = 0.68990451\n",
      "Iteration 6324, loss = 0.68990412\n",
      "Iteration 6325, loss = 0.68990412\n",
      "Iteration 6326, loss = 0.68990362\n",
      "Iteration 6327, loss = 0.68990419\n",
      "Iteration 6328, loss = 0.68990303\n",
      "Iteration 6329, loss = 0.68990285\n",
      "Iteration 6330, loss = 0.68990262\n",
      "Iteration 6331, loss = 0.68990271\n",
      "Iteration 6332, loss = 0.68990244\n",
      "Iteration 6333, loss = 0.68990197\n",
      "Iteration 6334, loss = 0.68990175\n",
      "Iteration 6335, loss = 0.68990163\n",
      "Iteration 6336, loss = 0.68990122\n",
      "Iteration 6337, loss = 0.68990079\n",
      "Iteration 6338, loss = 0.68990117\n",
      "Iteration 6339, loss = 0.68990078\n",
      "Iteration 6340, loss = 0.68990047\n",
      "Iteration 6341, loss = 0.68990031\n",
      "Iteration 6342, loss = 0.68990028\n",
      "Iteration 6343, loss = 0.68989972\n",
      "Iteration 6344, loss = 0.68989954\n",
      "Iteration 6345, loss = 0.68989912\n",
      "Iteration 6346, loss = 0.68989884\n",
      "Iteration 6347, loss = 0.68989851\n",
      "Iteration 6348, loss = 0.68989876\n",
      "Iteration 6349, loss = 0.68989836\n",
      "Iteration 6350, loss = 0.68989805\n",
      "Iteration 6351, loss = 0.68989800\n",
      "Iteration 6352, loss = 0.68989769\n",
      "Iteration 6353, loss = 0.68989774\n",
      "Iteration 6354, loss = 0.68989720\n",
      "Iteration 6355, loss = 0.68989688\n",
      "Iteration 6356, loss = 0.68989737\n",
      "Iteration 6357, loss = 0.68989675\n",
      "Iteration 6358, loss = 0.68989595\n",
      "Iteration 6359, loss = 0.68989594\n",
      "Iteration 6360, loss = 0.68989571\n",
      "Iteration 6361, loss = 0.68989542\n",
      "Iteration 6362, loss = 0.68989530\n",
      "Iteration 6363, loss = 0.68989515\n",
      "Iteration 6364, loss = 0.68989505\n",
      "Iteration 6365, loss = 0.68989483\n",
      "Iteration 6366, loss = 0.68989483\n",
      "Iteration 6367, loss = 0.68989451\n",
      "Iteration 6368, loss = 0.68989395\n",
      "Iteration 6369, loss = 0.68989377\n",
      "Iteration 6370, loss = 0.68989431\n",
      "Iteration 6371, loss = 0.68989323\n",
      "Iteration 6372, loss = 0.68989324\n",
      "Iteration 6373, loss = 0.68989370\n",
      "Iteration 6374, loss = 0.68989301\n",
      "Iteration 6375, loss = 0.68989246\n",
      "Iteration 6376, loss = 0.68989234\n",
      "Iteration 6377, loss = 0.68989248\n",
      "Iteration 6378, loss = 0.68989178\n",
      "Iteration 6379, loss = 0.68989133\n",
      "Iteration 6380, loss = 0.68989142\n",
      "Iteration 6381, loss = 0.68989132\n",
      "Iteration 6382, loss = 0.68989062\n",
      "Iteration 6383, loss = 0.68989110\n",
      "Iteration 6384, loss = 0.68989050\n",
      "Iteration 6385, loss = 0.68988991\n",
      "Iteration 6386, loss = 0.68989017\n",
      "Iteration 6387, loss = 0.68988980\n",
      "Iteration 6388, loss = 0.68988970\n",
      "Iteration 6389, loss = 0.68988934\n",
      "Iteration 6390, loss = 0.68988883\n",
      "Iteration 6391, loss = 0.68988880\n",
      "Iteration 6392, loss = 0.68988876\n",
      "Iteration 6393, loss = 0.68988867\n",
      "Iteration 6394, loss = 0.68988824\n",
      "Iteration 6395, loss = 0.68988774\n",
      "Iteration 6396, loss = 0.68988738\n",
      "Iteration 6397, loss = 0.68988747\n",
      "Iteration 6398, loss = 0.68988709\n",
      "Iteration 6399, loss = 0.68988688\n",
      "Iteration 6400, loss = 0.68988650\n",
      "Iteration 6401, loss = 0.68988656\n",
      "Iteration 6402, loss = 0.68988644\n",
      "Iteration 6403, loss = 0.68988574\n",
      "Iteration 6404, loss = 0.68988648\n",
      "Iteration 6405, loss = 0.68988519\n",
      "Iteration 6406, loss = 0.68988530\n",
      "Iteration 6407, loss = 0.68988480\n",
      "Iteration 6408, loss = 0.68988493\n",
      "Iteration 6409, loss = 0.68988463\n",
      "Iteration 6410, loss = 0.68988501\n",
      "Iteration 6411, loss = 0.68988380\n",
      "Iteration 6412, loss = 0.68988398\n",
      "Iteration 6413, loss = 0.68988372\n",
      "Iteration 6414, loss = 0.68988368\n",
      "Iteration 6415, loss = 0.68988355\n",
      "Iteration 6416, loss = 0.68988343\n",
      "Iteration 6417, loss = 0.68988297\n",
      "Iteration 6418, loss = 0.68988257\n",
      "Iteration 6419, loss = 0.68988244\n",
      "Iteration 6420, loss = 0.68988222\n",
      "Iteration 6421, loss = 0.68988199\n",
      "Iteration 6422, loss = 0.68988217\n",
      "Iteration 6423, loss = 0.68988138\n",
      "Iteration 6424, loss = 0.68988133\n",
      "Iteration 6425, loss = 0.68988161\n",
      "Iteration 6426, loss = 0.68988079\n",
      "Iteration 6427, loss = 0.68988084\n",
      "Iteration 6428, loss = 0.68988032\n",
      "Iteration 6429, loss = 0.68988029\n",
      "Iteration 6430, loss = 0.68987972\n",
      "Iteration 6431, loss = 0.68988020\n",
      "Iteration 6432, loss = 0.68988043\n",
      "Iteration 6433, loss = 0.68987904\n",
      "Iteration 6434, loss = 0.68987915\n",
      "Iteration 6435, loss = 0.68987894\n",
      "Iteration 6436, loss = 0.68987878\n",
      "Iteration 6437, loss = 0.68987882\n",
      "Iteration 6438, loss = 0.68987824\n",
      "Iteration 6439, loss = 0.68987811\n",
      "Iteration 6440, loss = 0.68987819\n",
      "Iteration 6441, loss = 0.68987838\n",
      "Iteration 6442, loss = 0.68987750\n",
      "Iteration 6443, loss = 0.68987734\n",
      "Iteration 6444, loss = 0.68987669\n",
      "Iteration 6445, loss = 0.68987663\n",
      "Iteration 6446, loss = 0.68987706\n",
      "Iteration 6447, loss = 0.68987643\n",
      "Iteration 6448, loss = 0.68987621\n",
      "Iteration 6449, loss = 0.68987591\n",
      "Iteration 6450, loss = 0.68987560\n",
      "Iteration 6451, loss = 0.68987549\n",
      "Iteration 6452, loss = 0.68987532\n",
      "Iteration 6453, loss = 0.68987493\n",
      "Iteration 6454, loss = 0.68987496\n",
      "Iteration 6455, loss = 0.68987480\n",
      "Iteration 6456, loss = 0.68987459\n",
      "Iteration 6457, loss = 0.68987421\n",
      "Iteration 6458, loss = 0.68987404\n",
      "Iteration 6459, loss = 0.68987381\n",
      "Iteration 6460, loss = 0.68987378\n",
      "Iteration 6461, loss = 0.68987348\n",
      "Iteration 6462, loss = 0.68987348\n",
      "Iteration 6463, loss = 0.68987297\n",
      "Iteration 6464, loss = 0.68987266\n",
      "Iteration 6465, loss = 0.68987244\n",
      "Iteration 6466, loss = 0.68987263\n",
      "Iteration 6467, loss = 0.68987341\n",
      "Iteration 6468, loss = 0.68987198\n",
      "Iteration 6469, loss = 0.68987194\n",
      "Iteration 6470, loss = 0.68987128\n",
      "Iteration 6471, loss = 0.68987112\n",
      "Iteration 6472, loss = 0.68987086\n",
      "Iteration 6473, loss = 0.68987127\n",
      "Iteration 6474, loss = 0.68987046\n",
      "Iteration 6475, loss = 0.68987025\n",
      "Iteration 6476, loss = 0.68986987\n",
      "Iteration 6477, loss = 0.68986968\n",
      "Iteration 6478, loss = 0.68987001\n",
      "Iteration 6479, loss = 0.68987024\n",
      "Iteration 6480, loss = 0.68986966\n",
      "Iteration 6481, loss = 0.68986935\n",
      "Iteration 6482, loss = 0.68986883\n",
      "Iteration 6483, loss = 0.68986840\n",
      "Iteration 6484, loss = 0.68986832\n",
      "Iteration 6485, loss = 0.68986799\n",
      "Iteration 6486, loss = 0.68986790\n",
      "Iteration 6487, loss = 0.68986772\n",
      "Iteration 6488, loss = 0.68986738\n",
      "Iteration 6489, loss = 0.68986769\n",
      "Iteration 6490, loss = 0.68986769\n",
      "Iteration 6491, loss = 0.68986746\n",
      "Iteration 6492, loss = 0.68986647\n",
      "Iteration 6493, loss = 0.68986654\n",
      "Iteration 6494, loss = 0.68986636\n",
      "Iteration 6495, loss = 0.68986621\n",
      "Iteration 6496, loss = 0.68986576\n",
      "Iteration 6497, loss = 0.68986573\n",
      "Iteration 6498, loss = 0.68986570\n",
      "Iteration 6499, loss = 0.68986539\n",
      "Iteration 6500, loss = 0.68986497\n",
      "Iteration 6501, loss = 0.68986520\n",
      "Iteration 6502, loss = 0.68986456\n",
      "Iteration 6503, loss = 0.68986459\n",
      "Iteration 6504, loss = 0.68986435\n",
      "Iteration 6505, loss = 0.68986383\n",
      "Iteration 6506, loss = 0.68986336\n",
      "Iteration 6507, loss = 0.68986351\n",
      "Iteration 6508, loss = 0.68986348\n",
      "Iteration 6509, loss = 0.68986330\n",
      "Iteration 6510, loss = 0.68986263\n",
      "Iteration 6511, loss = 0.68986298\n",
      "Iteration 6512, loss = 0.68986223\n",
      "Iteration 6513, loss = 0.68986205\n",
      "Iteration 6514, loss = 0.68986225\n",
      "Iteration 6515, loss = 0.68986189\n",
      "Iteration 6516, loss = 0.68986149\n",
      "Iteration 6517, loss = 0.68986122\n",
      "Iteration 6518, loss = 0.68986120\n",
      "Iteration 6519, loss = 0.68986093\n",
      "Iteration 6520, loss = 0.68986059\n",
      "Iteration 6521, loss = 0.68986096\n",
      "Iteration 6522, loss = 0.68986017\n",
      "Iteration 6523, loss = 0.68986049\n",
      "Iteration 6524, loss = 0.68986012\n",
      "Iteration 6525, loss = 0.68985989\n",
      "Iteration 6526, loss = 0.68985994\n",
      "Iteration 6527, loss = 0.68986031\n",
      "Iteration 6528, loss = 0.68985912\n",
      "Iteration 6529, loss = 0.68985984\n",
      "Iteration 6530, loss = 0.68985895\n",
      "Iteration 6531, loss = 0.68985846\n",
      "Iteration 6532, loss = 0.68985816\n",
      "Iteration 6533, loss = 0.68985818\n",
      "Iteration 6534, loss = 0.68985791\n",
      "Iteration 6535, loss = 0.68985762\n",
      "Iteration 6536, loss = 0.68985736\n",
      "Iteration 6537, loss = 0.68985764\n",
      "Iteration 6538, loss = 0.68985693\n",
      "Iteration 6539, loss = 0.68985686\n",
      "Iteration 6540, loss = 0.68985649\n",
      "Iteration 6541, loss = 0.68985658\n",
      "Iteration 6542, loss = 0.68985597\n",
      "Iteration 6543, loss = 0.68985637\n",
      "Iteration 6544, loss = 0.68985614\n",
      "Iteration 6545, loss = 0.68985549\n",
      "Iteration 6546, loss = 0.68985558\n",
      "Iteration 6547, loss = 0.68985541\n",
      "Iteration 6548, loss = 0.68985471\n",
      "Iteration 6549, loss = 0.68985501\n",
      "Iteration 6550, loss = 0.68985533\n",
      "Iteration 6551, loss = 0.68985447\n",
      "Iteration 6552, loss = 0.68985436\n",
      "Iteration 6553, loss = 0.68985415\n",
      "Iteration 6554, loss = 0.68985350\n",
      "Iteration 6555, loss = 0.68985363\n",
      "Iteration 6556, loss = 0.68985317\n",
      "Iteration 6557, loss = 0.68985296\n",
      "Iteration 6558, loss = 0.68985286\n",
      "Iteration 6559, loss = 0.68985285\n",
      "Iteration 6560, loss = 0.68985228\n",
      "Iteration 6561, loss = 0.68985193\n",
      "Iteration 6562, loss = 0.68985191\n",
      "Iteration 6563, loss = 0.68985195\n",
      "Iteration 6564, loss = 0.68985165\n",
      "Iteration 6565, loss = 0.68985130\n",
      "Iteration 6566, loss = 0.68985096\n",
      "Iteration 6567, loss = 0.68985134\n",
      "Iteration 6568, loss = 0.68985052\n",
      "Iteration 6569, loss = 0.68985057\n",
      "Iteration 6570, loss = 0.68985028\n",
      "Iteration 6571, loss = 0.68984996\n",
      "Iteration 6572, loss = 0.68984987\n",
      "Iteration 6573, loss = 0.68984950\n",
      "Iteration 6574, loss = 0.68984935\n",
      "Iteration 6575, loss = 0.68984941\n",
      "Iteration 6576, loss = 0.68984899\n",
      "Iteration 6577, loss = 0.68984865\n",
      "Iteration 6578, loss = 0.68984848\n",
      "Iteration 6579, loss = 0.68984821\n",
      "Iteration 6580, loss = 0.68984800\n",
      "Iteration 6581, loss = 0.68984764\n",
      "Iteration 6582, loss = 0.68984746\n",
      "Iteration 6583, loss = 0.68984741\n",
      "Iteration 6584, loss = 0.68984689\n",
      "Iteration 6585, loss = 0.68984657\n",
      "Iteration 6586, loss = 0.68984717\n",
      "Iteration 6587, loss = 0.68984622\n",
      "Iteration 6588, loss = 0.68984653\n",
      "Iteration 6589, loss = 0.68984604\n",
      "Iteration 6590, loss = 0.68984641\n",
      "Iteration 6591, loss = 0.68984556\n",
      "Iteration 6592, loss = 0.68984580\n",
      "Iteration 6593, loss = 0.68984542\n",
      "Iteration 6594, loss = 0.68984508\n",
      "Iteration 6595, loss = 0.68984510\n",
      "Iteration 6596, loss = 0.68984455\n",
      "Iteration 6597, loss = 0.68984465\n",
      "Iteration 6598, loss = 0.68984408\n",
      "Iteration 6599, loss = 0.68984432\n",
      "Iteration 6600, loss = 0.68984377\n",
      "Iteration 6601, loss = 0.68984359\n",
      "Iteration 6602, loss = 0.68984369\n",
      "Iteration 6603, loss = 0.68984311\n",
      "Iteration 6604, loss = 0.68984294\n",
      "Iteration 6605, loss = 0.68984271\n",
      "Iteration 6606, loss = 0.68984247\n",
      "Iteration 6607, loss = 0.68984221\n",
      "Iteration 6608, loss = 0.68984183\n",
      "Iteration 6609, loss = 0.68984178\n",
      "Iteration 6610, loss = 0.68984161\n",
      "Iteration 6611, loss = 0.68984119\n",
      "Iteration 6612, loss = 0.68984110\n",
      "Iteration 6613, loss = 0.68984118\n",
      "Iteration 6614, loss = 0.68984058\n",
      "Iteration 6615, loss = 0.68984042\n",
      "Iteration 6616, loss = 0.68984037\n",
      "Iteration 6617, loss = 0.68984011\n",
      "Iteration 6618, loss = 0.68983998\n",
      "Iteration 6619, loss = 0.68983998\n",
      "Iteration 6620, loss = 0.68983934\n",
      "Iteration 6621, loss = 0.68983948\n",
      "Iteration 6622, loss = 0.68983905\n",
      "Iteration 6623, loss = 0.68983909\n",
      "Iteration 6624, loss = 0.68983927\n",
      "Iteration 6625, loss = 0.68983850\n",
      "Iteration 6626, loss = 0.68983845\n",
      "Iteration 6627, loss = 0.68983832\n",
      "Iteration 6628, loss = 0.68983794\n",
      "Iteration 6629, loss = 0.68983790\n",
      "Iteration 6630, loss = 0.68983730\n",
      "Iteration 6631, loss = 0.68983724\n",
      "Iteration 6632, loss = 0.68983672\n",
      "Iteration 6633, loss = 0.68983695\n",
      "Iteration 6634, loss = 0.68983758\n",
      "Iteration 6635, loss = 0.68983676\n",
      "Iteration 6636, loss = 0.68983629\n",
      "Iteration 6637, loss = 0.68983570\n",
      "Iteration 6638, loss = 0.68983570\n",
      "Iteration 6639, loss = 0.68983512\n",
      "Iteration 6640, loss = 0.68983523\n",
      "Iteration 6641, loss = 0.68983512\n",
      "Iteration 6642, loss = 0.68983500\n",
      "Iteration 6643, loss = 0.68983458\n",
      "Iteration 6644, loss = 0.68983424\n",
      "Iteration 6645, loss = 0.68983425\n",
      "Iteration 6646, loss = 0.68983381\n",
      "Iteration 6647, loss = 0.68983359\n",
      "Iteration 6648, loss = 0.68983320\n",
      "Iteration 6649, loss = 0.68983338\n",
      "Iteration 6650, loss = 0.68983360\n",
      "Iteration 6651, loss = 0.68983284\n",
      "Iteration 6652, loss = 0.68983289\n",
      "Iteration 6653, loss = 0.68983230\n",
      "Iteration 6654, loss = 0.68983245\n",
      "Iteration 6655, loss = 0.68983197\n",
      "Iteration 6656, loss = 0.68983199\n",
      "Iteration 6657, loss = 0.68983157\n",
      "Iteration 6658, loss = 0.68983122\n",
      "Iteration 6659, loss = 0.68983107\n",
      "Iteration 6660, loss = 0.68983114\n",
      "Iteration 6661, loss = 0.68983060\n",
      "Iteration 6662, loss = 0.68983033\n",
      "Iteration 6663, loss = 0.68983044\n",
      "Iteration 6664, loss = 0.68983079\n",
      "Iteration 6665, loss = 0.68982964\n",
      "Iteration 6666, loss = 0.68982947\n",
      "Iteration 6667, loss = 0.68982934\n",
      "Iteration 6668, loss = 0.68982898\n",
      "Iteration 6669, loss = 0.68982882\n",
      "Iteration 6670, loss = 0.68982919\n",
      "Iteration 6671, loss = 0.68982888\n",
      "Iteration 6672, loss = 0.68982826\n",
      "Iteration 6673, loss = 0.68982816\n",
      "Iteration 6674, loss = 0.68982798\n",
      "Iteration 6675, loss = 0.68982873\n",
      "Iteration 6676, loss = 0.68982745\n",
      "Iteration 6677, loss = 0.68982714\n",
      "Iteration 6678, loss = 0.68982713\n",
      "Iteration 6679, loss = 0.68982693\n",
      "Iteration 6680, loss = 0.68982649\n",
      "Iteration 6681, loss = 0.68982652\n",
      "Iteration 6682, loss = 0.68982634\n",
      "Iteration 6683, loss = 0.68982632\n",
      "Iteration 6684, loss = 0.68982577\n",
      "Iteration 6685, loss = 0.68982561\n",
      "Iteration 6686, loss = 0.68982530\n",
      "Iteration 6687, loss = 0.68982488\n",
      "Iteration 6688, loss = 0.68982509\n",
      "Iteration 6689, loss = 0.68982480\n",
      "Iteration 6690, loss = 0.68982458\n",
      "Iteration 6691, loss = 0.68982463\n",
      "Iteration 6692, loss = 0.68982428\n",
      "Iteration 6693, loss = 0.68982421\n",
      "Iteration 6694, loss = 0.68982390\n",
      "Iteration 6695, loss = 0.68982366\n",
      "Iteration 6696, loss = 0.68982351\n",
      "Iteration 6697, loss = 0.68982318\n",
      "Iteration 6698, loss = 0.68982295\n",
      "Iteration 6699, loss = 0.68982286\n",
      "Iteration 6700, loss = 0.68982303\n",
      "Iteration 6701, loss = 0.68982231\n",
      "Iteration 6702, loss = 0.68982223\n",
      "Iteration 6703, loss = 0.68982201\n",
      "Iteration 6704, loss = 0.68982184\n",
      "Iteration 6705, loss = 0.68982228\n",
      "Iteration 6706, loss = 0.68982150\n",
      "Iteration 6707, loss = 0.68982142\n",
      "Iteration 6708, loss = 0.68982146\n",
      "Iteration 6709, loss = 0.68982111\n",
      "Iteration 6710, loss = 0.68982051\n",
      "Iteration 6711, loss = 0.68982052\n",
      "Iteration 6712, loss = 0.68982022\n",
      "Iteration 6713, loss = 0.68981995\n",
      "Iteration 6714, loss = 0.68981972\n",
      "Iteration 6715, loss = 0.68981927\n",
      "Iteration 6716, loss = 0.68981912\n",
      "Iteration 6717, loss = 0.68981939\n",
      "Iteration 6718, loss = 0.68981887\n",
      "Iteration 6719, loss = 0.68981862\n",
      "Iteration 6720, loss = 0.68981873\n",
      "Iteration 6721, loss = 0.68981862\n",
      "Iteration 6722, loss = 0.68981827\n",
      "Iteration 6723, loss = 0.68981826\n",
      "Iteration 6724, loss = 0.68981734\n",
      "Iteration 6725, loss = 0.68981768\n",
      "Iteration 6726, loss = 0.68981714\n",
      "Iteration 6727, loss = 0.68981710\n",
      "Iteration 6728, loss = 0.68981748\n",
      "Iteration 6729, loss = 0.68981666\n",
      "Iteration 6730, loss = 0.68981670\n",
      "Iteration 6731, loss = 0.68981673\n",
      "Iteration 6732, loss = 0.68981580\n",
      "Iteration 6733, loss = 0.68981562\n",
      "Iteration 6734, loss = 0.68981569\n",
      "Iteration 6735, loss = 0.68981512\n",
      "Iteration 6736, loss = 0.68981495\n",
      "Iteration 6737, loss = 0.68981552\n",
      "Iteration 6738, loss = 0.68981543\n",
      "Iteration 6739, loss = 0.68981464\n",
      "Iteration 6740, loss = 0.68981435\n",
      "Iteration 6741, loss = 0.68981439\n",
      "Iteration 6742, loss = 0.68981435\n",
      "Iteration 6743, loss = 0.68981372\n",
      "Iteration 6744, loss = 0.68981394\n",
      "Iteration 6745, loss = 0.68981313\n",
      "Iteration 6746, loss = 0.68981287\n",
      "Iteration 6747, loss = 0.68981288\n",
      "Iteration 6748, loss = 0.68981258\n",
      "Iteration 6749, loss = 0.68981214\n",
      "Iteration 6750, loss = 0.68981212\n",
      "Iteration 6751, loss = 0.68981188\n",
      "Iteration 6752, loss = 0.68981163\n",
      "Iteration 6753, loss = 0.68981280\n",
      "Iteration 6754, loss = 0.68981143\n",
      "Iteration 6755, loss = 0.68981114\n",
      "Iteration 6756, loss = 0.68981083\n",
      "Iteration 6757, loss = 0.68981103\n",
      "Iteration 6758, loss = 0.68981041\n",
      "Iteration 6759, loss = 0.68981025\n",
      "Iteration 6760, loss = 0.68981047\n",
      "Iteration 6761, loss = 0.68980956\n",
      "Iteration 6762, loss = 0.68980962\n",
      "Iteration 6763, loss = 0.68980997\n",
      "Iteration 6764, loss = 0.68980931\n",
      "Iteration 6765, loss = 0.68980915\n",
      "Iteration 6766, loss = 0.68980877\n",
      "Iteration 6767, loss = 0.68980860\n",
      "Iteration 6768, loss = 0.68980825\n",
      "Iteration 6769, loss = 0.68980830\n",
      "Iteration 6770, loss = 0.68980824\n",
      "Iteration 6771, loss = 0.68980826\n",
      "Iteration 6772, loss = 0.68980901\n",
      "Iteration 6773, loss = 0.68980725\n",
      "Iteration 6774, loss = 0.68980721\n",
      "Iteration 6775, loss = 0.68980694\n",
      "Iteration 6776, loss = 0.68980661\n",
      "Iteration 6777, loss = 0.68980647\n",
      "Iteration 6778, loss = 0.68980665\n",
      "Iteration 6779, loss = 0.68980634\n",
      "Iteration 6780, loss = 0.68980606\n",
      "Iteration 6781, loss = 0.68980590\n",
      "Iteration 6782, loss = 0.68980610\n",
      "Iteration 6783, loss = 0.68980570\n",
      "Iteration 6784, loss = 0.68980520\n",
      "Iteration 6785, loss = 0.68980492\n",
      "Iteration 6786, loss = 0.68980453\n",
      "Iteration 6787, loss = 0.68980459\n",
      "Iteration 6788, loss = 0.68980430\n",
      "Iteration 6789, loss = 0.68980562\n",
      "Iteration 6790, loss = 0.68980392\n",
      "Iteration 6791, loss = 0.68980406\n",
      "Iteration 6792, loss = 0.68980341\n",
      "Iteration 6793, loss = 0.68980348\n",
      "Iteration 6794, loss = 0.68980296\n",
      "Iteration 6795, loss = 0.68980343\n",
      "Iteration 6796, loss = 0.68980258\n",
      "Iteration 6797, loss = 0.68980280\n",
      "Iteration 6798, loss = 0.68980204\n",
      "Iteration 6799, loss = 0.68980182\n",
      "Iteration 6800, loss = 0.68980170\n",
      "Iteration 6801, loss = 0.68980206\n",
      "Iteration 6802, loss = 0.68980174\n",
      "Iteration 6803, loss = 0.68980088\n",
      "Iteration 6804, loss = 0.68980094\n",
      "Iteration 6805, loss = 0.68980072\n",
      "Iteration 6806, loss = 0.68980046\n",
      "Iteration 6807, loss = 0.68980043\n",
      "Iteration 6808, loss = 0.68979991\n",
      "Iteration 6809, loss = 0.68979978\n",
      "Iteration 6810, loss = 0.68979972\n",
      "Iteration 6811, loss = 0.68980052\n",
      "Iteration 6812, loss = 0.68979922\n",
      "Iteration 6813, loss = 0.68979927\n",
      "Iteration 6814, loss = 0.68979886\n",
      "Iteration 6815, loss = 0.68979895\n",
      "Iteration 6816, loss = 0.68979835\n",
      "Iteration 6817, loss = 0.68979827\n",
      "Iteration 6818, loss = 0.68979827\n",
      "Iteration 6819, loss = 0.68979767\n",
      "Iteration 6820, loss = 0.68979733\n",
      "Iteration 6821, loss = 0.68979752\n",
      "Iteration 6822, loss = 0.68979731\n",
      "Iteration 6823, loss = 0.68979705\n",
      "Iteration 6824, loss = 0.68979691\n",
      "Iteration 6825, loss = 0.68979662\n",
      "Iteration 6826, loss = 0.68979646\n",
      "Iteration 6827, loss = 0.68979593\n",
      "Iteration 6828, loss = 0.68979631\n",
      "Iteration 6829, loss = 0.68979568\n",
      "Iteration 6830, loss = 0.68979558\n",
      "Iteration 6831, loss = 0.68979556\n",
      "Iteration 6832, loss = 0.68979525\n",
      "Iteration 6833, loss = 0.68979519\n",
      "Iteration 6834, loss = 0.68979507\n",
      "Iteration 6835, loss = 0.68979454\n",
      "Iteration 6836, loss = 0.68979424\n",
      "Iteration 6837, loss = 0.68979399\n",
      "Iteration 6838, loss = 0.68979399\n",
      "Iteration 6839, loss = 0.68979342\n",
      "Iteration 6840, loss = 0.68979337\n",
      "Iteration 6841, loss = 0.68979376\n",
      "Iteration 6842, loss = 0.68979305\n",
      "Iteration 6843, loss = 0.68979291\n",
      "Iteration 6844, loss = 0.68979266\n",
      "Iteration 6845, loss = 0.68979275\n",
      "Iteration 6846, loss = 0.68979239\n",
      "Iteration 6847, loss = 0.68979250\n",
      "Iteration 6848, loss = 0.68979199\n",
      "Iteration 6849, loss = 0.68979173\n",
      "Iteration 6850, loss = 0.68979130\n",
      "Iteration 6851, loss = 0.68979103\n",
      "Iteration 6852, loss = 0.68979102\n",
      "Iteration 6853, loss = 0.68979084\n",
      "Iteration 6854, loss = 0.68979098\n",
      "Iteration 6855, loss = 0.68979054\n",
      "Iteration 6856, loss = 0.68979050\n",
      "Iteration 6857, loss = 0.68979026\n",
      "Iteration 6858, loss = 0.68978992\n",
      "Iteration 6859, loss = 0.68978980\n",
      "Iteration 6860, loss = 0.68978933\n",
      "Iteration 6861, loss = 0.68978959\n",
      "Iteration 6862, loss = 0.68978912\n",
      "Iteration 6863, loss = 0.68978877\n",
      "Iteration 6864, loss = 0.68978883\n",
      "Iteration 6865, loss = 0.68978880\n",
      "Iteration 6866, loss = 0.68978860\n",
      "Iteration 6867, loss = 0.68978778\n",
      "Iteration 6868, loss = 0.68978788\n",
      "Iteration 6869, loss = 0.68978761\n",
      "Iteration 6870, loss = 0.68978746\n",
      "Iteration 6871, loss = 0.68978736\n",
      "Iteration 6872, loss = 0.68978679\n",
      "Iteration 6873, loss = 0.68978682\n",
      "Iteration 6874, loss = 0.68978706\n",
      "Iteration 6875, loss = 0.68978642\n",
      "Iteration 6876, loss = 0.68978615\n",
      "Iteration 6877, loss = 0.68978607\n",
      "Iteration 6878, loss = 0.68978604\n",
      "Iteration 6879, loss = 0.68978613\n",
      "Iteration 6880, loss = 0.68978556\n",
      "Iteration 6881, loss = 0.68978501\n",
      "Iteration 6882, loss = 0.68978511\n",
      "Iteration 6883, loss = 0.68978507\n",
      "Iteration 6884, loss = 0.68978431\n",
      "Iteration 6885, loss = 0.68978470\n",
      "Iteration 6886, loss = 0.68978409\n",
      "Iteration 6887, loss = 0.68978379\n",
      "Iteration 6888, loss = 0.68978421\n",
      "Iteration 6889, loss = 0.68978363\n",
      "Iteration 6890, loss = 0.68978312\n",
      "Iteration 6891, loss = 0.68978334\n",
      "Iteration 6892, loss = 0.68978315\n",
      "Iteration 6893, loss = 0.68978269\n",
      "Iteration 6894, loss = 0.68978264\n",
      "Iteration 6895, loss = 0.68978222\n",
      "Iteration 6896, loss = 0.68978208\n",
      "Iteration 6897, loss = 0.68978183\n",
      "Iteration 6898, loss = 0.68978205\n",
      "Iteration 6899, loss = 0.68978160\n",
      "Iteration 6900, loss = 0.68978141\n",
      "Iteration 6901, loss = 0.68978106\n",
      "Iteration 6902, loss = 0.68978068\n",
      "Iteration 6903, loss = 0.68978048\n",
      "Iteration 6904, loss = 0.68978095\n",
      "Iteration 6905, loss = 0.68978026\n",
      "Iteration 6906, loss = 0.68978016\n",
      "Iteration 6907, loss = 0.68977999\n",
      "Iteration 6908, loss = 0.68977966\n",
      "Iteration 6909, loss = 0.68977948\n",
      "Iteration 6910, loss = 0.68977919\n",
      "Iteration 6911, loss = 0.68977916\n",
      "Iteration 6912, loss = 0.68977885\n",
      "Iteration 6913, loss = 0.68977924\n",
      "Iteration 6914, loss = 0.68977814\n",
      "Iteration 6915, loss = 0.68977841\n",
      "Iteration 6916, loss = 0.68977787\n",
      "Iteration 6917, loss = 0.68977770\n",
      "Iteration 6918, loss = 0.68977788\n",
      "Iteration 6919, loss = 0.68977754\n",
      "Iteration 6920, loss = 0.68977718\n",
      "Iteration 6921, loss = 0.68977700\n",
      "Iteration 6922, loss = 0.68977676\n",
      "Iteration 6923, loss = 0.68977677\n",
      "Iteration 6924, loss = 0.68977661\n",
      "Iteration 6925, loss = 0.68977608\n",
      "Iteration 6926, loss = 0.68977624\n",
      "Iteration 6927, loss = 0.68977550\n",
      "Iteration 6928, loss = 0.68977584\n",
      "Iteration 6929, loss = 0.68977553\n",
      "Iteration 6930, loss = 0.68977501\n",
      "Iteration 6931, loss = 0.68977485\n",
      "Iteration 6932, loss = 0.68977468\n",
      "Iteration 6933, loss = 0.68977595\n",
      "Iteration 6934, loss = 0.68977421\n",
      "Iteration 6935, loss = 0.68977420\n",
      "Iteration 6936, loss = 0.68977388\n",
      "Iteration 6937, loss = 0.68977390\n",
      "Iteration 6938, loss = 0.68977334\n",
      "Iteration 6939, loss = 0.68977330\n",
      "Iteration 6940, loss = 0.68977290\n",
      "Iteration 6941, loss = 0.68977350\n",
      "Iteration 6942, loss = 0.68977253\n",
      "Iteration 6943, loss = 0.68977214\n",
      "Iteration 6944, loss = 0.68977209\n",
      "Iteration 6945, loss = 0.68977198\n",
      "Iteration 6946, loss = 0.68977144\n",
      "Iteration 6947, loss = 0.68977127\n",
      "Iteration 6948, loss = 0.68977119\n",
      "Iteration 6949, loss = 0.68977085\n",
      "Iteration 6950, loss = 0.68977072\n",
      "Iteration 6951, loss = 0.68977060\n",
      "Iteration 6952, loss = 0.68977033\n",
      "Iteration 6953, loss = 0.68976996\n",
      "Iteration 6954, loss = 0.68976975\n",
      "Iteration 6955, loss = 0.68977015\n",
      "Iteration 6956, loss = 0.68976934\n",
      "Iteration 6957, loss = 0.68976948\n",
      "Iteration 6958, loss = 0.68976905\n",
      "Iteration 6959, loss = 0.68976896\n",
      "Iteration 6960, loss = 0.68976853\n",
      "Iteration 6961, loss = 0.68976862\n",
      "Iteration 6962, loss = 0.68976835\n",
      "Iteration 6963, loss = 0.68976792\n",
      "Iteration 6964, loss = 0.68976785\n",
      "Iteration 6965, loss = 0.68976789\n",
      "Iteration 6966, loss = 0.68976747\n",
      "Iteration 6967, loss = 0.68976763\n",
      "Iteration 6968, loss = 0.68976694\n",
      "Iteration 6969, loss = 0.68976683\n",
      "Iteration 6970, loss = 0.68976663\n",
      "Iteration 6971, loss = 0.68976635\n",
      "Iteration 6972, loss = 0.68976717\n",
      "Iteration 6973, loss = 0.68976653\n",
      "Iteration 6974, loss = 0.68976592\n",
      "Iteration 6975, loss = 0.68976573\n",
      "Iteration 6976, loss = 0.68976528\n",
      "Iteration 6977, loss = 0.68976539\n",
      "Iteration 6978, loss = 0.68976561\n",
      "Iteration 6979, loss = 0.68976532\n",
      "Iteration 6980, loss = 0.68976462\n",
      "Iteration 6981, loss = 0.68976467\n",
      "Iteration 6982, loss = 0.68976428\n",
      "Iteration 6983, loss = 0.68976434\n",
      "Iteration 6984, loss = 0.68976437\n",
      "Iteration 6985, loss = 0.68976395\n",
      "Iteration 6986, loss = 0.68976353\n",
      "Iteration 6987, loss = 0.68976356\n",
      "Iteration 6988, loss = 0.68976326\n",
      "Iteration 6989, loss = 0.68976294\n",
      "Iteration 6990, loss = 0.68976267\n",
      "Iteration 6991, loss = 0.68976269\n",
      "Iteration 6992, loss = 0.68976247\n",
      "Iteration 6993, loss = 0.68976274\n",
      "Iteration 6994, loss = 0.68976194\n",
      "Iteration 6995, loss = 0.68976146\n",
      "Iteration 6996, loss = 0.68976183\n",
      "Iteration 6997, loss = 0.68976151\n",
      "Iteration 6998, loss = 0.68976094\n",
      "Iteration 6999, loss = 0.68976071\n",
      "Iteration 7000, loss = 0.68976054\n",
      "Iteration 7001, loss = 0.68976028\n",
      "Iteration 7002, loss = 0.68976028\n",
      "Iteration 7003, loss = 0.68976021\n",
      "Iteration 7004, loss = 0.68976006\n",
      "Iteration 7005, loss = 0.68975968\n",
      "Iteration 7006, loss = 0.68975975\n",
      "Iteration 7007, loss = 0.68975926\n",
      "Iteration 7008, loss = 0.68975912\n",
      "Iteration 7009, loss = 0.68975883\n",
      "Iteration 7010, loss = 0.68975892\n",
      "Iteration 7011, loss = 0.68975828\n",
      "Iteration 7012, loss = 0.68975798\n",
      "Iteration 7013, loss = 0.68975768\n",
      "Iteration 7014, loss = 0.68975773\n",
      "Iteration 7015, loss = 0.68975750\n",
      "Iteration 7016, loss = 0.68975734\n",
      "Iteration 7017, loss = 0.68975731\n",
      "Iteration 7018, loss = 0.68975718\n",
      "Iteration 7019, loss = 0.68975692\n",
      "Iteration 7020, loss = 0.68975653\n",
      "Iteration 7021, loss = 0.68975662\n",
      "Iteration 7022, loss = 0.68975606\n",
      "Iteration 7023, loss = 0.68975604\n",
      "Iteration 7024, loss = 0.68975627\n",
      "Iteration 7025, loss = 0.68975567\n",
      "Iteration 7026, loss = 0.68975590\n",
      "Iteration 7027, loss = 0.68975551\n",
      "Iteration 7028, loss = 0.68975511\n",
      "Iteration 7029, loss = 0.68975477\n",
      "Iteration 7030, loss = 0.68975481\n",
      "Iteration 7031, loss = 0.68975469\n",
      "Iteration 7032, loss = 0.68975421\n",
      "Iteration 7033, loss = 0.68975481\n",
      "Iteration 7034, loss = 0.68975379\n",
      "Iteration 7035, loss = 0.68975467\n",
      "Iteration 7036, loss = 0.68975330\n",
      "Iteration 7037, loss = 0.68975332\n",
      "Iteration 7038, loss = 0.68975301\n",
      "Iteration 7039, loss = 0.68975290\n",
      "Iteration 7040, loss = 0.68975251\n",
      "Iteration 7041, loss = 0.68975225\n",
      "Iteration 7042, loss = 0.68975213\n",
      "Iteration 7043, loss = 0.68975177\n",
      "Iteration 7044, loss = 0.68975248\n",
      "Iteration 7045, loss = 0.68975139\n",
      "Iteration 7046, loss = 0.68975129\n",
      "Iteration 7047, loss = 0.68975140\n",
      "Iteration 7048, loss = 0.68975087\n",
      "Iteration 7049, loss = 0.68975060\n",
      "Iteration 7050, loss = 0.68975108\n",
      "Iteration 7051, loss = 0.68975064\n",
      "Iteration 7052, loss = 0.68975029\n",
      "Iteration 7053, loss = 0.68975000\n",
      "Iteration 7054, loss = 0.68974941\n",
      "Iteration 7055, loss = 0.68974936\n",
      "Iteration 7056, loss = 0.68974969\n",
      "Iteration 7057, loss = 0.68974930\n",
      "Iteration 7058, loss = 0.68974863\n",
      "Iteration 7059, loss = 0.68974863\n",
      "Iteration 7060, loss = 0.68974844\n",
      "Iteration 7061, loss = 0.68974805\n",
      "Iteration 7062, loss = 0.68974794\n",
      "Iteration 7063, loss = 0.68974770\n",
      "Iteration 7064, loss = 0.68974832\n",
      "Iteration 7065, loss = 0.68974736\n",
      "Iteration 7066, loss = 0.68974724\n",
      "Iteration 7067, loss = 0.68974696\n",
      "Iteration 7068, loss = 0.68974699\n",
      "Iteration 7069, loss = 0.68974704\n",
      "Iteration 7070, loss = 0.68974637\n",
      "Iteration 7071, loss = 0.68974677\n",
      "Iteration 7072, loss = 0.68974602\n",
      "Iteration 7073, loss = 0.68974587\n",
      "Iteration 7074, loss = 0.68974544\n",
      "Iteration 7075, loss = 0.68974540\n",
      "Iteration 7076, loss = 0.68974530\n",
      "Iteration 7077, loss = 0.68974557\n",
      "Iteration 7078, loss = 0.68974547\n",
      "Iteration 7079, loss = 0.68974446\n",
      "Iteration 7080, loss = 0.68974425\n",
      "Iteration 7081, loss = 0.68974424\n",
      "Iteration 7082, loss = 0.68974383\n",
      "Iteration 7083, loss = 0.68974394\n",
      "Iteration 7084, loss = 0.68974352\n",
      "Iteration 7085, loss = 0.68974361\n",
      "Iteration 7086, loss = 0.68974303\n",
      "Iteration 7087, loss = 0.68974297\n",
      "Iteration 7088, loss = 0.68974271\n",
      "Iteration 7089, loss = 0.68974267\n",
      "Iteration 7090, loss = 0.68974235\n",
      "Iteration 7091, loss = 0.68974235\n",
      "Iteration 7092, loss = 0.68974222\n",
      "Iteration 7093, loss = 0.68974212\n",
      "Iteration 7094, loss = 0.68974155\n",
      "Iteration 7095, loss = 0.68974157\n",
      "Iteration 7096, loss = 0.68974142\n",
      "Iteration 7097, loss = 0.68974123\n",
      "Iteration 7098, loss = 0.68974067\n",
      "Iteration 7099, loss = 0.68974045\n",
      "Iteration 7100, loss = 0.68974034\n",
      "Iteration 7101, loss = 0.68974008\n",
      "Iteration 7102, loss = 0.68973995\n",
      "Iteration 7103, loss = 0.68973967\n",
      "Iteration 7104, loss = 0.68973954\n",
      "Iteration 7105, loss = 0.68973952\n",
      "Iteration 7106, loss = 0.68973959\n",
      "Iteration 7107, loss = 0.68973925\n",
      "Iteration 7108, loss = 0.68973967\n",
      "Iteration 7109, loss = 0.68973858\n",
      "Iteration 7110, loss = 0.68973815\n",
      "Iteration 7111, loss = 0.68973833\n",
      "Iteration 7112, loss = 0.68973848\n",
      "Iteration 7113, loss = 0.68973857\n",
      "Iteration 7114, loss = 0.68973738\n",
      "Iteration 7115, loss = 0.68973760\n",
      "Iteration 7116, loss = 0.68973701\n",
      "Iteration 7117, loss = 0.68973692\n",
      "Iteration 7118, loss = 0.68973668\n",
      "Iteration 7119, loss = 0.68973629\n",
      "Iteration 7120, loss = 0.68973661\n",
      "Iteration 7121, loss = 0.68973605\n",
      "Iteration 7122, loss = 0.68973592\n",
      "Iteration 7123, loss = 0.68973549\n",
      "Iteration 7124, loss = 0.68973560\n",
      "Iteration 7125, loss = 0.68973520\n",
      "Iteration 7126, loss = 0.68973483\n",
      "Iteration 7127, loss = 0.68973468\n",
      "Iteration 7128, loss = 0.68973446\n",
      "Iteration 7129, loss = 0.68973458\n",
      "Iteration 7130, loss = 0.68973420\n",
      "Iteration 7131, loss = 0.68973406\n",
      "Iteration 7132, loss = 0.68973404\n",
      "Iteration 7133, loss = 0.68973373\n",
      "Iteration 7134, loss = 0.68973340\n",
      "Iteration 7135, loss = 0.68973319\n",
      "Iteration 7136, loss = 0.68973285\n",
      "Iteration 7137, loss = 0.68973282\n",
      "Iteration 7138, loss = 0.68973261\n",
      "Iteration 7139, loss = 0.68973233\n",
      "Iteration 7140, loss = 0.68973237\n",
      "Iteration 7141, loss = 0.68973182\n",
      "Iteration 7142, loss = 0.68973135\n",
      "Iteration 7143, loss = 0.68973229\n",
      "Iteration 7144, loss = 0.68973185\n",
      "Iteration 7145, loss = 0.68973179\n",
      "Iteration 7146, loss = 0.68973108\n",
      "Iteration 7147, loss = 0.68973089\n",
      "Iteration 7148, loss = 0.68973048\n",
      "Iteration 7149, loss = 0.68973054\n",
      "Iteration 7150, loss = 0.68973023\n",
      "Iteration 7151, loss = 0.68972985\n",
      "Iteration 7152, loss = 0.68972982\n",
      "Iteration 7153, loss = 0.68973020\n",
      "Iteration 7154, loss = 0.68972931\n",
      "Iteration 7155, loss = 0.68972930\n",
      "Iteration 7156, loss = 0.68972967\n",
      "Iteration 7157, loss = 0.68972881\n",
      "Iteration 7158, loss = 0.68972883\n",
      "Iteration 7159, loss = 0.68972835\n",
      "Iteration 7160, loss = 0.68972882\n",
      "Iteration 7161, loss = 0.68972839\n",
      "Iteration 7162, loss = 0.68972887\n",
      "Iteration 7163, loss = 0.68972787\n",
      "Iteration 7164, loss = 0.68972766\n",
      "Iteration 7165, loss = 0.68972765\n",
      "Iteration 7166, loss = 0.68972708\n",
      "Iteration 7167, loss = 0.68972715\n",
      "Iteration 7168, loss = 0.68972675\n",
      "Iteration 7169, loss = 0.68972662\n",
      "Iteration 7170, loss = 0.68972667\n",
      "Iteration 7171, loss = 0.68972656\n",
      "Iteration 7172, loss = 0.68972610\n",
      "Iteration 7173, loss = 0.68972612\n",
      "Iteration 7174, loss = 0.68972591\n",
      "Iteration 7175, loss = 0.68972556\n",
      "Iteration 7176, loss = 0.68972539\n",
      "Iteration 7177, loss = 0.68972537\n",
      "Iteration 7178, loss = 0.68972489\n",
      "Iteration 7179, loss = 0.68972492\n",
      "Iteration 7180, loss = 0.68972467\n",
      "Iteration 7181, loss = 0.68972443\n",
      "Iteration 7182, loss = 0.68972428\n",
      "Iteration 7183, loss = 0.68972431\n",
      "Iteration 7184, loss = 0.68972390\n",
      "Iteration 7185, loss = 0.68972365\n",
      "Iteration 7186, loss = 0.68972317\n",
      "Iteration 7187, loss = 0.68972356\n",
      "Iteration 7188, loss = 0.68972288\n",
      "Iteration 7189, loss = 0.68972270\n",
      "Iteration 7190, loss = 0.68972272\n",
      "Iteration 7191, loss = 0.68972227\n",
      "Iteration 7192, loss = 0.68972193\n",
      "Iteration 7193, loss = 0.68972224\n",
      "Iteration 7194, loss = 0.68972222\n",
      "Iteration 7195, loss = 0.68972138\n",
      "Iteration 7196, loss = 0.68972114\n",
      "Iteration 7197, loss = 0.68972135\n",
      "Iteration 7198, loss = 0.68972098\n",
      "Iteration 7199, loss = 0.68972066\n",
      "Iteration 7200, loss = 0.68972103\n",
      "Iteration 7201, loss = 0.68972056\n",
      "Iteration 7202, loss = 0.68972020\n",
      "Iteration 7203, loss = 0.68972051\n",
      "Iteration 7204, loss = 0.68971992\n",
      "Iteration 7205, loss = 0.68971948\n",
      "Iteration 7206, loss = 0.68971932\n",
      "Iteration 7207, loss = 0.68971930\n",
      "Iteration 7208, loss = 0.68971914\n",
      "Iteration 7209, loss = 0.68971940\n",
      "Iteration 7210, loss = 0.68971886\n",
      "Iteration 7211, loss = 0.68971836\n",
      "Iteration 7212, loss = 0.68971837\n",
      "Iteration 7213, loss = 0.68971809\n",
      "Iteration 7214, loss = 0.68971777\n",
      "Iteration 7215, loss = 0.68971800\n",
      "Iteration 7216, loss = 0.68971738\n",
      "Iteration 7217, loss = 0.68971758\n",
      "Iteration 7218, loss = 0.68971686\n",
      "Iteration 7219, loss = 0.68971682\n",
      "Iteration 7220, loss = 0.68971664\n",
      "Iteration 7221, loss = 0.68971662\n",
      "Iteration 7222, loss = 0.68971616\n",
      "Iteration 7223, loss = 0.68971748\n",
      "Iteration 7224, loss = 0.68971593\n",
      "Iteration 7225, loss = 0.68971569\n",
      "Iteration 7226, loss = 0.68971585\n",
      "Iteration 7227, loss = 0.68971530\n",
      "Iteration 7228, loss = 0.68971509\n",
      "Iteration 7229, loss = 0.68971628\n",
      "Iteration 7230, loss = 0.68971535\n",
      "Iteration 7231, loss = 0.68971464\n",
      "Iteration 7232, loss = 0.68971411\n",
      "Iteration 7233, loss = 0.68971443\n",
      "Iteration 7234, loss = 0.68971403\n",
      "Iteration 7235, loss = 0.68971369\n",
      "Iteration 7236, loss = 0.68971371\n",
      "Iteration 7237, loss = 0.68971364\n",
      "Iteration 7238, loss = 0.68971320\n",
      "Iteration 7239, loss = 0.68971342\n",
      "Iteration 7240, loss = 0.68971279\n",
      "Iteration 7241, loss = 0.68971265\n",
      "Iteration 7242, loss = 0.68971212\n",
      "Iteration 7243, loss = 0.68971244\n",
      "Iteration 7244, loss = 0.68971225\n",
      "Iteration 7245, loss = 0.68971190\n",
      "Iteration 7246, loss = 0.68971236\n",
      "Iteration 7247, loss = 0.68971170\n",
      "Iteration 7248, loss = 0.68971192\n",
      "Iteration 7249, loss = 0.68971098\n",
      "Iteration 7250, loss = 0.68971094\n",
      "Iteration 7251, loss = 0.68971081\n",
      "Iteration 7252, loss = 0.68971053\n",
      "Iteration 7253, loss = 0.68971081\n",
      "Iteration 7254, loss = 0.68971072\n",
      "Iteration 7255, loss = 0.68971002\n",
      "Iteration 7256, loss = 0.68971012\n",
      "Iteration 7257, loss = 0.68971021\n",
      "Iteration 7258, loss = 0.68970978\n",
      "Iteration 7259, loss = 0.68970930\n",
      "Iteration 7260, loss = 0.68970888\n",
      "Iteration 7261, loss = 0.68970916\n",
      "Iteration 7262, loss = 0.68970868\n",
      "Iteration 7263, loss = 0.68970859\n",
      "Iteration 7264, loss = 0.68970843\n",
      "Iteration 7265, loss = 0.68970827\n",
      "Iteration 7266, loss = 0.68970825\n",
      "Iteration 7267, loss = 0.68970833\n",
      "Iteration 7268, loss = 0.68970760\n",
      "Iteration 7269, loss = 0.68970761\n",
      "Iteration 7270, loss = 0.68970730\n",
      "Iteration 7271, loss = 0.68970721\n",
      "Iteration 7272, loss = 0.68970674\n",
      "Iteration 7273, loss = 0.68970652\n",
      "Iteration 7274, loss = 0.68970644\n",
      "Iteration 7275, loss = 0.68970626\n",
      "Iteration 7276, loss = 0.68970653\n",
      "Iteration 7277, loss = 0.68970607\n",
      "Iteration 7278, loss = 0.68970596\n",
      "Iteration 7279, loss = 0.68970551\n",
      "Iteration 7280, loss = 0.68970553\n",
      "Iteration 7281, loss = 0.68970504\n",
      "Iteration 7282, loss = 0.68970535\n",
      "Iteration 7283, loss = 0.68970523\n",
      "Iteration 7284, loss = 0.68970456\n",
      "Iteration 7285, loss = 0.68970439\n",
      "Iteration 7286, loss = 0.68970448\n",
      "Iteration 7287, loss = 0.68970413\n",
      "Iteration 7288, loss = 0.68970401\n",
      "Iteration 7289, loss = 0.68970369\n",
      "Iteration 7290, loss = 0.68970340\n",
      "Iteration 7291, loss = 0.68970349\n",
      "Iteration 7292, loss = 0.68970279\n",
      "Iteration 7293, loss = 0.68970297\n",
      "Iteration 7294, loss = 0.68970294\n",
      "Iteration 7295, loss = 0.68970263\n",
      "Iteration 7296, loss = 0.68970255\n",
      "Iteration 7297, loss = 0.68970227\n",
      "Iteration 7298, loss = 0.68970224\n",
      "Iteration 7299, loss = 0.68970223\n",
      "Iteration 7300, loss = 0.68970142\n",
      "Iteration 7301, loss = 0.68970141\n",
      "Iteration 7302, loss = 0.68970148\n",
      "Iteration 7303, loss = 0.68970175\n",
      "Iteration 7304, loss = 0.68970088\n",
      "Iteration 7305, loss = 0.68970139\n",
      "Iteration 7306, loss = 0.68970050\n",
      "Iteration 7307, loss = 0.68970076\n",
      "Iteration 7308, loss = 0.68970114\n",
      "Iteration 7309, loss = 0.68970013\n",
      "Iteration 7310, loss = 0.68969980\n",
      "Iteration 7311, loss = 0.68969985\n",
      "Iteration 7312, loss = 0.68969942\n",
      "Iteration 7313, loss = 0.68969928\n",
      "Iteration 7314, loss = 0.68969909\n",
      "Iteration 7315, loss = 0.68969907\n",
      "Iteration 7316, loss = 0.68969929\n",
      "Iteration 7317, loss = 0.68969894\n",
      "Iteration 7318, loss = 0.68969848\n",
      "Iteration 7319, loss = 0.68969811\n",
      "Iteration 7320, loss = 0.68969787\n",
      "Iteration 7321, loss = 0.68969816\n",
      "Iteration 7322, loss = 0.68969741\n",
      "Iteration 7323, loss = 0.68969766\n",
      "Iteration 7324, loss = 0.68969721\n",
      "Iteration 7325, loss = 0.68969692\n",
      "Iteration 7326, loss = 0.68969687\n",
      "Iteration 7327, loss = 0.68969659\n",
      "Iteration 7328, loss = 0.68969646\n",
      "Iteration 7329, loss = 0.68969630\n",
      "Iteration 7330, loss = 0.68969578\n",
      "Iteration 7331, loss = 0.68969623\n",
      "Iteration 7332, loss = 0.68969635\n",
      "Iteration 7333, loss = 0.68969567\n",
      "Iteration 7334, loss = 0.68969591\n",
      "Iteration 7335, loss = 0.68969538\n",
      "Iteration 7336, loss = 0.68969497\n",
      "Iteration 7337, loss = 0.68969464\n",
      "Iteration 7338, loss = 0.68969465\n",
      "Iteration 7339, loss = 0.68969433\n",
      "Iteration 7340, loss = 0.68969424\n",
      "Iteration 7341, loss = 0.68969430\n",
      "Iteration 7342, loss = 0.68969427\n",
      "Iteration 7343, loss = 0.68969413\n",
      "Iteration 7344, loss = 0.68969335\n",
      "Iteration 7345, loss = 0.68969310\n",
      "Iteration 7346, loss = 0.68969319\n",
      "Iteration 7347, loss = 0.68969284\n",
      "Iteration 7348, loss = 0.68969318\n",
      "Iteration 7349, loss = 0.68969265\n",
      "Iteration 7350, loss = 0.68969284\n",
      "Iteration 7351, loss = 0.68969268\n",
      "Iteration 7352, loss = 0.68969202\n",
      "Iteration 7353, loss = 0.68969187\n",
      "Iteration 7354, loss = 0.68969187\n",
      "Iteration 7355, loss = 0.68969167\n",
      "Iteration 7356, loss = 0.68969156\n",
      "Iteration 7357, loss = 0.68969111\n",
      "Iteration 7358, loss = 0.68969087\n",
      "Iteration 7359, loss = 0.68969146\n",
      "Iteration 7360, loss = 0.68969077\n",
      "Iteration 7361, loss = 0.68969041\n",
      "Iteration 7362, loss = 0.68968984\n",
      "Iteration 7363, loss = 0.68969000\n",
      "Iteration 7364, loss = 0.68968978\n",
      "Iteration 7365, loss = 0.68968973\n",
      "Iteration 7366, loss = 0.68968893\n",
      "Iteration 7367, loss = 0.68968896\n",
      "Iteration 7368, loss = 0.68968889\n",
      "Iteration 7369, loss = 0.68968934\n",
      "Iteration 7370, loss = 0.68968898\n",
      "Iteration 7371, loss = 0.68968804\n",
      "Iteration 7372, loss = 0.68968835\n",
      "Iteration 7373, loss = 0.68968820\n",
      "Iteration 7374, loss = 0.68968738\n",
      "Iteration 7375, loss = 0.68968772\n",
      "Iteration 7376, loss = 0.68968742\n",
      "Iteration 7377, loss = 0.68968756\n",
      "Iteration 7378, loss = 0.68968745\n",
      "Iteration 7379, loss = 0.68968715\n",
      "Iteration 7380, loss = 0.68968656\n",
      "Iteration 7381, loss = 0.68968703\n",
      "Iteration 7382, loss = 0.68968620\n",
      "Iteration 7383, loss = 0.68968600\n",
      "Iteration 7384, loss = 0.68968565\n",
      "Iteration 7385, loss = 0.68968595\n",
      "Iteration 7386, loss = 0.68968531\n",
      "Iteration 7387, loss = 0.68968517\n",
      "Iteration 7388, loss = 0.68968571\n",
      "Iteration 7389, loss = 0.68968486\n",
      "Iteration 7390, loss = 0.68968488\n",
      "Iteration 7391, loss = 0.68968481\n",
      "Iteration 7392, loss = 0.68968445\n",
      "Iteration 7393, loss = 0.68968437\n",
      "Iteration 7394, loss = 0.68968404\n",
      "Iteration 7395, loss = 0.68968421\n",
      "Iteration 7396, loss = 0.68968430\n",
      "Iteration 7397, loss = 0.68968374\n",
      "Iteration 7398, loss = 0.68968361\n",
      "Iteration 7399, loss = 0.68968315\n",
      "Iteration 7400, loss = 0.68968308\n",
      "Iteration 7401, loss = 0.68968285\n",
      "Iteration 7402, loss = 0.68968388\n",
      "Iteration 7403, loss = 0.68968301\n",
      "Iteration 7404, loss = 0.68968227\n",
      "Iteration 7405, loss = 0.68968220\n",
      "Iteration 7406, loss = 0.68968203\n",
      "Iteration 7407, loss = 0.68968168\n",
      "Iteration 7408, loss = 0.68968140\n",
      "Iteration 7409, loss = 0.68968129\n",
      "Iteration 7410, loss = 0.68968119\n",
      "Iteration 7411, loss = 0.68968128\n",
      "Iteration 7412, loss = 0.68968090\n",
      "Iteration 7413, loss = 0.68968067\n",
      "Iteration 7414, loss = 0.68968152\n",
      "Iteration 7415, loss = 0.68968031\n",
      "Iteration 7416, loss = 0.68968040\n",
      "Iteration 7417, loss = 0.68967954\n",
      "Iteration 7418, loss = 0.68967953\n",
      "Iteration 7419, loss = 0.68967985\n",
      "Iteration 7420, loss = 0.68967924\n",
      "Iteration 7421, loss = 0.68967931\n",
      "Iteration 7422, loss = 0.68967893\n",
      "Iteration 7423, loss = 0.68967879\n",
      "Iteration 7424, loss = 0.68967844\n",
      "Iteration 7425, loss = 0.68967844\n",
      "Iteration 7426, loss = 0.68967810\n",
      "Iteration 7427, loss = 0.68967825\n",
      "Iteration 7428, loss = 0.68967814\n",
      "Iteration 7429, loss = 0.68967782\n",
      "Iteration 7430, loss = 0.68967784\n",
      "Iteration 7431, loss = 0.68967715\n",
      "Iteration 7432, loss = 0.68967692\n",
      "Iteration 7433, loss = 0.68967678\n",
      "Iteration 7434, loss = 0.68967655\n",
      "Iteration 7435, loss = 0.68967661\n",
      "Iteration 7436, loss = 0.68967611\n",
      "Iteration 7437, loss = 0.68967660\n",
      "Iteration 7438, loss = 0.68967616\n",
      "Iteration 7439, loss = 0.68967573\n",
      "Iteration 7440, loss = 0.68967598\n",
      "Iteration 7441, loss = 0.68967543\n",
      "Iteration 7442, loss = 0.68967569\n",
      "Iteration 7443, loss = 0.68967525\n",
      "Iteration 7444, loss = 0.68967470\n",
      "Iteration 7445, loss = 0.68967470\n",
      "Iteration 7446, loss = 0.68967453\n",
      "Iteration 7447, loss = 0.68967398\n",
      "Iteration 7448, loss = 0.68967379\n",
      "Iteration 7449, loss = 0.68967393\n",
      "Iteration 7450, loss = 0.68967359\n",
      "Iteration 7451, loss = 0.68967436\n",
      "Iteration 7452, loss = 0.68967288\n",
      "Iteration 7453, loss = 0.68967290\n",
      "Iteration 7454, loss = 0.68967337\n",
      "Iteration 7455, loss = 0.68967293\n",
      "Iteration 7456, loss = 0.68967237\n",
      "Iteration 7457, loss = 0.68967221\n",
      "Iteration 7458, loss = 0.68967199\n",
      "Iteration 7459, loss = 0.68967178\n",
      "Iteration 7460, loss = 0.68967130\n",
      "Iteration 7461, loss = 0.68967150\n",
      "Iteration 7462, loss = 0.68967168\n",
      "Iteration 7463, loss = 0.68967086\n",
      "Iteration 7464, loss = 0.68967084\n",
      "Iteration 7465, loss = 0.68967090\n",
      "Iteration 7466, loss = 0.68967039\n",
      "Iteration 7467, loss = 0.68967064\n",
      "Iteration 7468, loss = 0.68967013\n",
      "Iteration 7469, loss = 0.68967048\n",
      "Iteration 7470, loss = 0.68966936\n",
      "Iteration 7471, loss = 0.68966957\n",
      "Iteration 7472, loss = 0.68966913\n",
      "Iteration 7473, loss = 0.68966921\n",
      "Iteration 7474, loss = 0.68966895\n",
      "Iteration 7475, loss = 0.68966858\n",
      "Iteration 7476, loss = 0.68966856\n",
      "Iteration 7477, loss = 0.68966827\n",
      "Iteration 7478, loss = 0.68966906\n",
      "Iteration 7479, loss = 0.68966867\n",
      "Iteration 7480, loss = 0.68966778\n",
      "Iteration 7481, loss = 0.68966825\n",
      "Iteration 7482, loss = 0.68966766\n",
      "Iteration 7483, loss = 0.68966741\n",
      "Iteration 7484, loss = 0.68966726\n",
      "Iteration 7485, loss = 0.68966667\n",
      "Iteration 7486, loss = 0.68966664\n",
      "Iteration 7487, loss = 0.68966641\n",
      "Iteration 7488, loss = 0.68966695\n",
      "Iteration 7489, loss = 0.68966613\n",
      "Iteration 7490, loss = 0.68966609\n",
      "Iteration 7491, loss = 0.68966630\n",
      "Iteration 7492, loss = 0.68966641\n",
      "Iteration 7493, loss = 0.68966573\n",
      "Iteration 7494, loss = 0.68966594\n",
      "Iteration 7495, loss = 0.68966575\n",
      "Iteration 7496, loss = 0.68966543\n",
      "Iteration 7497, loss = 0.68966489\n",
      "Iteration 7498, loss = 0.68966536\n",
      "Iteration 7499, loss = 0.68966463\n",
      "Iteration 7500, loss = 0.68966420\n",
      "Iteration 7501, loss = 0.68966412\n",
      "Iteration 7502, loss = 0.68966415\n",
      "Iteration 7503, loss = 0.68966397\n",
      "Iteration 7504, loss = 0.68966420\n",
      "Iteration 7505, loss = 0.68966394\n",
      "Iteration 7506, loss = 0.68966324\n",
      "Iteration 7507, loss = 0.68966377\n",
      "Iteration 7508, loss = 0.68966301\n",
      "Iteration 7509, loss = 0.68966262\n",
      "Iteration 7510, loss = 0.68966280\n",
      "Iteration 7511, loss = 0.68966285\n",
      "Iteration 7512, loss = 0.68966233\n",
      "Iteration 7513, loss = 0.68966244\n",
      "Iteration 7514, loss = 0.68966207\n",
      "Iteration 7515, loss = 0.68966161\n",
      "Iteration 7516, loss = 0.68966182\n",
      "Iteration 7517, loss = 0.68966138\n",
      "Iteration 7518, loss = 0.68966138\n",
      "Iteration 7519, loss = 0.68966098\n",
      "Iteration 7520, loss = 0.68966077\n",
      "Iteration 7521, loss = 0.68966104\n",
      "Iteration 7522, loss = 0.68966051\n",
      "Iteration 7523, loss = 0.68966055\n",
      "Iteration 7524, loss = 0.68966027\n",
      "Iteration 7525, loss = 0.68966013\n",
      "Iteration 7526, loss = 0.68966038\n",
      "Iteration 7527, loss = 0.68965998\n",
      "Iteration 7528, loss = 0.68966005\n",
      "Iteration 7529, loss = 0.68965899\n",
      "Iteration 7530, loss = 0.68965920\n",
      "Iteration 7531, loss = 0.68965874\n",
      "Iteration 7532, loss = 0.68965869\n",
      "Iteration 7533, loss = 0.68965880\n",
      "Iteration 7534, loss = 0.68965830\n",
      "Iteration 7535, loss = 0.68965799\n",
      "Iteration 7536, loss = 0.68965804\n",
      "Iteration 7537, loss = 0.68965798\n",
      "Iteration 7538, loss = 0.68965806\n",
      "Iteration 7539, loss = 0.68965732\n",
      "Iteration 7540, loss = 0.68965724\n",
      "Iteration 7541, loss = 0.68965676\n",
      "Iteration 7542, loss = 0.68965716\n",
      "Iteration 7543, loss = 0.68965634\n",
      "Iteration 7544, loss = 0.68965639\n",
      "Iteration 7545, loss = 0.68965613\n",
      "Iteration 7546, loss = 0.68965593\n",
      "Iteration 7547, loss = 0.68965613\n",
      "Iteration 7548, loss = 0.68965549\n",
      "Iteration 7549, loss = 0.68965559\n",
      "Iteration 7550, loss = 0.68965530\n",
      "Iteration 7551, loss = 0.68965520\n",
      "Iteration 7552, loss = 0.68965514\n",
      "Iteration 7553, loss = 0.68965463\n",
      "Iteration 7554, loss = 0.68965434\n",
      "Iteration 7555, loss = 0.68965435\n",
      "Iteration 7556, loss = 0.68965410\n",
      "Iteration 7557, loss = 0.68965383\n",
      "Iteration 7558, loss = 0.68965360\n",
      "Iteration 7559, loss = 0.68965374\n",
      "Iteration 7560, loss = 0.68965308\n",
      "Iteration 7561, loss = 0.68965318\n",
      "Iteration 7562, loss = 0.68965301\n",
      "Iteration 7563, loss = 0.68965307\n",
      "Iteration 7564, loss = 0.68965268\n",
      "Iteration 7565, loss = 0.68965261\n",
      "Iteration 7566, loss = 0.68965247\n",
      "Iteration 7567, loss = 0.68965259\n",
      "Iteration 7568, loss = 0.68965175\n",
      "Iteration 7569, loss = 0.68965211\n",
      "Iteration 7570, loss = 0.68965176\n",
      "Iteration 7571, loss = 0.68965131\n",
      "Iteration 7572, loss = 0.68965167\n",
      "Iteration 7573, loss = 0.68965137\n",
      "Iteration 7574, loss = 0.68965118\n",
      "Iteration 7575, loss = 0.68965083\n",
      "Iteration 7576, loss = 0.68965097\n",
      "Iteration 7577, loss = 0.68965071\n",
      "Iteration 7578, loss = 0.68965033\n",
      "Iteration 7579, loss = 0.68965020\n",
      "Iteration 7580, loss = 0.68964984\n",
      "Iteration 7581, loss = 0.68964987\n",
      "Iteration 7582, loss = 0.68964945\n",
      "Iteration 7583, loss = 0.68964989\n",
      "Iteration 7584, loss = 0.68964931\n",
      "Iteration 7585, loss = 0.68964908\n",
      "Iteration 7586, loss = 0.68964877\n",
      "Iteration 7587, loss = 0.68964866\n",
      "Iteration 7588, loss = 0.68964901\n",
      "Iteration 7589, loss = 0.68964854\n",
      "Iteration 7590, loss = 0.68964803\n",
      "Iteration 7591, loss = 0.68964798\n",
      "Iteration 7592, loss = 0.68964775\n",
      "Iteration 7593, loss = 0.68964741\n",
      "Iteration 7594, loss = 0.68964751\n",
      "Iteration 7595, loss = 0.68964766\n",
      "Iteration 7596, loss = 0.68964705\n",
      "Iteration 7597, loss = 0.68964770\n",
      "Iteration 7598, loss = 0.68964647\n",
      "Iteration 7599, loss = 0.68964648\n",
      "Iteration 7600, loss = 0.68964662\n",
      "Iteration 7601, loss = 0.68964645\n",
      "Iteration 7602, loss = 0.68964637\n",
      "Iteration 7603, loss = 0.68964601\n",
      "Iteration 7604, loss = 0.68964559\n",
      "Iteration 7605, loss = 0.68964569\n",
      "Iteration 7606, loss = 0.68964520\n",
      "Iteration 7607, loss = 0.68964535\n",
      "Iteration 7608, loss = 0.68964530\n",
      "Iteration 7609, loss = 0.68964462\n",
      "Iteration 7610, loss = 0.68964445\n",
      "Iteration 7611, loss = 0.68964476\n",
      "Iteration 7612, loss = 0.68964438\n",
      "Iteration 7613, loss = 0.68964402\n",
      "Iteration 7614, loss = 0.68964397\n",
      "Iteration 7615, loss = 0.68964477\n",
      "Iteration 7616, loss = 0.68964437\n",
      "Iteration 7617, loss = 0.68964344\n",
      "Iteration 7618, loss = 0.68964354\n",
      "Iteration 7619, loss = 0.68964293\n",
      "Iteration 7620, loss = 0.68964300\n",
      "Iteration 7621, loss = 0.68964280\n",
      "Iteration 7622, loss = 0.68964397\n",
      "Iteration 7623, loss = 0.68964250\n",
      "Iteration 7624, loss = 0.68964249\n",
      "Iteration 7625, loss = 0.68964247\n",
      "Iteration 7626, loss = 0.68964168\n",
      "Iteration 7627, loss = 0.68964162\n",
      "Iteration 7628, loss = 0.68964157\n",
      "Iteration 7629, loss = 0.68964142\n",
      "Iteration 7630, loss = 0.68964110\n",
      "Iteration 7631, loss = 0.68964113\n",
      "Iteration 7632, loss = 0.68964131\n",
      "Iteration 7633, loss = 0.68964102\n",
      "Iteration 7634, loss = 0.68964038\n",
      "Iteration 7635, loss = 0.68964043\n",
      "Iteration 7636, loss = 0.68964018\n",
      "Iteration 7637, loss = 0.68963992\n",
      "Iteration 7638, loss = 0.68963982\n",
      "Iteration 7639, loss = 0.68963948\n",
      "Iteration 7640, loss = 0.68963938\n",
      "Iteration 7641, loss = 0.68963922\n",
      "Iteration 7642, loss = 0.68963890\n",
      "Iteration 7643, loss = 0.68963886\n",
      "Iteration 7644, loss = 0.68963867\n",
      "Iteration 7645, loss = 0.68963823\n",
      "Iteration 7646, loss = 0.68963844\n",
      "Iteration 7647, loss = 0.68963806\n",
      "Iteration 7648, loss = 0.68963813\n",
      "Iteration 7649, loss = 0.68963763\n",
      "Iteration 7650, loss = 0.68963754\n",
      "Iteration 7651, loss = 0.68963755\n",
      "Iteration 7652, loss = 0.68963703\n",
      "Iteration 7653, loss = 0.68963733\n",
      "Iteration 7654, loss = 0.68963704\n",
      "Iteration 7655, loss = 0.68963697\n",
      "Iteration 7656, loss = 0.68963650\n",
      "Iteration 7657, loss = 0.68963688\n",
      "Iteration 7658, loss = 0.68963709\n",
      "Iteration 7659, loss = 0.68963605\n",
      "Iteration 7660, loss = 0.68963591\n",
      "Iteration 7661, loss = 0.68963629\n",
      "Iteration 7662, loss = 0.68963602\n",
      "Iteration 7663, loss = 0.68963539\n",
      "Iteration 7664, loss = 0.68963530\n",
      "Iteration 7665, loss = 0.68963526\n",
      "Iteration 7666, loss = 0.68963497\n",
      "Iteration 7667, loss = 0.68963508\n",
      "Iteration 7668, loss = 0.68963427\n",
      "Iteration 7669, loss = 0.68963449\n",
      "Iteration 7670, loss = 0.68963462\n",
      "Iteration 7671, loss = 0.68963393\n",
      "Iteration 7672, loss = 0.68963375\n",
      "Iteration 7673, loss = 0.68963386\n",
      "Iteration 7674, loss = 0.68963404\n",
      "Iteration 7675, loss = 0.68963447\n",
      "Iteration 7676, loss = 0.68963306\n",
      "Iteration 7677, loss = 0.68963293\n",
      "Iteration 7678, loss = 0.68963315\n",
      "Iteration 7679, loss = 0.68963254\n",
      "Iteration 7680, loss = 0.68963218\n",
      "Iteration 7681, loss = 0.68963227\n",
      "Iteration 7682, loss = 0.68963207\n",
      "Iteration 7683, loss = 0.68963203\n",
      "Iteration 7684, loss = 0.68963182\n",
      "Iteration 7685, loss = 0.68963188\n",
      "Iteration 7686, loss = 0.68963148\n",
      "Iteration 7687, loss = 0.68963135\n",
      "Iteration 7688, loss = 0.68963096\n",
      "Iteration 7689, loss = 0.68963140\n",
      "Iteration 7690, loss = 0.68963114\n",
      "Iteration 7691, loss = 0.68963114\n",
      "Iteration 7692, loss = 0.68963081\n",
      "Iteration 7693, loss = 0.68963012\n",
      "Iteration 7694, loss = 0.68963122\n",
      "Iteration 7695, loss = 0.68962980\n",
      "Iteration 7696, loss = 0.68963000\n",
      "Iteration 7697, loss = 0.68962949\n",
      "Iteration 7698, loss = 0.68962961\n",
      "Iteration 7699, loss = 0.68962920\n",
      "Iteration 7700, loss = 0.68962951\n",
      "Iteration 7701, loss = 0.68962929\n",
      "Iteration 7702, loss = 0.68962916\n",
      "Iteration 7703, loss = 0.68962886\n",
      "Iteration 7704, loss = 0.68962845\n",
      "Iteration 7705, loss = 0.68962840\n",
      "Iteration 7706, loss = 0.68962802\n",
      "Iteration 7707, loss = 0.68962807\n",
      "Iteration 7708, loss = 0.68962779\n",
      "Iteration 7709, loss = 0.68962771\n",
      "Iteration 7710, loss = 0.68962730\n",
      "Iteration 7711, loss = 0.68962694\n",
      "Iteration 7712, loss = 0.68962743\n",
      "Iteration 7713, loss = 0.68962661\n",
      "Iteration 7714, loss = 0.68962662\n",
      "Iteration 7715, loss = 0.68962671\n",
      "Iteration 7716, loss = 0.68962604\n",
      "Iteration 7717, loss = 0.68962587\n",
      "Iteration 7718, loss = 0.68962581\n",
      "Iteration 7719, loss = 0.68962554\n",
      "Iteration 7720, loss = 0.68962530\n",
      "Iteration 7721, loss = 0.68962567\n",
      "Iteration 7722, loss = 0.68962510\n",
      "Iteration 7723, loss = 0.68962526\n",
      "Iteration 7724, loss = 0.68962475\n",
      "Iteration 7725, loss = 0.68962443\n",
      "Iteration 7726, loss = 0.68962499\n",
      "Iteration 7727, loss = 0.68962454\n",
      "Iteration 7728, loss = 0.68962439\n",
      "Iteration 7729, loss = 0.68962393\n",
      "Iteration 7730, loss = 0.68962374\n",
      "Iteration 7731, loss = 0.68962397\n",
      "Iteration 7732, loss = 0.68962306\n",
      "Iteration 7733, loss = 0.68962318\n",
      "Iteration 7734, loss = 0.68962336\n",
      "Iteration 7735, loss = 0.68962264\n",
      "Iteration 7736, loss = 0.68962272\n",
      "Iteration 7737, loss = 0.68962220\n",
      "Iteration 7738, loss = 0.68962262\n",
      "Iteration 7739, loss = 0.68962224\n",
      "Iteration 7740, loss = 0.68962202\n",
      "Iteration 7741, loss = 0.68962178\n",
      "Iteration 7742, loss = 0.68962144\n",
      "Iteration 7743, loss = 0.68962118\n",
      "Iteration 7744, loss = 0.68962100\n",
      "Iteration 7745, loss = 0.68962080\n",
      "Iteration 7746, loss = 0.68962124\n",
      "Iteration 7747, loss = 0.68962105\n",
      "Iteration 7748, loss = 0.68962048\n",
      "Iteration 7749, loss = 0.68962057\n",
      "Iteration 7750, loss = 0.68962037\n",
      "Iteration 7751, loss = 0.68962002\n",
      "Iteration 7752, loss = 0.68962017\n",
      "Iteration 7753, loss = 0.68961977\n",
      "Iteration 7754, loss = 0.68961942\n",
      "Iteration 7755, loss = 0.68961948\n",
      "Iteration 7756, loss = 0.68962016\n",
      "Iteration 7757, loss = 0.68961917\n",
      "Iteration 7758, loss = 0.68961894\n",
      "Iteration 7759, loss = 0.68961889\n",
      "Iteration 7760, loss = 0.68961863\n",
      "Iteration 7761, loss = 0.68961795\n",
      "Iteration 7762, loss = 0.68961784\n",
      "Iteration 7763, loss = 0.68961792\n",
      "Iteration 7764, loss = 0.68961774\n",
      "Iteration 7765, loss = 0.68961736\n",
      "Iteration 7766, loss = 0.68961776\n",
      "Iteration 7767, loss = 0.68961850\n",
      "Iteration 7768, loss = 0.68961698\n",
      "Iteration 7769, loss = 0.68961673\n",
      "Iteration 7770, loss = 0.68961673\n",
      "Iteration 7771, loss = 0.68961653\n",
      "Iteration 7772, loss = 0.68961640\n",
      "Iteration 7773, loss = 0.68961666\n",
      "Iteration 7774, loss = 0.68961602\n",
      "Iteration 7775, loss = 0.68961612\n",
      "Iteration 7776, loss = 0.68961600\n",
      "Iteration 7777, loss = 0.68961619\n",
      "Iteration 7778, loss = 0.68961594\n",
      "Iteration 7779, loss = 0.68961541\n",
      "Iteration 7780, loss = 0.68961543\n",
      "Iteration 7781, loss = 0.68961476\n",
      "Iteration 7782, loss = 0.68961524\n",
      "Iteration 7783, loss = 0.68961494\n",
      "Iteration 7784, loss = 0.68961490\n",
      "Iteration 7785, loss = 0.68961534\n",
      "Iteration 7786, loss = 0.68961440\n",
      "Iteration 7787, loss = 0.68961400\n",
      "Iteration 7788, loss = 0.68961396\n",
      "Iteration 7789, loss = 0.68961351\n",
      "Iteration 7790, loss = 0.68961346\n",
      "Iteration 7791, loss = 0.68961330\n",
      "Iteration 7792, loss = 0.68961309\n",
      "Iteration 7793, loss = 0.68961300\n",
      "Iteration 7794, loss = 0.68961295\n",
      "Iteration 7795, loss = 0.68961268\n",
      "Iteration 7796, loss = 0.68961279\n",
      "Iteration 7797, loss = 0.68961230\n",
      "Iteration 7798, loss = 0.68961256\n",
      "Iteration 7799, loss = 0.68961208\n",
      "Iteration 7800, loss = 0.68961188\n",
      "Iteration 7801, loss = 0.68961173\n",
      "Iteration 7802, loss = 0.68961154\n",
      "Iteration 7803, loss = 0.68961179\n",
      "Iteration 7804, loss = 0.68961131\n",
      "Iteration 7805, loss = 0.68961097\n",
      "Iteration 7806, loss = 0.68961152\n",
      "Iteration 7807, loss = 0.68961048\n",
      "Iteration 7808, loss = 0.68961050\n",
      "Iteration 7809, loss = 0.68961006\n",
      "Iteration 7810, loss = 0.68961048\n",
      "Iteration 7811, loss = 0.68961003\n",
      "Iteration 7812, loss = 0.68960974\n",
      "Iteration 7813, loss = 0.68960965\n",
      "Iteration 7814, loss = 0.68960960\n",
      "Iteration 7815, loss = 0.68960897\n",
      "Iteration 7816, loss = 0.68960898\n",
      "Iteration 7817, loss = 0.68960877\n",
      "Iteration 7818, loss = 0.68960871\n",
      "Iteration 7819, loss = 0.68960830\n",
      "Iteration 7820, loss = 0.68960808\n",
      "Iteration 7821, loss = 0.68960802\n",
      "Iteration 7822, loss = 0.68960775\n",
      "Iteration 7823, loss = 0.68960808\n",
      "Iteration 7824, loss = 0.68960776\n",
      "Iteration 7825, loss = 0.68960779\n",
      "Iteration 7826, loss = 0.68960710\n",
      "Iteration 7827, loss = 0.68960693\n",
      "Iteration 7828, loss = 0.68960776\n",
      "Iteration 7829, loss = 0.68960634\n",
      "Iteration 7830, loss = 0.68960655\n",
      "Iteration 7831, loss = 0.68960638\n",
      "Iteration 7832, loss = 0.68960585\n",
      "Iteration 7833, loss = 0.68960608\n",
      "Iteration 7834, loss = 0.68960578\n",
      "Iteration 7835, loss = 0.68960532\n",
      "Iteration 7836, loss = 0.68960527\n",
      "Iteration 7837, loss = 0.68960532\n",
      "Iteration 7838, loss = 0.68960550\n",
      "Iteration 7839, loss = 0.68960480\n",
      "Iteration 7840, loss = 0.68960476\n",
      "Iteration 7841, loss = 0.68960434\n",
      "Iteration 7842, loss = 0.68960427\n",
      "Iteration 7843, loss = 0.68960380\n",
      "Iteration 7844, loss = 0.68960367\n",
      "Iteration 7845, loss = 0.68960393\n",
      "Iteration 7846, loss = 0.68960370\n",
      "Iteration 7847, loss = 0.68960401\n",
      "Iteration 7848, loss = 0.68960311\n",
      "Iteration 7849, loss = 0.68960299\n",
      "Iteration 7850, loss = 0.68960303\n",
      "Iteration 7851, loss = 0.68960241\n",
      "Iteration 7852, loss = 0.68960263\n",
      "Iteration 7853, loss = 0.68960234\n",
      "Iteration 7854, loss = 0.68960199\n",
      "Iteration 7855, loss = 0.68960188\n",
      "Iteration 7856, loss = 0.68960158\n",
      "Iteration 7857, loss = 0.68960188\n",
      "Iteration 7858, loss = 0.68960145\n",
      "Iteration 7859, loss = 0.68960155\n",
      "Iteration 7860, loss = 0.68960106\n",
      "Iteration 7861, loss = 0.68960094\n",
      "Iteration 7862, loss = 0.68960078\n",
      "Iteration 7863, loss = 0.68960080\n",
      "Iteration 7864, loss = 0.68960040\n",
      "Iteration 7865, loss = 0.68960042\n",
      "Iteration 7866, loss = 0.68960018\n",
      "Iteration 7867, loss = 0.68959988\n",
      "Iteration 7868, loss = 0.68959980\n",
      "Iteration 7869, loss = 0.68959990\n",
      "Iteration 7870, loss = 0.68959958\n",
      "Iteration 7871, loss = 0.68959999\n",
      "Iteration 7872, loss = 0.68959915\n",
      "Iteration 7873, loss = 0.68959895\n",
      "Iteration 7874, loss = 0.68959864\n",
      "Iteration 7875, loss = 0.68959848\n",
      "Iteration 7876, loss = 0.68959855\n",
      "Iteration 7877, loss = 0.68959823\n",
      "Iteration 7878, loss = 0.68959849\n",
      "Iteration 7879, loss = 0.68959824\n",
      "Iteration 7880, loss = 0.68959764\n",
      "Iteration 7881, loss = 0.68959764\n",
      "Iteration 7882, loss = 0.68959822\n",
      "Iteration 7883, loss = 0.68959712\n",
      "Iteration 7884, loss = 0.68959700\n",
      "Iteration 7885, loss = 0.68959720\n",
      "Iteration 7886, loss = 0.68959672\n",
      "Iteration 7887, loss = 0.68959655\n",
      "Iteration 7888, loss = 0.68959636\n",
      "Iteration 7889, loss = 0.68959612\n",
      "Iteration 7890, loss = 0.68959606\n",
      "Iteration 7891, loss = 0.68959588\n",
      "Iteration 7892, loss = 0.68959577\n",
      "Iteration 7893, loss = 0.68959557\n",
      "Iteration 7894, loss = 0.68959531\n",
      "Iteration 7895, loss = 0.68959521\n",
      "Iteration 7896, loss = 0.68959517\n",
      "Iteration 7897, loss = 0.68959512\n",
      "Iteration 7898, loss = 0.68959486\n",
      "Iteration 7899, loss = 0.68959459\n",
      "Iteration 7900, loss = 0.68959452\n",
      "Iteration 7901, loss = 0.68959442\n",
      "Iteration 7902, loss = 0.68959512\n",
      "Iteration 7903, loss = 0.68959404\n",
      "Iteration 7904, loss = 0.68959405\n",
      "Iteration 7905, loss = 0.68959377\n",
      "Iteration 7906, loss = 0.68959432\n",
      "Iteration 7907, loss = 0.68959336\n",
      "Iteration 7908, loss = 0.68959326\n",
      "Iteration 7909, loss = 0.68959319\n",
      "Iteration 7910, loss = 0.68959301\n",
      "Iteration 7911, loss = 0.68959280\n",
      "Iteration 7912, loss = 0.68959276\n",
      "Iteration 7913, loss = 0.68959267\n",
      "Iteration 7914, loss = 0.68959228\n",
      "Iteration 7915, loss = 0.68959262\n",
      "Iteration 7916, loss = 0.68959214\n",
      "Iteration 7917, loss = 0.68959172\n",
      "Iteration 7918, loss = 0.68959189\n",
      "Iteration 7919, loss = 0.68959181\n",
      "Iteration 7920, loss = 0.68959159\n",
      "Iteration 7921, loss = 0.68959190\n",
      "Iteration 7922, loss = 0.68959108\n",
      "Iteration 7923, loss = 0.68959093\n",
      "Iteration 7924, loss = 0.68959089\n",
      "Iteration 7925, loss = 0.68959024\n",
      "Iteration 7926, loss = 0.68959034\n",
      "Iteration 7927, loss = 0.68959062\n",
      "Iteration 7928, loss = 0.68958985\n",
      "Iteration 7929, loss = 0.68958960\n",
      "Iteration 7930, loss = 0.68958978\n",
      "Iteration 7931, loss = 0.68958925\n",
      "Iteration 7932, loss = 0.68958926\n",
      "Iteration 7933, loss = 0.68959007\n",
      "Iteration 7934, loss = 0.68958892\n",
      "Iteration 7935, loss = 0.68958895\n",
      "Iteration 7936, loss = 0.68958918\n",
      "Iteration 7937, loss = 0.68958892\n",
      "Iteration 7938, loss = 0.68958828\n",
      "Iteration 7939, loss = 0.68958814\n",
      "Iteration 7940, loss = 0.68958799\n",
      "Iteration 7941, loss = 0.68958831\n",
      "Iteration 7942, loss = 0.68958778\n",
      "Iteration 7943, loss = 0.68958778\n",
      "Iteration 7944, loss = 0.68958714\n",
      "Iteration 7945, loss = 0.68958746\n",
      "Iteration 7946, loss = 0.68958719\n",
      "Iteration 7947, loss = 0.68958695\n",
      "Iteration 7948, loss = 0.68958679\n",
      "Iteration 7949, loss = 0.68958636\n",
      "Iteration 7950, loss = 0.68958598\n",
      "Iteration 7951, loss = 0.68958588\n",
      "Iteration 7952, loss = 0.68958591\n",
      "Iteration 7953, loss = 0.68958593\n",
      "Iteration 7954, loss = 0.68958664\n",
      "Iteration 7955, loss = 0.68958554\n",
      "Iteration 7956, loss = 0.68958528\n",
      "Iteration 7957, loss = 0.68958502\n",
      "Iteration 7958, loss = 0.68958554\n",
      "Iteration 7959, loss = 0.68958470\n",
      "Iteration 7960, loss = 0.68958447\n",
      "Iteration 7961, loss = 0.68958423\n",
      "Iteration 7962, loss = 0.68958438\n",
      "Iteration 7963, loss = 0.68958401\n",
      "Iteration 7964, loss = 0.68958432\n",
      "Iteration 7965, loss = 0.68958356\n",
      "Iteration 7966, loss = 0.68958355\n",
      "Iteration 7967, loss = 0.68958370\n",
      "Iteration 7968, loss = 0.68958390\n",
      "Iteration 7969, loss = 0.68958384\n",
      "Iteration 7970, loss = 0.68958310\n",
      "Iteration 7971, loss = 0.68958258\n",
      "Iteration 7972, loss = 0.68958249\n",
      "Iteration 7973, loss = 0.68958235\n",
      "Iteration 7974, loss = 0.68958230\n",
      "Iteration 7975, loss = 0.68958203\n",
      "Iteration 7976, loss = 0.68958216\n",
      "Iteration 7977, loss = 0.68958169\n",
      "Iteration 7978, loss = 0.68958146\n",
      "Iteration 7979, loss = 0.68958124\n",
      "Iteration 7980, loss = 0.68958155\n",
      "Iteration 7981, loss = 0.68958124\n",
      "Iteration 7982, loss = 0.68958115\n",
      "Iteration 7983, loss = 0.68958060\n",
      "Iteration 7984, loss = 0.68958065\n",
      "Iteration 7985, loss = 0.68958043\n",
      "Iteration 7986, loss = 0.68958034\n",
      "Iteration 7987, loss = 0.68958030\n",
      "Iteration 7988, loss = 0.68958025\n",
      "Iteration 7989, loss = 0.68957977\n",
      "Iteration 7990, loss = 0.68957982\n",
      "Iteration 7991, loss = 0.68957955\n",
      "Iteration 7992, loss = 0.68957929\n",
      "Iteration 7993, loss = 0.68957896\n",
      "Iteration 7994, loss = 0.68957882\n",
      "Iteration 7995, loss = 0.68957861\n",
      "Iteration 7996, loss = 0.68957840\n",
      "Iteration 7997, loss = 0.68957925\n",
      "Iteration 7998, loss = 0.68957938\n",
      "Iteration 7999, loss = 0.68957831\n",
      "Iteration 8000, loss = 0.68957805\n",
      "Iteration 8001, loss = 0.68957761\n",
      "Iteration 8002, loss = 0.68957775\n",
      "Iteration 8003, loss = 0.68957715\n",
      "Iteration 8004, loss = 0.68957721\n",
      "Iteration 8005, loss = 0.68957744\n",
      "Iteration 8006, loss = 0.68957701\n",
      "Iteration 8007, loss = 0.68957684\n",
      "Iteration 8008, loss = 0.68957696\n",
      "Iteration 8009, loss = 0.68957715\n",
      "Iteration 8010, loss = 0.68957627\n",
      "Iteration 8011, loss = 0.68957584\n",
      "Iteration 8012, loss = 0.68957602\n",
      "Iteration 8013, loss = 0.68957628\n",
      "Iteration 8014, loss = 0.68957578\n",
      "Iteration 8015, loss = 0.68957538\n",
      "Iteration 8016, loss = 0.68957534\n",
      "Iteration 8017, loss = 0.68957511\n",
      "Iteration 8018, loss = 0.68957620\n",
      "Iteration 8019, loss = 0.68957537\n",
      "Iteration 8020, loss = 0.68957479\n",
      "Iteration 8021, loss = 0.68957442\n",
      "Iteration 8022, loss = 0.68957452\n",
      "Iteration 8023, loss = 0.68957442\n",
      "Iteration 8024, loss = 0.68957403\n",
      "Iteration 8025, loss = 0.68957430\n",
      "Iteration 8026, loss = 0.68957378\n",
      "Iteration 8027, loss = 0.68957378\n",
      "Iteration 8028, loss = 0.68957328\n",
      "Iteration 8029, loss = 0.68957304\n",
      "Iteration 8030, loss = 0.68957284\n",
      "Iteration 8031, loss = 0.68957285\n",
      "Iteration 8032, loss = 0.68957297\n",
      "Iteration 8033, loss = 0.68957240\n",
      "Iteration 8034, loss = 0.68957233\n",
      "Iteration 8035, loss = 0.68957213\n",
      "Iteration 8036, loss = 0.68957207\n",
      "Iteration 8037, loss = 0.68957184\n",
      "Iteration 8038, loss = 0.68957178\n",
      "Iteration 8039, loss = 0.68957125\n",
      "Iteration 8040, loss = 0.68957192\n",
      "Iteration 8041, loss = 0.68957120\n",
      "Iteration 8042, loss = 0.68957091\n",
      "Iteration 8043, loss = 0.68957129\n",
      "Iteration 8044, loss = 0.68957089\n",
      "Iteration 8045, loss = 0.68957085\n",
      "Iteration 8046, loss = 0.68957087\n",
      "Iteration 8047, loss = 0.68957028\n",
      "Iteration 8048, loss = 0.68957057\n",
      "Iteration 8049, loss = 0.68957080\n",
      "Iteration 8050, loss = 0.68956977\n",
      "Iteration 8051, loss = 0.68956956\n",
      "Iteration 8052, loss = 0.68956971\n",
      "Iteration 8053, loss = 0.68956949\n",
      "Iteration 8054, loss = 0.68956919\n",
      "Iteration 8055, loss = 0.68956893\n",
      "Iteration 8056, loss = 0.68956892\n",
      "Iteration 8057, loss = 0.68956839\n",
      "Iteration 8058, loss = 0.68956866\n",
      "Iteration 8059, loss = 0.68956866\n",
      "Iteration 8060, loss = 0.68956815\n",
      "Iteration 8061, loss = 0.68956835\n",
      "Iteration 8062, loss = 0.68956900\n",
      "Iteration 8063, loss = 0.68956812\n",
      "Iteration 8064, loss = 0.68956734\n",
      "Iteration 8065, loss = 0.68956829\n",
      "Iteration 8066, loss = 0.68956765\n",
      "Iteration 8067, loss = 0.68956712\n",
      "Iteration 8068, loss = 0.68956677\n",
      "Iteration 8069, loss = 0.68956707\n",
      "Iteration 8070, loss = 0.68956658\n",
      "Iteration 8071, loss = 0.68956665\n",
      "Iteration 8072, loss = 0.68956638\n",
      "Iteration 8073, loss = 0.68956608\n",
      "Iteration 8074, loss = 0.68956685\n",
      "Iteration 8075, loss = 0.68956619\n",
      "Iteration 8076, loss = 0.68956635\n",
      "Iteration 8077, loss = 0.68956540\n",
      "Iteration 8078, loss = 0.68956539\n",
      "Iteration 8079, loss = 0.68956523\n",
      "Iteration 8080, loss = 0.68956490\n",
      "Iteration 8081, loss = 0.68956467\n",
      "Iteration 8082, loss = 0.68956519\n",
      "Iteration 8083, loss = 0.68956484\n",
      "Iteration 8084, loss = 0.68956521\n",
      "Iteration 8085, loss = 0.68956421\n",
      "Iteration 8086, loss = 0.68956386\n",
      "Iteration 8087, loss = 0.68956447\n",
      "Iteration 8088, loss = 0.68956440\n",
      "Iteration 8089, loss = 0.68956396\n",
      "Iteration 8090, loss = 0.68956363\n",
      "Iteration 8091, loss = 0.68956286\n",
      "Iteration 8092, loss = 0.68956289\n",
      "Iteration 8093, loss = 0.68956296\n",
      "Iteration 8094, loss = 0.68956293\n",
      "Iteration 8095, loss = 0.68956289\n",
      "Iteration 8096, loss = 0.68956259\n",
      "Iteration 8097, loss = 0.68956238\n",
      "Iteration 8098, loss = 0.68956233\n",
      "Iteration 8099, loss = 0.68956273\n",
      "Iteration 8100, loss = 0.68956196\n",
      "Iteration 8101, loss = 0.68956175\n",
      "Iteration 8102, loss = 0.68956151\n",
      "Iteration 8103, loss = 0.68956215\n",
      "Iteration 8104, loss = 0.68956114\n",
      "Iteration 8105, loss = 0.68956189\n",
      "Iteration 8106, loss = 0.68956148\n",
      "Iteration 8107, loss = 0.68956098\n",
      "Iteration 8108, loss = 0.68956081\n",
      "Iteration 8109, loss = 0.68956112\n",
      "Iteration 8110, loss = 0.68956079\n",
      "Iteration 8111, loss = 0.68956025\n",
      "Iteration 8112, loss = 0.68956010\n",
      "Iteration 8113, loss = 0.68955996\n",
      "Iteration 8114, loss = 0.68956023\n",
      "Iteration 8115, loss = 0.68955939\n",
      "Iteration 8116, loss = 0.68956039\n",
      "Iteration 8117, loss = 0.68955957\n",
      "Iteration 8118, loss = 0.68955909\n",
      "Iteration 8119, loss = 0.68955952\n",
      "Iteration 8120, loss = 0.68955885\n",
      "Iteration 8121, loss = 0.68955860\n",
      "Iteration 8122, loss = 0.68955855\n",
      "Iteration 8123, loss = 0.68955859\n",
      "Iteration 8124, loss = 0.68955821\n",
      "Iteration 8125, loss = 0.68955789\n",
      "Iteration 8126, loss = 0.68955814\n",
      "Iteration 8127, loss = 0.68955769\n",
      "Iteration 8128, loss = 0.68955750\n",
      "Iteration 8129, loss = 0.68955766\n",
      "Iteration 8130, loss = 0.68955722\n",
      "Iteration 8131, loss = 0.68955707\n",
      "Iteration 8132, loss = 0.68955711\n",
      "Iteration 8133, loss = 0.68955695\n",
      "Iteration 8134, loss = 0.68955738\n",
      "Iteration 8135, loss = 0.68955689\n",
      "Iteration 8136, loss = 0.68955648\n",
      "Iteration 8137, loss = 0.68955612\n",
      "Iteration 8138, loss = 0.68955594\n",
      "Iteration 8139, loss = 0.68955599\n",
      "Iteration 8140, loss = 0.68955551\n",
      "Iteration 8141, loss = 0.68955547\n",
      "Iteration 8142, loss = 0.68955544\n",
      "Iteration 8143, loss = 0.68955530\n",
      "Iteration 8144, loss = 0.68955615\n",
      "Iteration 8145, loss = 0.68955486\n",
      "Iteration 8146, loss = 0.68955498\n",
      "Iteration 8147, loss = 0.68955497\n",
      "Iteration 8148, loss = 0.68955527\n",
      "Iteration 8149, loss = 0.68955474\n",
      "Iteration 8150, loss = 0.68955434\n",
      "Iteration 8151, loss = 0.68955432\n",
      "Iteration 8152, loss = 0.68955416\n",
      "Iteration 8153, loss = 0.68955402\n",
      "Iteration 8154, loss = 0.68955371\n",
      "Iteration 8155, loss = 0.68955354\n",
      "Iteration 8156, loss = 0.68955333\n",
      "Iteration 8157, loss = 0.68955376\n",
      "Iteration 8158, loss = 0.68955270\n",
      "Iteration 8159, loss = 0.68955324\n",
      "Iteration 8160, loss = 0.68955291\n",
      "Iteration 8161, loss = 0.68955242\n",
      "Iteration 8162, loss = 0.68955248\n",
      "Iteration 8163, loss = 0.68955250\n",
      "Iteration 8164, loss = 0.68955223\n",
      "Iteration 8165, loss = 0.68955209\n",
      "Iteration 8166, loss = 0.68955158\n",
      "Iteration 8167, loss = 0.68955170\n",
      "Iteration 8168, loss = 0.68955114\n",
      "Iteration 8169, loss = 0.68955117\n",
      "Iteration 8170, loss = 0.68955080\n",
      "Iteration 8171, loss = 0.68955090\n",
      "Iteration 8172, loss = 0.68955089\n",
      "Iteration 8173, loss = 0.68955039\n",
      "Iteration 8174, loss = 0.68955039\n",
      "Iteration 8175, loss = 0.68955024\n",
      "Iteration 8176, loss = 0.68955023\n",
      "Iteration 8177, loss = 0.68954983\n",
      "Iteration 8178, loss = 0.68954986\n",
      "Iteration 8179, loss = 0.68954992\n",
      "Iteration 8180, loss = 0.68954937\n",
      "Iteration 8181, loss = 0.68954932\n",
      "Iteration 8182, loss = 0.68954951\n",
      "Iteration 8183, loss = 0.68954925\n",
      "Iteration 8184, loss = 0.68954917\n",
      "Iteration 8185, loss = 0.68954889\n",
      "Iteration 8186, loss = 0.68954838\n",
      "Iteration 8187, loss = 0.68954828\n",
      "Iteration 8188, loss = 0.68954811\n",
      "Iteration 8189, loss = 0.68954788\n",
      "Iteration 8190, loss = 0.68954780\n",
      "Iteration 8191, loss = 0.68954798\n",
      "Iteration 8192, loss = 0.68954768\n",
      "Iteration 8193, loss = 0.68954757\n",
      "Iteration 8194, loss = 0.68954726\n",
      "Iteration 8195, loss = 0.68954708\n",
      "Iteration 8196, loss = 0.68954711\n",
      "Iteration 8197, loss = 0.68954665\n",
      "Iteration 8198, loss = 0.68954684\n",
      "Iteration 8199, loss = 0.68954637\n",
      "Iteration 8200, loss = 0.68954649\n",
      "Iteration 8201, loss = 0.68954602\n",
      "Iteration 8202, loss = 0.68954675\n",
      "Iteration 8203, loss = 0.68954593\n",
      "Iteration 8204, loss = 0.68954546\n",
      "Iteration 8205, loss = 0.68954540\n",
      "Iteration 8206, loss = 0.68954567\n",
      "Iteration 8207, loss = 0.68954538\n",
      "Iteration 8208, loss = 0.68954499\n",
      "Iteration 8209, loss = 0.68954448\n",
      "Iteration 8210, loss = 0.68954466\n",
      "Iteration 8211, loss = 0.68954473\n",
      "Iteration 8212, loss = 0.68954452\n",
      "Iteration 8213, loss = 0.68954390\n",
      "Iteration 8214, loss = 0.68954380\n",
      "Iteration 8215, loss = 0.68954388\n",
      "Iteration 8216, loss = 0.68954327\n",
      "Iteration 8217, loss = 0.68954345\n",
      "Iteration 8218, loss = 0.68954310\n",
      "Iteration 8219, loss = 0.68954326\n",
      "Iteration 8220, loss = 0.68954267\n",
      "Iteration 8221, loss = 0.68954303\n",
      "Iteration 8222, loss = 0.68954248\n",
      "Iteration 8223, loss = 0.68954236\n",
      "Iteration 8224, loss = 0.68954201\n",
      "Iteration 8225, loss = 0.68954243\n",
      "Iteration 8226, loss = 0.68954173\n",
      "Iteration 8227, loss = 0.68954170\n",
      "Iteration 8228, loss = 0.68954161\n",
      "Iteration 8229, loss = 0.68954238\n",
      "Iteration 8230, loss = 0.68954123\n",
      "Iteration 8231, loss = 0.68954092\n",
      "Iteration 8232, loss = 0.68954138\n",
      "Iteration 8233, loss = 0.68954039\n",
      "Iteration 8234, loss = 0.68954040\n",
      "Iteration 8235, loss = 0.68954068\n",
      "Iteration 8236, loss = 0.68954009\n",
      "Iteration 8237, loss = 0.68954058\n",
      "Iteration 8238, loss = 0.68953964\n",
      "Iteration 8239, loss = 0.68953976\n",
      "Iteration 8240, loss = 0.68953943\n",
      "Iteration 8241, loss = 0.68954023\n",
      "Iteration 8242, loss = 0.68953940\n",
      "Iteration 8243, loss = 0.68953907\n",
      "Iteration 8244, loss = 0.68953851\n",
      "Iteration 8245, loss = 0.68953852\n",
      "Iteration 8246, loss = 0.68953851\n",
      "Iteration 8247, loss = 0.68953864\n",
      "Iteration 8248, loss = 0.68953844\n",
      "Iteration 8249, loss = 0.68953803\n",
      "Iteration 8250, loss = 0.68953811\n",
      "Iteration 8251, loss = 0.68953777\n",
      "Iteration 8252, loss = 0.68953762\n",
      "Iteration 8253, loss = 0.68953750\n",
      "Iteration 8254, loss = 0.68953751\n",
      "Iteration 8255, loss = 0.68953691\n",
      "Iteration 8256, loss = 0.68953678\n",
      "Iteration 8257, loss = 0.68953682\n",
      "Iteration 8258, loss = 0.68953713\n",
      "Iteration 8259, loss = 0.68953627\n",
      "Iteration 8260, loss = 0.68953649\n",
      "Iteration 8261, loss = 0.68953602\n",
      "Iteration 8262, loss = 0.68953611\n",
      "Iteration 8263, loss = 0.68953587\n",
      "Iteration 8264, loss = 0.68953566\n",
      "Iteration 8265, loss = 0.68953538\n",
      "Iteration 8266, loss = 0.68953542\n",
      "Iteration 8267, loss = 0.68953531\n",
      "Iteration 8268, loss = 0.68953530\n",
      "Iteration 8269, loss = 0.68953517\n",
      "Iteration 8270, loss = 0.68953431\n",
      "Iteration 8271, loss = 0.68953442\n",
      "Iteration 8272, loss = 0.68953447\n",
      "Iteration 8273, loss = 0.68953427\n",
      "Iteration 8274, loss = 0.68953438\n",
      "Iteration 8275, loss = 0.68953386\n",
      "Iteration 8276, loss = 0.68953398\n",
      "Iteration 8277, loss = 0.68953351\n",
      "Iteration 8278, loss = 0.68953341\n",
      "Iteration 8279, loss = 0.68953349\n",
      "Iteration 8280, loss = 0.68953280\n",
      "Iteration 8281, loss = 0.68953314\n",
      "Iteration 8282, loss = 0.68953275\n",
      "Iteration 8283, loss = 0.68953268\n",
      "Iteration 8284, loss = 0.68953250\n",
      "Iteration 8285, loss = 0.68953298\n",
      "Iteration 8286, loss = 0.68953247\n",
      "Iteration 8287, loss = 0.68953204\n",
      "Iteration 8288, loss = 0.68953235\n",
      "Iteration 8289, loss = 0.68953195\n",
      "Iteration 8290, loss = 0.68953169\n",
      "Iteration 8291, loss = 0.68953137\n",
      "Iteration 8292, loss = 0.68953109\n",
      "Iteration 8293, loss = 0.68953131\n",
      "Iteration 8294, loss = 0.68953097\n",
      "Iteration 8295, loss = 0.68953101\n",
      "Iteration 8296, loss = 0.68953045\n",
      "Iteration 8297, loss = 0.68953044\n",
      "Iteration 8298, loss = 0.68953089\n",
      "Iteration 8299, loss = 0.68953003\n",
      "Iteration 8300, loss = 0.68952979\n",
      "Iteration 8301, loss = 0.68953004\n",
      "Iteration 8302, loss = 0.68953024\n",
      "Iteration 8303, loss = 0.68952974\n",
      "Iteration 8304, loss = 0.68952955\n",
      "Iteration 8305, loss = 0.68952909\n",
      "Iteration 8306, loss = 0.68952862\n",
      "Iteration 8307, loss = 0.68952869\n",
      "Iteration 8308, loss = 0.68952883\n",
      "Iteration 8309, loss = 0.68952835\n",
      "Iteration 8310, loss = 0.68952818\n",
      "Iteration 8311, loss = 0.68952844\n",
      "Iteration 8312, loss = 0.68952806\n",
      "Iteration 8313, loss = 0.68952822\n",
      "Iteration 8314, loss = 0.68952744\n",
      "Iteration 8315, loss = 0.68952726\n",
      "Iteration 8316, loss = 0.68952705\n",
      "Iteration 8317, loss = 0.68952706\n",
      "Iteration 8318, loss = 0.68952679\n",
      "Iteration 8319, loss = 0.68952765\n",
      "Iteration 8320, loss = 0.68952688\n",
      "Iteration 8321, loss = 0.68952619\n",
      "Iteration 8322, loss = 0.68952622\n",
      "Iteration 8323, loss = 0.68952581\n",
      "Iteration 8324, loss = 0.68952594\n",
      "Iteration 8325, loss = 0.68952602\n",
      "Iteration 8326, loss = 0.68952556\n",
      "Iteration 8327, loss = 0.68952542\n",
      "Iteration 8328, loss = 0.68952533\n",
      "Iteration 8329, loss = 0.68952509\n",
      "Iteration 8330, loss = 0.68952457\n",
      "Iteration 8331, loss = 0.68952465\n",
      "Iteration 8332, loss = 0.68952437\n",
      "Iteration 8333, loss = 0.68952445\n",
      "Iteration 8334, loss = 0.68952452\n",
      "Iteration 8335, loss = 0.68952416\n",
      "Iteration 8336, loss = 0.68952400\n",
      "Iteration 8337, loss = 0.68952372\n",
      "Iteration 8338, loss = 0.68952418\n",
      "Iteration 8339, loss = 0.68952361\n",
      "Iteration 8340, loss = 0.68952320\n",
      "Iteration 8341, loss = 0.68952306\n",
      "Iteration 8342, loss = 0.68952296\n",
      "Iteration 8343, loss = 0.68952334\n",
      "Iteration 8344, loss = 0.68952322\n",
      "Iteration 8345, loss = 0.68952266\n",
      "Iteration 8346, loss = 0.68952272\n",
      "Iteration 8347, loss = 0.68952217\n",
      "Iteration 8348, loss = 0.68952216\n",
      "Iteration 8349, loss = 0.68952181\n",
      "Iteration 8350, loss = 0.68952176\n",
      "Iteration 8351, loss = 0.68952178\n",
      "Iteration 8352, loss = 0.68952146\n",
      "Iteration 8353, loss = 0.68952139\n",
      "Iteration 8354, loss = 0.68952103\n",
      "Iteration 8355, loss = 0.68952142\n",
      "Iteration 8356, loss = 0.68952117\n",
      "Iteration 8357, loss = 0.68952052\n",
      "Iteration 8358, loss = 0.68952085\n",
      "Iteration 8359, loss = 0.68952026\n",
      "Iteration 8360, loss = 0.68952031\n",
      "Iteration 8361, loss = 0.68952038\n",
      "Iteration 8362, loss = 0.68951997\n",
      "Iteration 8363, loss = 0.68952036\n",
      "Iteration 8364, loss = 0.68951965\n",
      "Iteration 8365, loss = 0.68951955\n",
      "Iteration 8366, loss = 0.68951933\n",
      "Iteration 8367, loss = 0.68951943\n",
      "Iteration 8368, loss = 0.68951878\n",
      "Iteration 8369, loss = 0.68951881\n",
      "Iteration 8370, loss = 0.68951896\n",
      "Iteration 8371, loss = 0.68951841\n",
      "Iteration 8372, loss = 0.68951835\n",
      "Iteration 8373, loss = 0.68951844\n",
      "Iteration 8374, loss = 0.68951805\n",
      "Iteration 8375, loss = 0.68951771\n",
      "Iteration 8376, loss = 0.68951766\n",
      "Iteration 8377, loss = 0.68951751\n",
      "Iteration 8378, loss = 0.68951717\n",
      "Iteration 8379, loss = 0.68951767\n",
      "Iteration 8380, loss = 0.68951838\n",
      "Iteration 8381, loss = 0.68951709\n",
      "Iteration 8382, loss = 0.68951696\n",
      "Iteration 8383, loss = 0.68951674\n",
      "Iteration 8384, loss = 0.68951700\n",
      "Iteration 8385, loss = 0.68951625\n",
      "Iteration 8386, loss = 0.68951648\n",
      "Iteration 8387, loss = 0.68951641\n",
      "Iteration 8388, loss = 0.68951589\n",
      "Iteration 8389, loss = 0.68951605\n",
      "Iteration 8390, loss = 0.68951571\n",
      "Iteration 8391, loss = 0.68951585\n",
      "Iteration 8392, loss = 0.68951564\n",
      "Iteration 8393, loss = 0.68951626\n",
      "Iteration 8394, loss = 0.68951529\n",
      "Iteration 8395, loss = 0.68951545\n",
      "Iteration 8396, loss = 0.68951522\n",
      "Iteration 8397, loss = 0.68951466\n",
      "Iteration 8398, loss = 0.68951499\n",
      "Iteration 8399, loss = 0.68951470\n",
      "Iteration 8400, loss = 0.68951466\n",
      "Iteration 8401, loss = 0.68951446\n",
      "Iteration 8402, loss = 0.68951392\n",
      "Iteration 8403, loss = 0.68951390\n",
      "Iteration 8404, loss = 0.68951373\n",
      "Iteration 8405, loss = 0.68951352\n",
      "Iteration 8406, loss = 0.68951366\n",
      "Iteration 8407, loss = 0.68951340\n",
      "Iteration 8408, loss = 0.68951304\n",
      "Iteration 8409, loss = 0.68951285\n",
      "Iteration 8410, loss = 0.68951297\n",
      "Iteration 8411, loss = 0.68951277\n",
      "Iteration 8412, loss = 0.68951279\n",
      "Iteration 8413, loss = 0.68951239\n",
      "Iteration 8414, loss = 0.68951329\n",
      "Iteration 8415, loss = 0.68951212\n",
      "Iteration 8416, loss = 0.68951221\n",
      "Iteration 8417, loss = 0.68951167\n",
      "Iteration 8418, loss = 0.68951164\n",
      "Iteration 8419, loss = 0.68951165\n",
      "Iteration 8420, loss = 0.68951157\n",
      "Iteration 8421, loss = 0.68951156\n",
      "Iteration 8422, loss = 0.68951113\n",
      "Iteration 8423, loss = 0.68951090\n",
      "Iteration 8424, loss = 0.68951075\n",
      "Iteration 8425, loss = 0.68951065\n",
      "Iteration 8426, loss = 0.68951072\n",
      "Iteration 8427, loss = 0.68951043\n",
      "Iteration 8428, loss = 0.68951061\n",
      "Iteration 8429, loss = 0.68951026\n",
      "Iteration 8430, loss = 0.68951014\n",
      "Iteration 8431, loss = 0.68951050\n",
      "Iteration 8432, loss = 0.68950975\n",
      "Iteration 8433, loss = 0.68950952\n",
      "Iteration 8434, loss = 0.68950971\n",
      "Iteration 8435, loss = 0.68950956\n",
      "Iteration 8436, loss = 0.68950908\n",
      "Iteration 8437, loss = 0.68950949\n",
      "Iteration 8438, loss = 0.68950883\n",
      "Iteration 8439, loss = 0.68950876\n",
      "Iteration 8440, loss = 0.68950847\n",
      "Iteration 8441, loss = 0.68950820\n",
      "Iteration 8442, loss = 0.68950813\n",
      "Iteration 8443, loss = 0.68950799\n",
      "Iteration 8444, loss = 0.68950801\n",
      "Iteration 8445, loss = 0.68950757\n",
      "Iteration 8446, loss = 0.68950771\n",
      "Iteration 8447, loss = 0.68950787\n",
      "Iteration 8448, loss = 0.68950688\n",
      "Iteration 8449, loss = 0.68950769\n",
      "Iteration 8450, loss = 0.68950685\n",
      "Iteration 8451, loss = 0.68950663\n",
      "Iteration 8452, loss = 0.68950658\n",
      "Iteration 8453, loss = 0.68950675\n",
      "Iteration 8454, loss = 0.68950651\n",
      "Iteration 8455, loss = 0.68950627\n",
      "Iteration 8456, loss = 0.68950595\n",
      "Iteration 8457, loss = 0.68950639\n",
      "Iteration 8458, loss = 0.68950540\n",
      "Iteration 8459, loss = 0.68950567\n",
      "Iteration 8460, loss = 0.68950575\n",
      "Iteration 8461, loss = 0.68950524\n",
      "Iteration 8462, loss = 0.68950498\n",
      "Iteration 8463, loss = 0.68950485\n",
      "Iteration 8464, loss = 0.68950485\n",
      "Iteration 8465, loss = 0.68950436\n",
      "Iteration 8466, loss = 0.68950452\n",
      "Iteration 8467, loss = 0.68950494\n",
      "Iteration 8468, loss = 0.68950430\n",
      "Iteration 8469, loss = 0.68950401\n",
      "Iteration 8470, loss = 0.68950380\n",
      "Iteration 8471, loss = 0.68950346\n",
      "Iteration 8472, loss = 0.68950354\n",
      "Iteration 8473, loss = 0.68950332\n",
      "Iteration 8474, loss = 0.68950378\n",
      "Iteration 8475, loss = 0.68950329\n",
      "Iteration 8476, loss = 0.68950281\n",
      "Iteration 8477, loss = 0.68950265\n",
      "Iteration 8478, loss = 0.68950257\n",
      "Iteration 8479, loss = 0.68950258\n",
      "Iteration 8480, loss = 0.68950234\n",
      "Iteration 8481, loss = 0.68950228\n",
      "Iteration 8482, loss = 0.68950199\n",
      "Iteration 8483, loss = 0.68950224\n",
      "Iteration 8484, loss = 0.68950149\n",
      "Iteration 8485, loss = 0.68950175\n",
      "Iteration 8486, loss = 0.68950122\n",
      "Iteration 8487, loss = 0.68950155\n",
      "Iteration 8488, loss = 0.68950118\n",
      "Iteration 8489, loss = 0.68950095\n",
      "Iteration 8490, loss = 0.68950059\n",
      "Iteration 8491, loss = 0.68950055\n",
      "Iteration 8492, loss = 0.68950070\n",
      "Iteration 8493, loss = 0.68950067\n",
      "Iteration 8494, loss = 0.68950034\n",
      "Iteration 8495, loss = 0.68950025\n",
      "Iteration 8496, loss = 0.68950018\n",
      "Iteration 8497, loss = 0.68949981\n",
      "Iteration 8498, loss = 0.68949945\n",
      "Iteration 8499, loss = 0.68949935\n",
      "Iteration 8500, loss = 0.68949956\n",
      "Iteration 8501, loss = 0.68950122\n",
      "Iteration 8502, loss = 0.68949907\n",
      "Iteration 8503, loss = 0.68949926\n",
      "Iteration 8504, loss = 0.68949871\n",
      "Iteration 8505, loss = 0.68949930\n",
      "Iteration 8506, loss = 0.68949868\n",
      "Iteration 8507, loss = 0.68949842\n",
      "Iteration 8508, loss = 0.68949785\n",
      "Iteration 8509, loss = 0.68949809\n",
      "Iteration 8510, loss = 0.68949754\n",
      "Iteration 8511, loss = 0.68949770\n",
      "Iteration 8512, loss = 0.68949737\n",
      "Iteration 8513, loss = 0.68949718\n",
      "Iteration 8514, loss = 0.68949733\n",
      "Iteration 8515, loss = 0.68949698\n",
      "Iteration 8516, loss = 0.68949693\n",
      "Iteration 8517, loss = 0.68949667\n",
      "Iteration 8518, loss = 0.68949696\n",
      "Iteration 8519, loss = 0.68949648\n",
      "Iteration 8520, loss = 0.68949658\n",
      "Iteration 8521, loss = 0.68949604\n",
      "Iteration 8522, loss = 0.68949582\n",
      "Iteration 8523, loss = 0.68949636\n",
      "Iteration 8524, loss = 0.68949565\n",
      "Iteration 8525, loss = 0.68949549\n",
      "Iteration 8526, loss = 0.68949520\n",
      "Iteration 8527, loss = 0.68949499\n",
      "Iteration 8528, loss = 0.68949512\n",
      "Iteration 8529, loss = 0.68949479\n",
      "Iteration 8530, loss = 0.68949480\n",
      "Iteration 8531, loss = 0.68949440\n",
      "Iteration 8532, loss = 0.68949511\n",
      "Iteration 8533, loss = 0.68949463\n",
      "Iteration 8534, loss = 0.68949425\n",
      "Iteration 8535, loss = 0.68949392\n",
      "Iteration 8536, loss = 0.68949384\n",
      "Iteration 8537, loss = 0.68949367\n",
      "Iteration 8538, loss = 0.68949383\n",
      "Iteration 8539, loss = 0.68949364\n",
      "Iteration 8540, loss = 0.68949334\n",
      "Iteration 8541, loss = 0.68949333\n",
      "Iteration 8542, loss = 0.68949327\n",
      "Iteration 8543, loss = 0.68949326\n",
      "Iteration 8544, loss = 0.68949260\n",
      "Iteration 8545, loss = 0.68949311\n",
      "Iteration 8546, loss = 0.68949321\n",
      "Iteration 8547, loss = 0.68949238\n",
      "Iteration 8548, loss = 0.68949208\n",
      "Iteration 8549, loss = 0.68949233\n",
      "Iteration 8550, loss = 0.68949214\n",
      "Iteration 8551, loss = 0.68949167\n",
      "Iteration 8552, loss = 0.68949199\n",
      "Iteration 8553, loss = 0.68949162\n",
      "Iteration 8554, loss = 0.68949131\n",
      "Iteration 8555, loss = 0.68949167\n",
      "Iteration 8556, loss = 0.68949151\n",
      "Iteration 8557, loss = 0.68949105\n",
      "Iteration 8558, loss = 0.68949097\n",
      "Iteration 8559, loss = 0.68949079\n",
      "Iteration 8560, loss = 0.68949052\n",
      "Iteration 8561, loss = 0.68949089\n",
      "Iteration 8562, loss = 0.68949015\n",
      "Iteration 8563, loss = 0.68949021\n",
      "Iteration 8564, loss = 0.68949001\n",
      "Iteration 8565, loss = 0.68948960\n",
      "Iteration 8566, loss = 0.68948957\n",
      "Iteration 8567, loss = 0.68948935\n",
      "Iteration 8568, loss = 0.68948921\n",
      "Iteration 8569, loss = 0.68948915\n",
      "Iteration 8570, loss = 0.68948900\n",
      "Iteration 8571, loss = 0.68948888\n",
      "Iteration 8572, loss = 0.68948924\n",
      "Iteration 8573, loss = 0.68948871\n",
      "Iteration 8574, loss = 0.68948833\n",
      "Iteration 8575, loss = 0.68948831\n",
      "Iteration 8576, loss = 0.68948826\n",
      "Iteration 8577, loss = 0.68948817\n",
      "Iteration 8578, loss = 0.68948825\n",
      "Iteration 8579, loss = 0.68948781\n",
      "Iteration 8580, loss = 0.68948733\n",
      "Iteration 8581, loss = 0.68948729\n",
      "Iteration 8582, loss = 0.68948735\n",
      "Iteration 8583, loss = 0.68948670\n",
      "Iteration 8584, loss = 0.68948708\n",
      "Iteration 8585, loss = 0.68948704\n",
      "Iteration 8586, loss = 0.68948716\n",
      "Iteration 8587, loss = 0.68948634\n",
      "Iteration 8588, loss = 0.68948607\n",
      "Iteration 8589, loss = 0.68948623\n",
      "Iteration 8590, loss = 0.68948604\n",
      "Iteration 8591, loss = 0.68948572\n",
      "Iteration 8592, loss = 0.68948560\n",
      "Iteration 8593, loss = 0.68948644\n",
      "Iteration 8594, loss = 0.68948520\n",
      "Iteration 8595, loss = 0.68948520\n",
      "Iteration 8596, loss = 0.68948498\n",
      "Iteration 8597, loss = 0.68948482\n",
      "Iteration 8598, loss = 0.68948481\n",
      "Iteration 8599, loss = 0.68948443\n",
      "Iteration 8600, loss = 0.68948463\n",
      "Iteration 8601, loss = 0.68948433\n",
      "Iteration 8602, loss = 0.68948393\n",
      "Iteration 8603, loss = 0.68948476\n",
      "Iteration 8604, loss = 0.68948419\n",
      "Iteration 8605, loss = 0.68948394\n",
      "Iteration 8606, loss = 0.68948382\n",
      "Iteration 8607, loss = 0.68948384\n",
      "Iteration 8608, loss = 0.68948348\n",
      "Iteration 8609, loss = 0.68948302\n",
      "Iteration 8610, loss = 0.68948333\n",
      "Iteration 8611, loss = 0.68948292\n",
      "Iteration 8612, loss = 0.68948263\n",
      "Iteration 8613, loss = 0.68948263\n",
      "Iteration 8614, loss = 0.68948320\n",
      "Iteration 8615, loss = 0.68948264\n",
      "Iteration 8616, loss = 0.68948286\n",
      "Iteration 8617, loss = 0.68948199\n",
      "Iteration 8618, loss = 0.68948263\n",
      "Iteration 8619, loss = 0.68948207\n",
      "Iteration 8620, loss = 0.68948165\n",
      "Iteration 8621, loss = 0.68948173\n",
      "Iteration 8622, loss = 0.68948147\n",
      "Iteration 8623, loss = 0.68948110\n",
      "Iteration 8624, loss = 0.68948123\n",
      "Iteration 8625, loss = 0.68948121\n",
      "Iteration 8626, loss = 0.68948096\n",
      "Iteration 8627, loss = 0.68948086\n",
      "Iteration 8628, loss = 0.68948137\n",
      "Iteration 8629, loss = 0.68948102\n",
      "Iteration 8630, loss = 0.68948024\n",
      "Iteration 8631, loss = 0.68948013\n",
      "Iteration 8632, loss = 0.68947978\n",
      "Iteration 8633, loss = 0.68948051\n",
      "Iteration 8634, loss = 0.68947994\n",
      "Iteration 8635, loss = 0.68947998\n",
      "Iteration 8636, loss = 0.68947998\n",
      "Iteration 8637, loss = 0.68947918\n",
      "Iteration 8638, loss = 0.68947922\n",
      "Iteration 8639, loss = 0.68947943\n",
      "Iteration 8640, loss = 0.68947884\n",
      "Iteration 8641, loss = 0.68947867\n",
      "Iteration 8642, loss = 0.68947856\n",
      "Iteration 8643, loss = 0.68947834\n",
      "Iteration 8644, loss = 0.68947871\n",
      "Iteration 8645, loss = 0.68947803\n",
      "Iteration 8646, loss = 0.68947778\n",
      "Iteration 8647, loss = 0.68947835\n",
      "Iteration 8648, loss = 0.68947838\n",
      "Iteration 8649, loss = 0.68947785\n",
      "Iteration 8650, loss = 0.68947790\n",
      "Iteration 8651, loss = 0.68947714\n",
      "Iteration 8652, loss = 0.68947717\n",
      "Iteration 8653, loss = 0.68947703\n",
      "Iteration 8654, loss = 0.68947782\n",
      "Iteration 8655, loss = 0.68947659\n",
      "Iteration 8656, loss = 0.68947665\n",
      "Iteration 8657, loss = 0.68947696\n",
      "Iteration 8658, loss = 0.68947633\n",
      "Iteration 8659, loss = 0.68947617\n",
      "Iteration 8660, loss = 0.68947607\n",
      "Iteration 8661, loss = 0.68947648\n",
      "Iteration 8662, loss = 0.68947579\n",
      "Iteration 8663, loss = 0.68947551\n",
      "Iteration 8664, loss = 0.68947528\n",
      "Iteration 8665, loss = 0.68947541\n",
      "Iteration 8666, loss = 0.68947559\n",
      "Iteration 8667, loss = 0.68947519\n",
      "Iteration 8668, loss = 0.68947512\n",
      "Iteration 8669, loss = 0.68947468\n",
      "Iteration 8670, loss = 0.68947517\n",
      "Iteration 8671, loss = 0.68947459\n",
      "Iteration 8672, loss = 0.68947431\n",
      "Iteration 8673, loss = 0.68947403\n",
      "Iteration 8674, loss = 0.68947393\n",
      "Iteration 8675, loss = 0.68947396\n",
      "Iteration 8676, loss = 0.68947385\n",
      "Iteration 8677, loss = 0.68947370\n",
      "Iteration 8678, loss = 0.68947395\n",
      "Iteration 8679, loss = 0.68947391\n",
      "Iteration 8680, loss = 0.68947318\n",
      "Iteration 8681, loss = 0.68947329\n",
      "Iteration 8682, loss = 0.68947316\n",
      "Iteration 8683, loss = 0.68947311\n",
      "Iteration 8684, loss = 0.68947285\n",
      "Iteration 8685, loss = 0.68947259\n",
      "Iteration 8686, loss = 0.68947226\n",
      "Iteration 8687, loss = 0.68947228\n",
      "Iteration 8688, loss = 0.68947224\n",
      "Iteration 8689, loss = 0.68947243\n",
      "Iteration 8690, loss = 0.68947188\n",
      "Iteration 8691, loss = 0.68947210\n",
      "Iteration 8692, loss = 0.68947205\n",
      "Iteration 8693, loss = 0.68947191\n",
      "Iteration 8694, loss = 0.68947160\n",
      "Iteration 8695, loss = 0.68947120\n",
      "Iteration 8696, loss = 0.68947122\n",
      "Iteration 8697, loss = 0.68947135\n",
      "Iteration 8698, loss = 0.68947083\n",
      "Iteration 8699, loss = 0.68947066\n",
      "Iteration 8700, loss = 0.68947083\n",
      "Iteration 8701, loss = 0.68947012\n",
      "Iteration 8702, loss = 0.68947054\n",
      "Iteration 8703, loss = 0.68947016\n",
      "Iteration 8704, loss = 0.68947003\n",
      "Iteration 8705, loss = 0.68947022\n",
      "Iteration 8706, loss = 0.68946981\n",
      "Iteration 8707, loss = 0.68946976\n",
      "Iteration 8708, loss = 0.68946924\n",
      "Iteration 8709, loss = 0.68946973\n",
      "Iteration 8710, loss = 0.68946913\n",
      "Iteration 8711, loss = 0.68946912\n",
      "Iteration 8712, loss = 0.68946880\n",
      "Iteration 8713, loss = 0.68946865\n",
      "Iteration 8714, loss = 0.68946876\n",
      "Iteration 8715, loss = 0.68946927\n",
      "Iteration 8716, loss = 0.68946849\n",
      "Iteration 8717, loss = 0.68946881\n",
      "Iteration 8718, loss = 0.68946800\n",
      "Iteration 8719, loss = 0.68946831\n",
      "Iteration 8720, loss = 0.68946803\n",
      "Iteration 8721, loss = 0.68946828\n",
      "Iteration 8722, loss = 0.68946789\n",
      "Iteration 8723, loss = 0.68946783\n",
      "Iteration 8724, loss = 0.68946776\n",
      "Iteration 8725, loss = 0.68946711\n",
      "Iteration 8726, loss = 0.68946733\n",
      "Iteration 8727, loss = 0.68946688\n",
      "Iteration 8728, loss = 0.68946713\n",
      "Iteration 8729, loss = 0.68946648\n",
      "Iteration 8730, loss = 0.68946640\n",
      "Iteration 8731, loss = 0.68946678\n",
      "Iteration 8732, loss = 0.68946611\n",
      "Iteration 8733, loss = 0.68946582\n",
      "Iteration 8734, loss = 0.68946627\n",
      "Iteration 8735, loss = 0.68946612\n",
      "Iteration 8736, loss = 0.68946566\n",
      "Iteration 8737, loss = 0.68946554\n",
      "Iteration 8738, loss = 0.68946542\n",
      "Iteration 8739, loss = 0.68946519\n",
      "Iteration 8740, loss = 0.68946495\n",
      "Iteration 8741, loss = 0.68946520\n",
      "Iteration 8742, loss = 0.68946477\n",
      "Iteration 8743, loss = 0.68946451\n",
      "Iteration 8744, loss = 0.68946466\n",
      "Iteration 8745, loss = 0.68946433\n",
      "Iteration 8746, loss = 0.68946475\n",
      "Iteration 8747, loss = 0.68946408\n",
      "Iteration 8748, loss = 0.68946390\n",
      "Iteration 8749, loss = 0.68946367\n",
      "Iteration 8750, loss = 0.68946366\n",
      "Iteration 8751, loss = 0.68946364\n",
      "Iteration 8752, loss = 0.68946355\n",
      "Iteration 8753, loss = 0.68946325\n",
      "Iteration 8754, loss = 0.68946319\n",
      "Iteration 8755, loss = 0.68946360\n",
      "Iteration 8756, loss = 0.68946300\n",
      "Iteration 8757, loss = 0.68946281\n",
      "Iteration 8758, loss = 0.68946255\n",
      "Iteration 8759, loss = 0.68946304\n",
      "Iteration 8760, loss = 0.68946334\n",
      "Iteration 8761, loss = 0.68946235\n",
      "Iteration 8762, loss = 0.68946212\n",
      "Iteration 8763, loss = 0.68946208\n",
      "Iteration 8764, loss = 0.68946222\n",
      "Iteration 8765, loss = 0.68946163\n",
      "Iteration 8766, loss = 0.68946145\n",
      "Iteration 8767, loss = 0.68946169\n",
      "Iteration 8768, loss = 0.68946154\n",
      "Iteration 8769, loss = 0.68946125\n",
      "Iteration 8770, loss = 0.68946103\n",
      "Iteration 8771, loss = 0.68946080\n",
      "Iteration 8772, loss = 0.68946088\n",
      "Iteration 8773, loss = 0.68946044\n",
      "Iteration 8774, loss = 0.68946065\n",
      "Iteration 8775, loss = 0.68946038\n",
      "Iteration 8776, loss = 0.68946019\n",
      "Iteration 8777, loss = 0.68946019\n",
      "Iteration 8778, loss = 0.68945997\n",
      "Iteration 8779, loss = 0.68945983\n",
      "Iteration 8780, loss = 0.68945972\n",
      "Iteration 8781, loss = 0.68945953\n",
      "Iteration 8782, loss = 0.68945964\n",
      "Iteration 8783, loss = 0.68945921\n",
      "Iteration 8784, loss = 0.68945913\n",
      "Iteration 8785, loss = 0.68945901\n",
      "Iteration 8786, loss = 0.68945900\n",
      "Iteration 8787, loss = 0.68945870\n",
      "Iteration 8788, loss = 0.68945852\n",
      "Iteration 8789, loss = 0.68945843\n",
      "Iteration 8790, loss = 0.68945802\n",
      "Iteration 8791, loss = 0.68945813\n",
      "Iteration 8792, loss = 0.68945815\n",
      "Iteration 8793, loss = 0.68945972\n",
      "Iteration 8794, loss = 0.68945834\n",
      "Iteration 8795, loss = 0.68945779\n",
      "Iteration 8796, loss = 0.68945745\n",
      "Iteration 8797, loss = 0.68945756\n",
      "Iteration 8798, loss = 0.68945735\n",
      "Iteration 8799, loss = 0.68945776\n",
      "Iteration 8800, loss = 0.68945704\n",
      "Iteration 8801, loss = 0.68945698\n",
      "Iteration 8802, loss = 0.68945681\n",
      "Iteration 8803, loss = 0.68945747\n",
      "Iteration 8804, loss = 0.68945656\n",
      "Iteration 8805, loss = 0.68945611\n",
      "Iteration 8806, loss = 0.68945632\n",
      "Iteration 8807, loss = 0.68945614\n",
      "Iteration 8808, loss = 0.68945593\n",
      "Iteration 8809, loss = 0.68945591\n",
      "Iteration 8810, loss = 0.68945598\n",
      "Iteration 8811, loss = 0.68945603\n",
      "Iteration 8812, loss = 0.68945582\n",
      "Iteration 8813, loss = 0.68945533\n",
      "Iteration 8814, loss = 0.68945604\n",
      "Iteration 8815, loss = 0.68945480\n",
      "Iteration 8816, loss = 0.68945490\n",
      "Iteration 8817, loss = 0.68945479\n",
      "Iteration 8818, loss = 0.68945446\n",
      "Iteration 8819, loss = 0.68945414\n",
      "Iteration 8820, loss = 0.68945443\n",
      "Iteration 8821, loss = 0.68945416\n",
      "Iteration 8822, loss = 0.68945402\n",
      "Iteration 8823, loss = 0.68945462\n",
      "Iteration 8824, loss = 0.68945427\n",
      "Iteration 8825, loss = 0.68945336\n",
      "Iteration 8826, loss = 0.68945358\n",
      "Iteration 8827, loss = 0.68945323\n",
      "Iteration 8828, loss = 0.68945351\n",
      "Iteration 8829, loss = 0.68945331\n",
      "Iteration 8830, loss = 0.68945334\n",
      "Iteration 8831, loss = 0.68945297\n",
      "Iteration 8832, loss = 0.68945268\n",
      "Iteration 8833, loss = 0.68945294\n",
      "Iteration 8834, loss = 0.68945254\n",
      "Iteration 8835, loss = 0.68945234\n",
      "Iteration 8836, loss = 0.68945259\n",
      "Iteration 8837, loss = 0.68945203\n",
      "Iteration 8838, loss = 0.68945169\n",
      "Iteration 8839, loss = 0.68945209\n",
      "Iteration 8840, loss = 0.68945149\n",
      "Iteration 8841, loss = 0.68945153\n",
      "Iteration 8842, loss = 0.68945139\n",
      "Iteration 8843, loss = 0.68945128\n",
      "Iteration 8844, loss = 0.68945116\n",
      "Iteration 8845, loss = 0.68945085\n",
      "Iteration 8846, loss = 0.68945082\n",
      "Iteration 8847, loss = 0.68945175\n",
      "Iteration 8848, loss = 0.68945077\n",
      "Iteration 8849, loss = 0.68945024\n",
      "Iteration 8850, loss = 0.68945039\n",
      "Iteration 8851, loss = 0.68944992\n",
      "Iteration 8852, loss = 0.68944986\n",
      "Iteration 8853, loss = 0.68944950\n",
      "Iteration 8854, loss = 0.68944959\n",
      "Iteration 8855, loss = 0.68944945\n",
      "Iteration 8856, loss = 0.68944933\n",
      "Iteration 8857, loss = 0.68944892\n",
      "Iteration 8858, loss = 0.68944895\n",
      "Iteration 8859, loss = 0.68944872\n",
      "Iteration 8860, loss = 0.68944903\n",
      "Iteration 8861, loss = 0.68944830\n",
      "Iteration 8862, loss = 0.68944810\n",
      "Iteration 8863, loss = 0.68944806\n",
      "Iteration 8864, loss = 0.68944820\n",
      "Iteration 8865, loss = 0.68944787\n",
      "Iteration 8866, loss = 0.68944755\n",
      "Iteration 8867, loss = 0.68944775\n",
      "Iteration 8868, loss = 0.68944785\n",
      "Iteration 8869, loss = 0.68944706\n",
      "Iteration 8870, loss = 0.68944744\n",
      "Iteration 8871, loss = 0.68944704\n",
      "Iteration 8872, loss = 0.68944728\n",
      "Iteration 8873, loss = 0.68944678\n",
      "Iteration 8874, loss = 0.68944693\n",
      "Iteration 8875, loss = 0.68944663\n",
      "Iteration 8876, loss = 0.68944655\n",
      "Iteration 8877, loss = 0.68944649\n",
      "Iteration 8878, loss = 0.68944666\n",
      "Iteration 8879, loss = 0.68944622\n",
      "Iteration 8880, loss = 0.68944592\n",
      "Iteration 8881, loss = 0.68944565\n",
      "Iteration 8882, loss = 0.68944561\n",
      "Iteration 8883, loss = 0.68944551\n",
      "Iteration 8884, loss = 0.68944547\n",
      "Iteration 8885, loss = 0.68944566\n",
      "Iteration 8886, loss = 0.68944602\n",
      "Iteration 8887, loss = 0.68944474\n",
      "Iteration 8888, loss = 0.68944481\n",
      "Iteration 8889, loss = 0.68944487\n",
      "Iteration 8890, loss = 0.68944453\n",
      "Iteration 8891, loss = 0.68944477\n",
      "Iteration 8892, loss = 0.68944446\n",
      "Iteration 8893, loss = 0.68944409\n",
      "Iteration 8894, loss = 0.68944377\n",
      "Iteration 8895, loss = 0.68944364\n",
      "Iteration 8896, loss = 0.68944379\n",
      "Iteration 8897, loss = 0.68944343\n",
      "Iteration 8898, loss = 0.68944329\n",
      "Iteration 8899, loss = 0.68944322\n",
      "Iteration 8900, loss = 0.68944385\n",
      "Iteration 8901, loss = 0.68944282\n",
      "Iteration 8902, loss = 0.68944288\n",
      "Iteration 8903, loss = 0.68944273\n",
      "Iteration 8904, loss = 0.68944269\n",
      "Iteration 8905, loss = 0.68944244\n",
      "Iteration 8906, loss = 0.68944230\n",
      "Iteration 8907, loss = 0.68944187\n",
      "Iteration 8908, loss = 0.68944187\n",
      "Iteration 8909, loss = 0.68944207\n",
      "Iteration 8910, loss = 0.68944194\n",
      "Iteration 8911, loss = 0.68944154\n",
      "Iteration 8912, loss = 0.68944143\n",
      "Iteration 8913, loss = 0.68944161\n",
      "Iteration 8914, loss = 0.68944146\n",
      "Iteration 8915, loss = 0.68944154\n",
      "Iteration 8916, loss = 0.68944105\n",
      "Iteration 8917, loss = 0.68944077\n",
      "Iteration 8918, loss = 0.68944078\n",
      "Iteration 8919, loss = 0.68944060\n",
      "Iteration 8920, loss = 0.68944008\n",
      "Iteration 8921, loss = 0.68944009\n",
      "Iteration 8922, loss = 0.68944004\n",
      "Iteration 8923, loss = 0.68944014\n",
      "Iteration 8924, loss = 0.68944036\n",
      "Iteration 8925, loss = 0.68943963\n",
      "Iteration 8926, loss = 0.68943955\n",
      "Iteration 8927, loss = 0.68943918\n",
      "Iteration 8928, loss = 0.68943931\n",
      "Iteration 8929, loss = 0.68943898\n",
      "Iteration 8930, loss = 0.68943880\n",
      "Iteration 8931, loss = 0.68943924\n",
      "Iteration 8932, loss = 0.68943886\n",
      "Iteration 8933, loss = 0.68943855\n",
      "Iteration 8934, loss = 0.68943821\n",
      "Iteration 8935, loss = 0.68943814\n",
      "Iteration 8936, loss = 0.68943837\n",
      "Iteration 8937, loss = 0.68943797\n",
      "Iteration 8938, loss = 0.68943784\n",
      "Iteration 8939, loss = 0.68943790\n",
      "Iteration 8940, loss = 0.68943784\n",
      "Iteration 8941, loss = 0.68943738\n",
      "Iteration 8942, loss = 0.68943742\n",
      "Iteration 8943, loss = 0.68943701\n",
      "Iteration 8944, loss = 0.68943702\n",
      "Iteration 8945, loss = 0.68943702\n",
      "Iteration 8946, loss = 0.68943650\n",
      "Iteration 8947, loss = 0.68943654\n",
      "Iteration 8948, loss = 0.68943637\n",
      "Iteration 8949, loss = 0.68943617\n",
      "Iteration 8950, loss = 0.68943656\n",
      "Iteration 8951, loss = 0.68943582\n",
      "Iteration 8952, loss = 0.68943577\n",
      "Iteration 8953, loss = 0.68943586\n",
      "Iteration 8954, loss = 0.68943552\n",
      "Iteration 8955, loss = 0.68943542\n",
      "Iteration 8956, loss = 0.68943525\n",
      "Iteration 8957, loss = 0.68943535\n",
      "Iteration 8958, loss = 0.68943508\n",
      "Iteration 8959, loss = 0.68943513\n",
      "Iteration 8960, loss = 0.68943460\n",
      "Iteration 8961, loss = 0.68943470\n",
      "Iteration 8962, loss = 0.68943458\n",
      "Iteration 8963, loss = 0.68943516\n",
      "Iteration 8964, loss = 0.68943419\n",
      "Iteration 8965, loss = 0.68943438\n",
      "Iteration 8966, loss = 0.68943401\n",
      "Iteration 8967, loss = 0.68943436\n",
      "Iteration 8968, loss = 0.68943401\n",
      "Iteration 8969, loss = 0.68943370\n",
      "Iteration 8970, loss = 0.68943379\n",
      "Iteration 8971, loss = 0.68943356\n",
      "Iteration 8972, loss = 0.68943306\n",
      "Iteration 8973, loss = 0.68943304\n",
      "Iteration 8974, loss = 0.68943331\n",
      "Iteration 8975, loss = 0.68943303\n",
      "Iteration 8976, loss = 0.68943290\n",
      "Iteration 8977, loss = 0.68943292\n",
      "Iteration 8978, loss = 0.68943263\n",
      "Iteration 8979, loss = 0.68943248\n",
      "Iteration 8980, loss = 0.68943225\n",
      "Iteration 8981, loss = 0.68943241\n",
      "Iteration 8982, loss = 0.68943217\n",
      "Iteration 8983, loss = 0.68943222\n",
      "Iteration 8984, loss = 0.68943171\n",
      "Iteration 8985, loss = 0.68943162\n",
      "Iteration 8986, loss = 0.68943154\n",
      "Iteration 8987, loss = 0.68943187\n",
      "Iteration 8988, loss = 0.68943140\n",
      "Iteration 8989, loss = 0.68943124\n",
      "Iteration 8990, loss = 0.68943103\n",
      "Iteration 8991, loss = 0.68943071\n",
      "Iteration 8992, loss = 0.68943054\n",
      "Iteration 8993, loss = 0.68943063\n",
      "Iteration 8994, loss = 0.68943044\n",
      "Iteration 8995, loss = 0.68943015\n",
      "Iteration 8996, loss = 0.68942993\n",
      "Iteration 8997, loss = 0.68942977\n",
      "Iteration 8998, loss = 0.68942990\n",
      "Iteration 8999, loss = 0.68942980\n",
      "Iteration 9000, loss = 0.68942944\n",
      "Iteration 9001, loss = 0.68942952\n",
      "Iteration 9002, loss = 0.68942972\n",
      "Iteration 9003, loss = 0.68943035\n",
      "Iteration 9004, loss = 0.68942905\n",
      "Iteration 9005, loss = 0.68942909\n",
      "Iteration 9006, loss = 0.68942911\n",
      "Iteration 9007, loss = 0.68942884\n",
      "Iteration 9008, loss = 0.68942840\n",
      "Iteration 9009, loss = 0.68942845\n",
      "Iteration 9010, loss = 0.68942828\n",
      "Iteration 9011, loss = 0.68942808\n",
      "Iteration 9012, loss = 0.68942796\n",
      "Iteration 9013, loss = 0.68942792\n",
      "Iteration 9014, loss = 0.68942761\n",
      "Iteration 9015, loss = 0.68942765\n",
      "Iteration 9016, loss = 0.68942754\n",
      "Iteration 9017, loss = 0.68942757\n",
      "Iteration 9018, loss = 0.68942715\n",
      "Iteration 9019, loss = 0.68942711\n",
      "Iteration 9020, loss = 0.68942742\n",
      "Iteration 9021, loss = 0.68942662\n",
      "Iteration 9022, loss = 0.68942669\n",
      "Iteration 9023, loss = 0.68942685\n",
      "Iteration 9024, loss = 0.68942639\n",
      "Iteration 9025, loss = 0.68942631\n",
      "Iteration 9026, loss = 0.68942636\n",
      "Iteration 9027, loss = 0.68942646\n",
      "Iteration 9028, loss = 0.68942619\n",
      "Iteration 9029, loss = 0.68942584\n",
      "Iteration 9030, loss = 0.68942567\n",
      "Iteration 9031, loss = 0.68942555\n",
      "Iteration 9032, loss = 0.68942542\n",
      "Iteration 9033, loss = 0.68942526\n",
      "Iteration 9034, loss = 0.68942521\n",
      "Iteration 9035, loss = 0.68942490\n",
      "Iteration 9036, loss = 0.68942501\n",
      "Iteration 9037, loss = 0.68942506\n",
      "Iteration 9038, loss = 0.68942486\n",
      "Iteration 9039, loss = 0.68942445\n",
      "Iteration 9040, loss = 0.68942458\n",
      "Iteration 9041, loss = 0.68942410\n",
      "Iteration 9042, loss = 0.68942386\n",
      "Iteration 9043, loss = 0.68942460\n",
      "Iteration 9044, loss = 0.68942474\n",
      "Iteration 9045, loss = 0.68942350\n",
      "Iteration 9046, loss = 0.68942351\n",
      "Iteration 9047, loss = 0.68942370\n",
      "Iteration 9048, loss = 0.68942326\n",
      "Iteration 9049, loss = 0.68942303\n",
      "Iteration 9050, loss = 0.68942319\n",
      "Iteration 9051, loss = 0.68942283\n",
      "Iteration 9052, loss = 0.68942257\n",
      "Iteration 9053, loss = 0.68942262\n",
      "Iteration 9054, loss = 0.68942321\n",
      "Iteration 9055, loss = 0.68942301\n",
      "Iteration 9056, loss = 0.68942211\n",
      "Iteration 9057, loss = 0.68942364\n",
      "Iteration 9058, loss = 0.68942273\n",
      "Iteration 9059, loss = 0.68942198\n",
      "Iteration 9060, loss = 0.68942181\n",
      "Iteration 9061, loss = 0.68942155\n",
      "Iteration 9062, loss = 0.68942149\n",
      "Iteration 9063, loss = 0.68942196\n",
      "Iteration 9064, loss = 0.68942136\n",
      "Iteration 9065, loss = 0.68942106\n",
      "Iteration 9066, loss = 0.68942139\n",
      "Iteration 9067, loss = 0.68942071\n",
      "Iteration 9068, loss = 0.68942059\n",
      "Iteration 9069, loss = 0.68942059\n",
      "Iteration 9070, loss = 0.68942068\n",
      "Iteration 9071, loss = 0.68942019\n",
      "Iteration 9072, loss = 0.68942018\n",
      "Iteration 9073, loss = 0.68942083\n",
      "Iteration 9074, loss = 0.68942012\n",
      "Iteration 9075, loss = 0.68941965\n",
      "Iteration 9076, loss = 0.68941955\n",
      "Iteration 9077, loss = 0.68941920\n",
      "Iteration 9078, loss = 0.68941985\n",
      "Iteration 9079, loss = 0.68941985\n",
      "Iteration 9080, loss = 0.68941910\n",
      "Iteration 9081, loss = 0.68941880\n",
      "Iteration 9082, loss = 0.68941879\n",
      "Iteration 9083, loss = 0.68941857\n",
      "Iteration 9084, loss = 0.68941847\n",
      "Iteration 9085, loss = 0.68941840\n",
      "Iteration 9086, loss = 0.68941867\n",
      "Iteration 9087, loss = 0.68941819\n",
      "Iteration 9088, loss = 0.68941799\n",
      "Iteration 9089, loss = 0.68941791\n",
      "Iteration 9090, loss = 0.68941834\n",
      "Iteration 9091, loss = 0.68941745\n",
      "Iteration 9092, loss = 0.68941752\n",
      "Iteration 9093, loss = 0.68941727\n",
      "Iteration 9094, loss = 0.68941717\n",
      "Iteration 9095, loss = 0.68941700\n",
      "Iteration 9096, loss = 0.68941676\n",
      "Iteration 9097, loss = 0.68941685\n",
      "Iteration 9098, loss = 0.68941715\n",
      "Iteration 9099, loss = 0.68941719\n",
      "Iteration 9100, loss = 0.68941658\n",
      "Iteration 9101, loss = 0.68941635\n",
      "Iteration 9102, loss = 0.68941619\n",
      "Iteration 9103, loss = 0.68941651\n",
      "Iteration 9104, loss = 0.68941563\n",
      "Iteration 9105, loss = 0.68941599\n",
      "Iteration 9106, loss = 0.68941557\n",
      "Iteration 9107, loss = 0.68941566\n",
      "Iteration 9108, loss = 0.68941541\n",
      "Iteration 9109, loss = 0.68941523\n",
      "Iteration 9110, loss = 0.68941477\n",
      "Iteration 9111, loss = 0.68941572\n",
      "Iteration 9112, loss = 0.68941514\n",
      "Iteration 9113, loss = 0.68941473\n",
      "Iteration 9114, loss = 0.68941433\n",
      "Iteration 9115, loss = 0.68941422\n",
      "Iteration 9116, loss = 0.68941421\n",
      "Iteration 9117, loss = 0.68941444\n",
      "Iteration 9118, loss = 0.68941396\n",
      "Iteration 9119, loss = 0.68941391\n",
      "Iteration 9120, loss = 0.68941393\n",
      "Iteration 9121, loss = 0.68941356\n",
      "Iteration 9122, loss = 0.68941409\n",
      "Iteration 9123, loss = 0.68941325\n",
      "Iteration 9124, loss = 0.68941331\n",
      "Iteration 9125, loss = 0.68941315\n",
      "Iteration 9126, loss = 0.68941325\n",
      "Iteration 9127, loss = 0.68941310\n",
      "Iteration 9128, loss = 0.68941288\n",
      "Iteration 9129, loss = 0.68941257\n",
      "Iteration 9130, loss = 0.68941246\n",
      "Iteration 9131, loss = 0.68941226\n",
      "Iteration 9132, loss = 0.68941220\n",
      "Iteration 9133, loss = 0.68941209\n",
      "Iteration 9134, loss = 0.68941201\n",
      "Iteration 9135, loss = 0.68941210\n",
      "Iteration 9136, loss = 0.68941213\n",
      "Iteration 9137, loss = 0.68941160\n",
      "Iteration 9138, loss = 0.68941160\n",
      "Iteration 9139, loss = 0.68941110\n",
      "Iteration 9140, loss = 0.68941160\n",
      "Iteration 9141, loss = 0.68941091\n",
      "Iteration 9142, loss = 0.68941088\n",
      "Iteration 9143, loss = 0.68941138\n",
      "Iteration 9144, loss = 0.68941104\n",
      "Iteration 9145, loss = 0.68941048\n",
      "Iteration 9146, loss = 0.68941069\n",
      "Iteration 9147, loss = 0.68941017\n",
      "Iteration 9148, loss = 0.68941019\n",
      "Iteration 9149, loss = 0.68940992\n",
      "Iteration 9150, loss = 0.68941061\n",
      "Iteration 9151, loss = 0.68940983\n",
      "Iteration 9152, loss = 0.68940962\n",
      "Iteration 9153, loss = 0.68941027\n",
      "Iteration 9154, loss = 0.68940926\n",
      "Iteration 9155, loss = 0.68940925\n",
      "Iteration 9156, loss = 0.68940915\n",
      "Iteration 9157, loss = 0.68940944\n",
      "Iteration 9158, loss = 0.68940972\n",
      "Iteration 9159, loss = 0.68940857\n",
      "Iteration 9160, loss = 0.68940844\n",
      "Iteration 9161, loss = 0.68940882\n",
      "Iteration 9162, loss = 0.68940831\n",
      "Iteration 9163, loss = 0.68940801\n",
      "Iteration 9164, loss = 0.68940826\n",
      "Iteration 9165, loss = 0.68940826\n",
      "Iteration 9166, loss = 0.68940812\n",
      "Iteration 9167, loss = 0.68940766\n",
      "Iteration 9168, loss = 0.68940749\n",
      "Iteration 9169, loss = 0.68940769\n",
      "Iteration 9170, loss = 0.68940733\n",
      "Iteration 9171, loss = 0.68940840\n",
      "Iteration 9172, loss = 0.68940716\n",
      "Iteration 9173, loss = 0.68940706\n",
      "Iteration 9174, loss = 0.68940674\n",
      "Iteration 9175, loss = 0.68940659\n",
      "Iteration 9176, loss = 0.68940660\n",
      "Iteration 9177, loss = 0.68940647\n",
      "Iteration 9178, loss = 0.68940633\n",
      "Iteration 9179, loss = 0.68940635\n",
      "Iteration 9180, loss = 0.68940605\n",
      "Iteration 9181, loss = 0.68940627\n",
      "Iteration 9182, loss = 0.68940567\n",
      "Iteration 9183, loss = 0.68940628\n",
      "Iteration 9184, loss = 0.68940533\n",
      "Iteration 9185, loss = 0.68940559\n",
      "Iteration 9186, loss = 0.68940516\n",
      "Iteration 9187, loss = 0.68940521\n",
      "Iteration 9188, loss = 0.68940506\n",
      "Iteration 9189, loss = 0.68940518\n",
      "Iteration 9190, loss = 0.68940491\n",
      "Iteration 9191, loss = 0.68940487\n",
      "Iteration 9192, loss = 0.68940453\n",
      "Iteration 9193, loss = 0.68940417\n",
      "Iteration 9194, loss = 0.68940408\n",
      "Iteration 9195, loss = 0.68940411\n",
      "Iteration 9196, loss = 0.68940392\n",
      "Iteration 9197, loss = 0.68940378\n",
      "Iteration 9198, loss = 0.68940364\n",
      "Iteration 9199, loss = 0.68940357\n",
      "Iteration 9200, loss = 0.68940357\n",
      "Iteration 9201, loss = 0.68940312\n",
      "Iteration 9202, loss = 0.68940312\n",
      "Iteration 9203, loss = 0.68940312\n",
      "Iteration 9204, loss = 0.68940438\n",
      "Iteration 9205, loss = 0.68940272\n",
      "Iteration 9206, loss = 0.68940291\n",
      "Iteration 9207, loss = 0.68940242\n",
      "Iteration 9208, loss = 0.68940227\n",
      "Iteration 9209, loss = 0.68940200\n",
      "Iteration 9210, loss = 0.68940191\n",
      "Iteration 9211, loss = 0.68940202\n",
      "Iteration 9212, loss = 0.68940189\n",
      "Iteration 9213, loss = 0.68940131\n",
      "Iteration 9214, loss = 0.68940157\n",
      "Iteration 9215, loss = 0.68940154\n",
      "Iteration 9216, loss = 0.68940144\n",
      "Iteration 9217, loss = 0.68940110\n",
      "Iteration 9218, loss = 0.68940116\n",
      "Iteration 9219, loss = 0.68940109\n",
      "Iteration 9220, loss = 0.68940053\n",
      "Iteration 9221, loss = 0.68940181\n",
      "Iteration 9222, loss = 0.68940038\n",
      "Iteration 9223, loss = 0.68940062\n",
      "Iteration 9224, loss = 0.68940018\n",
      "Iteration 9225, loss = 0.68939989\n",
      "Iteration 9226, loss = 0.68939982\n",
      "Iteration 9227, loss = 0.68939980\n",
      "Iteration 9228, loss = 0.68939998\n",
      "Iteration 9229, loss = 0.68939976\n",
      "Iteration 9230, loss = 0.68939957\n",
      "Iteration 9231, loss = 0.68939958\n",
      "Iteration 9232, loss = 0.68939902\n",
      "Iteration 9233, loss = 0.68939994\n",
      "Iteration 9234, loss = 0.68939864\n",
      "Iteration 9235, loss = 0.68939861\n",
      "Iteration 9236, loss = 0.68939842\n",
      "Iteration 9237, loss = 0.68939860\n",
      "Iteration 9238, loss = 0.68939824\n",
      "Iteration 9239, loss = 0.68939820\n",
      "Iteration 9240, loss = 0.68939793\n",
      "Iteration 9241, loss = 0.68939809\n",
      "Iteration 9242, loss = 0.68939764\n",
      "Iteration 9243, loss = 0.68939829\n",
      "Iteration 9244, loss = 0.68939763\n",
      "Iteration 9245, loss = 0.68939760\n",
      "Iteration 9246, loss = 0.68939775\n",
      "Iteration 9247, loss = 0.68939727\n",
      "Iteration 9248, loss = 0.68939749\n",
      "Iteration 9249, loss = 0.68939695\n",
      "Iteration 9250, loss = 0.68939717\n",
      "Iteration 9251, loss = 0.68939678\n",
      "Iteration 9252, loss = 0.68939679\n",
      "Iteration 9253, loss = 0.68939637\n",
      "Iteration 9254, loss = 0.68939639\n",
      "Iteration 9255, loss = 0.68939622\n",
      "Iteration 9256, loss = 0.68939597\n",
      "Iteration 9257, loss = 0.68939588\n",
      "Iteration 9258, loss = 0.68939581\n",
      "Iteration 9259, loss = 0.68939574\n",
      "Iteration 9260, loss = 0.68939572\n",
      "Iteration 9261, loss = 0.68939606\n",
      "Iteration 9262, loss = 0.68939596\n",
      "Iteration 9263, loss = 0.68939531\n",
      "Iteration 9264, loss = 0.68939497\n",
      "Iteration 9265, loss = 0.68939492\n",
      "Iteration 9266, loss = 0.68939505\n",
      "Iteration 9267, loss = 0.68939467\n",
      "Iteration 9268, loss = 0.68939449\n",
      "Iteration 9269, loss = 0.68939469\n",
      "Iteration 9270, loss = 0.68939454\n",
      "Iteration 9271, loss = 0.68939446\n",
      "Iteration 9272, loss = 0.68939405\n",
      "Iteration 9273, loss = 0.68939409\n",
      "Iteration 9274, loss = 0.68939380\n",
      "Iteration 9275, loss = 0.68939359\n",
      "Iteration 9276, loss = 0.68939398\n",
      "Iteration 9277, loss = 0.68939328\n",
      "Iteration 9278, loss = 0.68939359\n",
      "Iteration 9279, loss = 0.68939395\n",
      "Iteration 9280, loss = 0.68939314\n",
      "Iteration 9281, loss = 0.68939307\n",
      "Iteration 9282, loss = 0.68939266\n",
      "Iteration 9283, loss = 0.68939290\n",
      "Iteration 9284, loss = 0.68939283\n",
      "Iteration 9285, loss = 0.68939221\n",
      "Iteration 9286, loss = 0.68939245\n",
      "Iteration 9287, loss = 0.68939221\n",
      "Iteration 9288, loss = 0.68939237\n",
      "Iteration 9289, loss = 0.68939173\n",
      "Iteration 9290, loss = 0.68939168\n",
      "Iteration 9291, loss = 0.68939177\n",
      "Iteration 9292, loss = 0.68939180\n",
      "Iteration 9293, loss = 0.68939128\n",
      "Iteration 9294, loss = 0.68939121\n",
      "Iteration 9295, loss = 0.68939131\n",
      "Iteration 9296, loss = 0.68939076\n",
      "Iteration 9297, loss = 0.68939101\n",
      "Iteration 9298, loss = 0.68939086\n",
      "Iteration 9299, loss = 0.68939068\n",
      "Iteration 9300, loss = 0.68939087\n",
      "Iteration 9301, loss = 0.68939018\n",
      "Iteration 9302, loss = 0.68939006\n",
      "Iteration 9303, loss = 0.68939033\n",
      "Iteration 9304, loss = 0.68938974\n",
      "Iteration 9305, loss = 0.68939007\n",
      "Iteration 9306, loss = 0.68938977\n",
      "Iteration 9307, loss = 0.68938918\n",
      "Iteration 9308, loss = 0.68938981\n",
      "Iteration 9309, loss = 0.68938948\n",
      "Iteration 9310, loss = 0.68938927\n",
      "Iteration 9311, loss = 0.68938954\n",
      "Iteration 9312, loss = 0.68938863\n",
      "Iteration 9313, loss = 0.68938861\n",
      "Iteration 9314, loss = 0.68938878\n",
      "Iteration 9315, loss = 0.68938858\n",
      "Iteration 9316, loss = 0.68938835\n",
      "Iteration 9317, loss = 0.68938890\n",
      "Iteration 9318, loss = 0.68938823\n",
      "Iteration 9319, loss = 0.68938771\n",
      "Iteration 9320, loss = 0.68938777\n",
      "Iteration 9321, loss = 0.68938792\n",
      "Iteration 9322, loss = 0.68938772\n",
      "Iteration 9323, loss = 0.68938737\n",
      "Iteration 9324, loss = 0.68938711\n",
      "Iteration 9325, loss = 0.68938695\n",
      "Iteration 9326, loss = 0.68938738\n",
      "Iteration 9327, loss = 0.68938723\n",
      "Iteration 9328, loss = 0.68938668\n",
      "Iteration 9329, loss = 0.68938683\n",
      "Iteration 9330, loss = 0.68938660\n",
      "Iteration 9331, loss = 0.68938623\n",
      "Iteration 9332, loss = 0.68938627\n",
      "Iteration 9333, loss = 0.68938606\n",
      "Iteration 9334, loss = 0.68938578\n",
      "Iteration 9335, loss = 0.68938577\n",
      "Iteration 9336, loss = 0.68938565\n",
      "Iteration 9337, loss = 0.68938553\n",
      "Iteration 9338, loss = 0.68938585\n",
      "Iteration 9339, loss = 0.68938539\n",
      "Iteration 9340, loss = 0.68938534\n",
      "Iteration 9341, loss = 0.68938509\n",
      "Iteration 9342, loss = 0.68938501\n",
      "Iteration 9343, loss = 0.68938486\n",
      "Iteration 9344, loss = 0.68938495\n",
      "Iteration 9345, loss = 0.68938477\n",
      "Iteration 9346, loss = 0.68938452\n",
      "Iteration 9347, loss = 0.68938474\n",
      "Iteration 9348, loss = 0.68938433\n",
      "Iteration 9349, loss = 0.68938404\n",
      "Iteration 9350, loss = 0.68938409\n",
      "Iteration 9351, loss = 0.68938413\n",
      "Iteration 9352, loss = 0.68938415\n",
      "Iteration 9353, loss = 0.68938393\n",
      "Iteration 9354, loss = 0.68938351\n",
      "Iteration 9355, loss = 0.68938363\n",
      "Iteration 9356, loss = 0.68938336\n",
      "Iteration 9357, loss = 0.68938347\n",
      "Iteration 9358, loss = 0.68938339\n",
      "Iteration 9359, loss = 0.68938317\n",
      "Iteration 9360, loss = 0.68938293\n",
      "Iteration 9361, loss = 0.68938289\n",
      "Iteration 9362, loss = 0.68938272\n",
      "Iteration 9363, loss = 0.68938225\n",
      "Iteration 9364, loss = 0.68938286\n",
      "Iteration 9365, loss = 0.68938226\n",
      "Iteration 9366, loss = 0.68938209\n",
      "Iteration 9367, loss = 0.68938183\n",
      "Iteration 9368, loss = 0.68938197\n",
      "Iteration 9369, loss = 0.68938168\n",
      "Iteration 9370, loss = 0.68938142\n",
      "Iteration 9371, loss = 0.68938140\n",
      "Iteration 9372, loss = 0.68938116\n",
      "Iteration 9373, loss = 0.68938122\n",
      "Iteration 9374, loss = 0.68938121\n",
      "Iteration 9375, loss = 0.68938069\n",
      "Iteration 9376, loss = 0.68938061\n",
      "Iteration 9377, loss = 0.68938045\n",
      "Iteration 9378, loss = 0.68938088\n",
      "Iteration 9379, loss = 0.68938043\n",
      "Iteration 9380, loss = 0.68938018\n",
      "Iteration 9381, loss = 0.68937988\n",
      "Iteration 9382, loss = 0.68937993\n",
      "Iteration 9383, loss = 0.68937973\n",
      "Iteration 9384, loss = 0.68937936\n",
      "Iteration 9385, loss = 0.68937957\n",
      "Iteration 9386, loss = 0.68937928\n",
      "Iteration 9387, loss = 0.68937947\n",
      "Iteration 9388, loss = 0.68937918\n",
      "Iteration 9389, loss = 0.68937899\n",
      "Iteration 9390, loss = 0.68937936\n",
      "Iteration 9391, loss = 0.68937882\n",
      "Iteration 9392, loss = 0.68937851\n",
      "Iteration 9393, loss = 0.68937883\n",
      "Iteration 9394, loss = 0.68937896\n",
      "Iteration 9395, loss = 0.68937803\n",
      "Iteration 9396, loss = 0.68937778\n",
      "Iteration 9397, loss = 0.68937774\n",
      "Iteration 9398, loss = 0.68937799\n",
      "Iteration 9399, loss = 0.68937760\n",
      "Iteration 9400, loss = 0.68937789\n",
      "Iteration 9401, loss = 0.68937802\n",
      "Iteration 9402, loss = 0.68937748\n",
      "Iteration 9403, loss = 0.68937718\n",
      "Iteration 9404, loss = 0.68937714\n",
      "Iteration 9405, loss = 0.68937733\n",
      "Iteration 9406, loss = 0.68937645\n",
      "Iteration 9407, loss = 0.68937702\n",
      "Iteration 9408, loss = 0.68937648\n",
      "Iteration 9409, loss = 0.68937623\n",
      "Iteration 9410, loss = 0.68937640\n",
      "Iteration 9411, loss = 0.68937613\n",
      "Iteration 9412, loss = 0.68937612\n",
      "Iteration 9413, loss = 0.68937585\n",
      "Iteration 9414, loss = 0.68937633\n",
      "Iteration 9415, loss = 0.68937570\n",
      "Iteration 9416, loss = 0.68937561\n",
      "Iteration 9417, loss = 0.68937515\n",
      "Iteration 9418, loss = 0.68937516\n",
      "Iteration 9419, loss = 0.68937518\n",
      "Iteration 9420, loss = 0.68937494\n",
      "Iteration 9421, loss = 0.68937528\n",
      "Iteration 9422, loss = 0.68937556\n",
      "Iteration 9423, loss = 0.68937401\n",
      "Iteration 9424, loss = 0.68937433\n",
      "Iteration 9425, loss = 0.68937467\n",
      "Iteration 9426, loss = 0.68937403\n",
      "Iteration 9427, loss = 0.68937409\n",
      "Iteration 9428, loss = 0.68937365\n",
      "Iteration 9429, loss = 0.68937375\n",
      "Iteration 9430, loss = 0.68937353\n",
      "Iteration 9431, loss = 0.68937323\n",
      "Iteration 9432, loss = 0.68937326\n",
      "Iteration 9433, loss = 0.68937383\n",
      "Iteration 9434, loss = 0.68937295\n",
      "Iteration 9435, loss = 0.68937307\n",
      "Iteration 9436, loss = 0.68937276\n",
      "Iteration 9437, loss = 0.68937280\n",
      "Iteration 9438, loss = 0.68937300\n",
      "Iteration 9439, loss = 0.68937285\n",
      "Iteration 9440, loss = 0.68937263\n",
      "Iteration 9441, loss = 0.68937274\n",
      "Iteration 9442, loss = 0.68937208\n",
      "Iteration 9443, loss = 0.68937218\n",
      "Iteration 9444, loss = 0.68937209\n",
      "Iteration 9445, loss = 0.68937201\n",
      "Iteration 9446, loss = 0.68937180\n",
      "Iteration 9447, loss = 0.68937157\n",
      "Iteration 9448, loss = 0.68937143\n",
      "Iteration 9449, loss = 0.68937137\n",
      "Iteration 9450, loss = 0.68937110\n",
      "Iteration 9451, loss = 0.68937090\n",
      "Iteration 9452, loss = 0.68937092\n",
      "Iteration 9453, loss = 0.68937109\n",
      "Iteration 9454, loss = 0.68937055\n",
      "Iteration 9455, loss = 0.68937135\n",
      "Iteration 9456, loss = 0.68937049\n",
      "Iteration 9457, loss = 0.68937078\n",
      "Iteration 9458, loss = 0.68937058\n",
      "Iteration 9459, loss = 0.68936996\n",
      "Iteration 9460, loss = 0.68937023\n",
      "Iteration 9461, loss = 0.68936999\n",
      "Iteration 9462, loss = 0.68937014\n",
      "Iteration 9463, loss = 0.68936990\n",
      "Iteration 9464, loss = 0.68936946\n",
      "Iteration 9465, loss = 0.68937033\n",
      "Iteration 9466, loss = 0.68936928\n",
      "Iteration 9467, loss = 0.68936908\n",
      "Iteration 9468, loss = 0.68936905\n",
      "Iteration 9469, loss = 0.68936904\n",
      "Iteration 9470, loss = 0.68936874\n",
      "Iteration 9471, loss = 0.68936891\n",
      "Iteration 9472, loss = 0.68936863\n",
      "Iteration 9473, loss = 0.68936846\n",
      "Iteration 9474, loss = 0.68936834\n",
      "Iteration 9475, loss = 0.68936890\n",
      "Iteration 9476, loss = 0.68936775\n",
      "Iteration 9477, loss = 0.68936793\n",
      "Iteration 9478, loss = 0.68936796\n",
      "Iteration 9479, loss = 0.68936773\n",
      "Iteration 9480, loss = 0.68936745\n",
      "Iteration 9481, loss = 0.68936736\n",
      "Iteration 9482, loss = 0.68936705\n",
      "Iteration 9483, loss = 0.68936792\n",
      "Iteration 9484, loss = 0.68936711\n",
      "Iteration 9485, loss = 0.68936700\n",
      "Iteration 9486, loss = 0.68936676\n",
      "Iteration 9487, loss = 0.68936674\n",
      "Iteration 9488, loss = 0.68936715\n",
      "Iteration 9489, loss = 0.68936641\n",
      "Iteration 9490, loss = 0.68936664\n",
      "Iteration 9491, loss = 0.68936602\n",
      "Iteration 9492, loss = 0.68936603\n",
      "Iteration 9493, loss = 0.68936616\n",
      "Iteration 9494, loss = 0.68936596\n",
      "Iteration 9495, loss = 0.68936548\n",
      "Iteration 9496, loss = 0.68936552\n",
      "Iteration 9497, loss = 0.68936571\n",
      "Iteration 9498, loss = 0.68936524\n",
      "Iteration 9499, loss = 0.68936540\n",
      "Iteration 9500, loss = 0.68936545\n",
      "Iteration 9501, loss = 0.68936508\n",
      "Iteration 9502, loss = 0.68936488\n",
      "Iteration 9503, loss = 0.68936489\n",
      "Iteration 9504, loss = 0.68936465\n",
      "Iteration 9505, loss = 0.68936440\n",
      "Iteration 9506, loss = 0.68936446\n",
      "Iteration 9507, loss = 0.68936414\n",
      "Iteration 9508, loss = 0.68936418\n",
      "Iteration 9509, loss = 0.68936432\n",
      "Iteration 9510, loss = 0.68936426\n",
      "Iteration 9511, loss = 0.68936328\n",
      "Iteration 9512, loss = 0.68936365\n",
      "Iteration 9513, loss = 0.68936341\n",
      "Iteration 9514, loss = 0.68936361\n",
      "Iteration 9515, loss = 0.68936388\n",
      "Iteration 9516, loss = 0.68936332\n",
      "Iteration 9517, loss = 0.68936295\n",
      "Iteration 9518, loss = 0.68936314\n",
      "Iteration 9519, loss = 0.68936275\n",
      "Iteration 9520, loss = 0.68936424\n",
      "Iteration 9521, loss = 0.68936284\n",
      "Iteration 9522, loss = 0.68936245\n",
      "Iteration 9523, loss = 0.68936230\n",
      "Iteration 9524, loss = 0.68936219\n",
      "Iteration 9525, loss = 0.68936278\n",
      "Iteration 9526, loss = 0.68936280\n",
      "Iteration 9527, loss = 0.68936174\n",
      "Iteration 9528, loss = 0.68936210\n",
      "Iteration 9529, loss = 0.68936234\n",
      "Iteration 9530, loss = 0.68936140\n",
      "Iteration 9531, loss = 0.68936160\n",
      "Iteration 9532, loss = 0.68936123\n",
      "Iteration 9533, loss = 0.68936120\n",
      "Iteration 9534, loss = 0.68936072\n",
      "Iteration 9535, loss = 0.68936084\n",
      "Iteration 9536, loss = 0.68936065\n",
      "Iteration 9537, loss = 0.68936113\n",
      "Iteration 9538, loss = 0.68936021\n",
      "Iteration 9539, loss = 0.68936071\n",
      "Iteration 9540, loss = 0.68936011\n",
      "Iteration 9541, loss = 0.68936027\n",
      "Iteration 9542, loss = 0.68936019\n",
      "Iteration 9543, loss = 0.68936014\n",
      "Iteration 9544, loss = 0.68935956\n",
      "Iteration 9545, loss = 0.68935953\n",
      "Iteration 9546, loss = 0.68935921\n",
      "Iteration 9547, loss = 0.68935920\n",
      "Iteration 9548, loss = 0.68935951\n",
      "Iteration 9549, loss = 0.68935920\n",
      "Iteration 9550, loss = 0.68935880\n",
      "Iteration 9551, loss = 0.68935881\n",
      "Iteration 9552, loss = 0.68935868\n",
      "Iteration 9553, loss = 0.68935847\n",
      "Iteration 9554, loss = 0.68935804\n",
      "Iteration 9555, loss = 0.68935838\n",
      "Iteration 9556, loss = 0.68935829\n",
      "Iteration 9557, loss = 0.68935787\n",
      "Iteration 9558, loss = 0.68935883\n",
      "Iteration 9559, loss = 0.68935803\n",
      "Iteration 9560, loss = 0.68935756\n",
      "Iteration 9561, loss = 0.68935788\n",
      "Iteration 9562, loss = 0.68935712\n",
      "Iteration 9563, loss = 0.68935744\n",
      "Iteration 9564, loss = 0.68935726\n",
      "Iteration 9565, loss = 0.68935802\n",
      "Iteration 9566, loss = 0.68935816\n",
      "Iteration 9567, loss = 0.68935727\n",
      "Iteration 9568, loss = 0.68935644\n",
      "Iteration 9569, loss = 0.68935653\n",
      "Iteration 9570, loss = 0.68935681\n",
      "Iteration 9571, loss = 0.68935627\n",
      "Iteration 9572, loss = 0.68935623\n",
      "Iteration 9573, loss = 0.68935629\n",
      "Iteration 9574, loss = 0.68935638\n",
      "Iteration 9575, loss = 0.68935546\n",
      "Iteration 9576, loss = 0.68935599\n",
      "Iteration 9577, loss = 0.68935541\n",
      "Iteration 9578, loss = 0.68935575\n",
      "Iteration 9579, loss = 0.68935516\n",
      "Iteration 9580, loss = 0.68935534\n",
      "Iteration 9581, loss = 0.68935518\n",
      "Iteration 9582, loss = 0.68935501\n",
      "Iteration 9583, loss = 0.68935472\n",
      "Iteration 9584, loss = 0.68935479\n",
      "Iteration 9585, loss = 0.68935429\n",
      "Iteration 9586, loss = 0.68935491\n",
      "Iteration 9587, loss = 0.68935439\n",
      "Iteration 9588, loss = 0.68935461\n",
      "Iteration 9589, loss = 0.68935385\n",
      "Iteration 9590, loss = 0.68935387\n",
      "Iteration 9591, loss = 0.68935408\n",
      "Iteration 9592, loss = 0.68935344\n",
      "Iteration 9593, loss = 0.68935339\n",
      "Iteration 9594, loss = 0.68935338\n",
      "Iteration 9595, loss = 0.68935355\n",
      "Iteration 9596, loss = 0.68935293\n",
      "Iteration 9597, loss = 0.68935318\n",
      "Iteration 9598, loss = 0.68935375\n",
      "Iteration 9599, loss = 0.68935272\n",
      "Iteration 9600, loss = 0.68935255\n",
      "Iteration 9601, loss = 0.68935254\n",
      "Iteration 9602, loss = 0.68935242\n",
      "Iteration 9603, loss = 0.68935242\n",
      "Iteration 9604, loss = 0.68935191\n",
      "Iteration 9605, loss = 0.68935214\n",
      "Iteration 9606, loss = 0.68935189\n",
      "Iteration 9607, loss = 0.68935164\n",
      "Iteration 9608, loss = 0.68935175\n",
      "Iteration 9609, loss = 0.68935131\n",
      "Iteration 9610, loss = 0.68935129\n",
      "Iteration 9611, loss = 0.68935133\n",
      "Iteration 9612, loss = 0.68935143\n",
      "Iteration 9613, loss = 0.68935143\n",
      "Iteration 9614, loss = 0.68935085\n",
      "Iteration 9615, loss = 0.68935111\n",
      "Iteration 9616, loss = 0.68935138\n",
      "Iteration 9617, loss = 0.68935052\n",
      "Iteration 9618, loss = 0.68935047\n",
      "Iteration 9619, loss = 0.68935019\n",
      "Iteration 9620, loss = 0.68935029\n",
      "Iteration 9621, loss = 0.68934991\n",
      "Iteration 9622, loss = 0.68935007\n",
      "Iteration 9623, loss = 0.68935013\n",
      "Iteration 9624, loss = 0.68934959\n",
      "Iteration 9625, loss = 0.68934924\n",
      "Iteration 9626, loss = 0.68934911\n",
      "Iteration 9627, loss = 0.68934938\n",
      "Iteration 9628, loss = 0.68934931\n",
      "Iteration 9629, loss = 0.68934877\n",
      "Iteration 9630, loss = 0.68934884\n",
      "Iteration 9631, loss = 0.68934889\n",
      "Iteration 9632, loss = 0.68934871\n",
      "Iteration 9633, loss = 0.68934855\n",
      "Iteration 9634, loss = 0.68934825\n",
      "Iteration 9635, loss = 0.68934821\n",
      "Iteration 9636, loss = 0.68934816\n",
      "Iteration 9637, loss = 0.68934809\n",
      "Iteration 9638, loss = 0.68934798\n",
      "Iteration 9639, loss = 0.68934783\n",
      "Iteration 9640, loss = 0.68934853\n",
      "Iteration 9641, loss = 0.68934801\n",
      "Iteration 9642, loss = 0.68934741\n",
      "Iteration 9643, loss = 0.68934738\n",
      "Iteration 9644, loss = 0.68934720\n",
      "Iteration 9645, loss = 0.68934714\n",
      "Iteration 9646, loss = 0.68934707\n",
      "Iteration 9647, loss = 0.68934671\n",
      "Iteration 9648, loss = 0.68934689\n",
      "Iteration 9649, loss = 0.68934666\n",
      "Iteration 9650, loss = 0.68934664\n",
      "Iteration 9651, loss = 0.68934639\n",
      "Iteration 9652, loss = 0.68934687\n",
      "Iteration 9653, loss = 0.68934628\n",
      "Iteration 9654, loss = 0.68934630\n",
      "Iteration 9655, loss = 0.68934602\n",
      "Iteration 9656, loss = 0.68934558\n",
      "Iteration 9657, loss = 0.68934653\n",
      "Iteration 9658, loss = 0.68934565\n",
      "Iteration 9659, loss = 0.68934539\n",
      "Iteration 9660, loss = 0.68934517\n",
      "Iteration 9661, loss = 0.68934507\n",
      "Iteration 9662, loss = 0.68934512\n",
      "Iteration 9663, loss = 0.68934475\n",
      "Iteration 9664, loss = 0.68934479\n",
      "Iteration 9665, loss = 0.68934529\n",
      "Iteration 9666, loss = 0.68934520\n",
      "Iteration 9667, loss = 0.68934458\n",
      "Iteration 9668, loss = 0.68934409\n",
      "Iteration 9669, loss = 0.68934432\n",
      "Iteration 9670, loss = 0.68934401\n",
      "Iteration 9671, loss = 0.68934411\n",
      "Iteration 9672, loss = 0.68934414\n",
      "Iteration 9673, loss = 0.68934414\n",
      "Iteration 9674, loss = 0.68934333\n",
      "Iteration 9675, loss = 0.68934367\n",
      "Iteration 9676, loss = 0.68934372\n",
      "Iteration 9677, loss = 0.68934332\n",
      "Iteration 9678, loss = 0.68934342\n",
      "Iteration 9679, loss = 0.68934297\n",
      "Iteration 9680, loss = 0.68934341\n",
      "Iteration 9681, loss = 0.68934296\n",
      "Iteration 9682, loss = 0.68934302\n",
      "Iteration 9683, loss = 0.68934321\n",
      "Iteration 9684, loss = 0.68934245\n",
      "Iteration 9685, loss = 0.68934315\n",
      "Iteration 9686, loss = 0.68934212\n",
      "Iteration 9687, loss = 0.68934172\n",
      "Iteration 9688, loss = 0.68934182\n",
      "Iteration 9689, loss = 0.68934160\n",
      "Iteration 9690, loss = 0.68934218\n",
      "Iteration 9691, loss = 0.68934140\n",
      "Iteration 9692, loss = 0.68934157\n",
      "Iteration 9693, loss = 0.68934141\n",
      "Iteration 9694, loss = 0.68934155\n",
      "Iteration 9695, loss = 0.68934133\n",
      "Iteration 9696, loss = 0.68934091\n",
      "Iteration 9697, loss = 0.68934099\n",
      "Iteration 9698, loss = 0.68934110\n",
      "Iteration 9699, loss = 0.68934070\n",
      "Iteration 9700, loss = 0.68934047\n",
      "Iteration 9701, loss = 0.68934029\n",
      "Iteration 9702, loss = 0.68934033\n",
      "Iteration 9703, loss = 0.68934010\n",
      "Iteration 9704, loss = 0.68934049\n",
      "Iteration 9705, loss = 0.68934051\n",
      "Iteration 9706, loss = 0.68933988\n",
      "Iteration 9707, loss = 0.68933947\n",
      "Iteration 9708, loss = 0.68934027\n",
      "Iteration 9709, loss = 0.68933964\n",
      "Iteration 9710, loss = 0.68933926\n",
      "Iteration 9711, loss = 0.68933923\n",
      "Iteration 9712, loss = 0.68933891\n",
      "Iteration 9713, loss = 0.68933870\n",
      "Iteration 9714, loss = 0.68933929\n",
      "Iteration 9715, loss = 0.68933875\n",
      "Iteration 9716, loss = 0.68933843\n",
      "Iteration 9717, loss = 0.68933867\n",
      "Iteration 9718, loss = 0.68933825\n",
      "Iteration 9719, loss = 0.68933850\n",
      "Iteration 9720, loss = 0.68933823\n",
      "Iteration 9721, loss = 0.68933789\n",
      "Iteration 9722, loss = 0.68933772\n",
      "Iteration 9723, loss = 0.68933759\n",
      "Iteration 9724, loss = 0.68933774\n",
      "Iteration 9725, loss = 0.68933760\n",
      "Iteration 9726, loss = 0.68933702\n",
      "Iteration 9727, loss = 0.68933685\n",
      "Iteration 9728, loss = 0.68933721\n",
      "Iteration 9729, loss = 0.68933701\n",
      "Iteration 9730, loss = 0.68933676\n",
      "Iteration 9731, loss = 0.68933649\n",
      "Iteration 9732, loss = 0.68933647\n",
      "Iteration 9733, loss = 0.68933778\n",
      "Iteration 9734, loss = 0.68933669\n",
      "Iteration 9735, loss = 0.68933614\n",
      "Iteration 9736, loss = 0.68933608\n",
      "Iteration 9737, loss = 0.68933622\n",
      "Iteration 9738, loss = 0.68933657\n",
      "Iteration 9739, loss = 0.68933565\n",
      "Iteration 9740, loss = 0.68933586\n",
      "Iteration 9741, loss = 0.68933548\n",
      "Iteration 9742, loss = 0.68933532\n",
      "Iteration 9743, loss = 0.68933577\n",
      "Iteration 9744, loss = 0.68933526\n",
      "Iteration 9745, loss = 0.68933520\n",
      "Iteration 9746, loss = 0.68933496\n",
      "Iteration 9747, loss = 0.68933462\n",
      "Iteration 9748, loss = 0.68933488\n",
      "Iteration 9749, loss = 0.68933492\n",
      "Iteration 9750, loss = 0.68933499\n",
      "Iteration 9751, loss = 0.68933491\n",
      "Iteration 9752, loss = 0.68933448\n",
      "Iteration 9753, loss = 0.68933394\n",
      "Iteration 9754, loss = 0.68933407\n",
      "Iteration 9755, loss = 0.68933383\n",
      "Iteration 9756, loss = 0.68933381\n",
      "Iteration 9757, loss = 0.68933390\n",
      "Iteration 9758, loss = 0.68933359\n",
      "Iteration 9759, loss = 0.68933343\n",
      "Iteration 9760, loss = 0.68933351\n",
      "Iteration 9761, loss = 0.68933371\n",
      "Iteration 9762, loss = 0.68933346\n",
      "Iteration 9763, loss = 0.68933300\n",
      "Iteration 9764, loss = 0.68933299\n",
      "Iteration 9765, loss = 0.68933297\n",
      "Iteration 9766, loss = 0.68933269\n",
      "Iteration 9767, loss = 0.68933262\n",
      "Iteration 9768, loss = 0.68933232\n",
      "Iteration 9769, loss = 0.68933241\n",
      "Iteration 9770, loss = 0.68933222\n",
      "Iteration 9771, loss = 0.68933204\n",
      "Iteration 9772, loss = 0.68933188\n",
      "Iteration 9773, loss = 0.68933182\n",
      "Iteration 9774, loss = 0.68933221\n",
      "Iteration 9775, loss = 0.68933149\n",
      "Iteration 9776, loss = 0.68933170\n",
      "Iteration 9777, loss = 0.68933109\n",
      "Iteration 9778, loss = 0.68933145\n",
      "Iteration 9779, loss = 0.68933100\n",
      "Iteration 9780, loss = 0.68933072\n",
      "Iteration 9781, loss = 0.68933069\n",
      "Iteration 9782, loss = 0.68933063\n",
      "Iteration 9783, loss = 0.68933078\n",
      "Iteration 9784, loss = 0.68933042\n",
      "Iteration 9785, loss = 0.68933029\n",
      "Iteration 9786, loss = 0.68933049\n",
      "Iteration 9787, loss = 0.68933016\n",
      "Iteration 9788, loss = 0.68933025\n",
      "Iteration 9789, loss = 0.68932959\n",
      "Iteration 9790, loss = 0.68932988\n",
      "Iteration 9791, loss = 0.68932984\n",
      "Iteration 9792, loss = 0.68932989\n",
      "Iteration 9793, loss = 0.68932967\n",
      "Iteration 9794, loss = 0.68932948\n",
      "Iteration 9795, loss = 0.68932938\n",
      "Iteration 9796, loss = 0.68932910\n",
      "Iteration 9797, loss = 0.68932913\n",
      "Iteration 9798, loss = 0.68932945\n",
      "Iteration 9799, loss = 0.68932863\n",
      "Iteration 9800, loss = 0.68932912\n",
      "Iteration 9801, loss = 0.68932852\n",
      "Iteration 9802, loss = 0.68932871\n",
      "Iteration 9803, loss = 0.68932827\n",
      "Iteration 9804, loss = 0.68932857\n",
      "Iteration 9805, loss = 0.68932803\n",
      "Iteration 9806, loss = 0.68932815\n",
      "Iteration 9807, loss = 0.68932800\n",
      "Iteration 9808, loss = 0.68932854\n",
      "Iteration 9809, loss = 0.68932763\n",
      "Iteration 9810, loss = 0.68932763\n",
      "Iteration 9811, loss = 0.68932758\n",
      "Iteration 9812, loss = 0.68932714\n",
      "Iteration 9813, loss = 0.68932734\n",
      "Iteration 9814, loss = 0.68932715\n",
      "Iteration 9815, loss = 0.68932701\n",
      "Iteration 9816, loss = 0.68932681\n",
      "Iteration 9817, loss = 0.68932661\n",
      "Iteration 9818, loss = 0.68932663\n",
      "Iteration 9819, loss = 0.68932667\n",
      "Iteration 9820, loss = 0.68932645\n",
      "Iteration 9821, loss = 0.68932659\n",
      "Iteration 9822, loss = 0.68932593\n",
      "Iteration 9823, loss = 0.68932626\n",
      "Iteration 9824, loss = 0.68932566\n",
      "Iteration 9825, loss = 0.68932570\n",
      "Iteration 9826, loss = 0.68932605\n",
      "Iteration 9827, loss = 0.68932538\n",
      "Iteration 9828, loss = 0.68932551\n",
      "Iteration 9829, loss = 0.68932521\n",
      "Iteration 9830, loss = 0.68932482\n",
      "Iteration 9831, loss = 0.68932508\n",
      "Iteration 9832, loss = 0.68932496\n",
      "Iteration 9833, loss = 0.68932567\n",
      "Iteration 9834, loss = 0.68932465\n",
      "Iteration 9835, loss = 0.68932455\n",
      "Iteration 9836, loss = 0.68932486\n",
      "Iteration 9837, loss = 0.68932460\n",
      "Iteration 9838, loss = 0.68932481\n",
      "Iteration 9839, loss = 0.68932462\n",
      "Iteration 9840, loss = 0.68932418\n",
      "Iteration 9841, loss = 0.68932405\n",
      "Iteration 9842, loss = 0.68932389\n",
      "Iteration 9843, loss = 0.68932376\n",
      "Iteration 9844, loss = 0.68932342\n",
      "Iteration 9845, loss = 0.68932372\n",
      "Iteration 9846, loss = 0.68932343\n",
      "Iteration 9847, loss = 0.68932355\n",
      "Iteration 9848, loss = 0.68932311\n",
      "Iteration 9849, loss = 0.68932291\n",
      "Iteration 9850, loss = 0.68932307\n",
      "Iteration 9851, loss = 0.68932256\n",
      "Iteration 9852, loss = 0.68932250\n",
      "Iteration 9853, loss = 0.68932224\n",
      "Iteration 9854, loss = 0.68932221\n",
      "Iteration 9855, loss = 0.68932240\n",
      "Iteration 9856, loss = 0.68932240\n",
      "Iteration 9857, loss = 0.68932213\n",
      "Iteration 9858, loss = 0.68932198\n",
      "Iteration 9859, loss = 0.68932193\n",
      "Iteration 9860, loss = 0.68932172\n",
      "Iteration 9861, loss = 0.68932135\n",
      "Iteration 9862, loss = 0.68932133\n",
      "Iteration 9863, loss = 0.68932156\n",
      "Iteration 9864, loss = 0.68932128\n",
      "Iteration 9865, loss = 0.68932190\n",
      "Iteration 9866, loss = 0.68932070\n",
      "Iteration 9867, loss = 0.68932066\n",
      "Iteration 9868, loss = 0.68932055\n",
      "Iteration 9869, loss = 0.68932017\n",
      "Iteration 9870, loss = 0.68932014\n",
      "Iteration 9871, loss = 0.68932124\n",
      "Iteration 9872, loss = 0.68932010\n",
      "Iteration 9873, loss = 0.68932031\n",
      "Iteration 9874, loss = 0.68931985\n",
      "Iteration 9875, loss = 0.68931993\n",
      "Iteration 9876, loss = 0.68931979\n",
      "Iteration 9877, loss = 0.68931941\n",
      "Iteration 9878, loss = 0.68931936\n",
      "Iteration 9879, loss = 0.68931930\n",
      "Iteration 9880, loss = 0.68931912\n",
      "Iteration 9881, loss = 0.68931875\n",
      "Iteration 9882, loss = 0.68931890\n",
      "Iteration 9883, loss = 0.68931886\n",
      "Iteration 9884, loss = 0.68931923\n",
      "Iteration 9885, loss = 0.68931840\n",
      "Iteration 9886, loss = 0.68931883\n",
      "Iteration 9887, loss = 0.68931832\n",
      "Iteration 9888, loss = 0.68931809\n",
      "Iteration 9889, loss = 0.68931851\n",
      "Iteration 9890, loss = 0.68931797\n",
      "Iteration 9891, loss = 0.68931790\n",
      "Iteration 9892, loss = 0.68931795\n",
      "Iteration 9893, loss = 0.68931760\n",
      "Iteration 9894, loss = 0.68931777\n",
      "Iteration 9895, loss = 0.68931762\n",
      "Iteration 9896, loss = 0.68931724\n",
      "Iteration 9897, loss = 0.68931714\n",
      "Iteration 9898, loss = 0.68931694\n",
      "Iteration 9899, loss = 0.68931694\n",
      "Iteration 9900, loss = 0.68931666\n",
      "Iteration 9901, loss = 0.68931659\n",
      "Iteration 9902, loss = 0.68931754\n",
      "Iteration 9903, loss = 0.68931626\n",
      "Iteration 9904, loss = 0.68931605\n",
      "Iteration 9905, loss = 0.68931627\n",
      "Iteration 9906, loss = 0.68931598\n",
      "Iteration 9907, loss = 0.68931608\n",
      "Iteration 9908, loss = 0.68931608\n",
      "Iteration 9909, loss = 0.68931605\n",
      "Iteration 9910, loss = 0.68931579\n",
      "Iteration 9911, loss = 0.68931554\n",
      "Iteration 9912, loss = 0.68931585\n",
      "Iteration 9913, loss = 0.68931514\n",
      "Iteration 9914, loss = 0.68931523\n",
      "Iteration 9915, loss = 0.68931550\n",
      "Iteration 9916, loss = 0.68931477\n",
      "Iteration 9917, loss = 0.68931484\n",
      "Iteration 9918, loss = 0.68931499\n",
      "Iteration 9919, loss = 0.68931452\n",
      "Iteration 9920, loss = 0.68931550\n",
      "Iteration 9921, loss = 0.68931535\n",
      "Iteration 9922, loss = 0.68931409\n",
      "Iteration 9923, loss = 0.68931422\n",
      "Iteration 9924, loss = 0.68931393\n",
      "Iteration 9925, loss = 0.68931376\n",
      "Iteration 9926, loss = 0.68931398\n",
      "Iteration 9927, loss = 0.68931372\n",
      "Iteration 9928, loss = 0.68931473\n",
      "Iteration 9929, loss = 0.68931316\n",
      "Iteration 9930, loss = 0.68931334\n",
      "Iteration 9931, loss = 0.68931318\n",
      "Iteration 9932, loss = 0.68931327\n",
      "Iteration 9933, loss = 0.68931301\n",
      "Iteration 9934, loss = 0.68931299\n",
      "Iteration 9935, loss = 0.68931258\n",
      "Iteration 9936, loss = 0.68931279\n",
      "Iteration 9937, loss = 0.68931213\n",
      "Iteration 9938, loss = 0.68931245\n",
      "Iteration 9939, loss = 0.68931252\n",
      "Iteration 9940, loss = 0.68931196\n",
      "Iteration 9941, loss = 0.68931227\n",
      "Iteration 9942, loss = 0.68931245\n",
      "Iteration 9943, loss = 0.68931142\n",
      "Iteration 9944, loss = 0.68931211\n",
      "Iteration 9945, loss = 0.68931165\n",
      "Iteration 9946, loss = 0.68931143\n",
      "Iteration 9947, loss = 0.68931112\n",
      "Iteration 9948, loss = 0.68931185\n",
      "Iteration 9949, loss = 0.68931132\n",
      "Iteration 9950, loss = 0.68931115\n",
      "Iteration 9951, loss = 0.68931056\n",
      "Iteration 9952, loss = 0.68931096\n",
      "Iteration 9953, loss = 0.68931078\n",
      "Iteration 9954, loss = 0.68931055\n",
      "Iteration 9955, loss = 0.68931067\n",
      "Iteration 9956, loss = 0.68931018\n",
      "Iteration 9957, loss = 0.68931023\n",
      "Iteration 9958, loss = 0.68930976\n",
      "Iteration 9959, loss = 0.68931005\n",
      "Iteration 9960, loss = 0.68931033\n",
      "Iteration 9961, loss = 0.68930986\n",
      "Iteration 9962, loss = 0.68930923\n",
      "Iteration 9963, loss = 0.68930998\n",
      "Iteration 9964, loss = 0.68930961\n",
      "Iteration 9965, loss = 0.68930918\n",
      "Iteration 9966, loss = 0.68930967\n",
      "Iteration 9967, loss = 0.68930910\n",
      "Iteration 9968, loss = 0.68930892\n",
      "Iteration 9969, loss = 0.68930879\n",
      "Iteration 9970, loss = 0.68930853\n",
      "Iteration 9971, loss = 0.68930842\n",
      "Iteration 9972, loss = 0.68930823\n",
      "Iteration 9973, loss = 0.68930834\n",
      "Iteration 9974, loss = 0.68930811\n",
      "Iteration 9975, loss = 0.68930795\n",
      "Iteration 9976, loss = 0.68930795\n",
      "Iteration 9977, loss = 0.68930825\n",
      "Iteration 9978, loss = 0.68930773\n",
      "Iteration 9979, loss = 0.68930770\n",
      "Iteration 9980, loss = 0.68930744\n",
      "Iteration 9981, loss = 0.68930734\n",
      "Iteration 9982, loss = 0.68930725\n",
      "Iteration 9983, loss = 0.68930697\n",
      "Iteration 9984, loss = 0.68930699\n",
      "Iteration 9985, loss = 0.68930683\n",
      "Iteration 9986, loss = 0.68930642\n",
      "Iteration 9987, loss = 0.68930672\n",
      "Iteration 9988, loss = 0.68930649\n",
      "Iteration 9989, loss = 0.68930693\n",
      "Iteration 9990, loss = 0.68930683\n",
      "Iteration 9991, loss = 0.68930622\n",
      "Iteration 9992, loss = 0.68930618\n",
      "Iteration 9993, loss = 0.68930614\n",
      "Iteration 9994, loss = 0.68930602\n",
      "Iteration 9995, loss = 0.68930597\n",
      "Iteration 9996, loss = 0.68930588\n",
      "Iteration 9997, loss = 0.68930565\n",
      "Iteration 9998, loss = 0.68930548\n",
      "Iteration 9999, loss = 0.68930561\n",
      "Iteration 10000, loss = 0.68930507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paradoxtown/miniconda3/envs/times/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(random_state=1, max_iter=10000, learning_rate_init=0.000001, verbose=3, tol=0.00000001).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paradoxtown/miniconda3/envs/times/lib/python3.8/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pred = mlp_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paradoxtown/miniconda3/envs/times/lib/python3.8/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but MLPClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4835262689225289"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "times",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
